{"paragraphs":[{"text":"%md\n# Tutorial 9 Working with OEHCS and Spark Streaming\n\nThis tutorial was built for BDCS-CE version 17.3.1 and OEHCS 0.10.2.  If you are using a later version of BDCS-CE, there may be a newer version of this tutorial notebook at <https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake>.  Questions and feedback about the tutorial: <david.bayard@oracle.com> or <john.wyant@oracle.com>\n\n**Contents**\n\n+ About OEHCS\n+ About Spark Streaming\n+ Configuring Spark Streaming to work with Kafka (and csv files)\n+ Setting up your OEHCS for access from BDCSCE\n+ Creating a topic in OEHCS\n+ Downloading sample data\n+ Writing a Producer to stream data to OEHCS\n+ Example: Consuming streaming data from OEHCS\n+ More complex example: Spark SQL and Writing to Object Store\n+ Next Steps\n\n\nAs a reminder, the documentation for BDCS-CE can be found here: <http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html>\n\n**NOTE 1: Please ensure that you have run the \"Setting up your BDCSCE Environment\" tutorial first.  That tutorial setups the environment needed by this tutorial.**\n\n**NOTE 2: Please run these paragraphs one at a time.  It will not work if you try to run the entire note all at once.**\n","user":"anonymous","dateUpdated":"2017-07-31T19:39:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tutorial 9 Working with OEHCS and Spark Streaming</h1>\n<p>This tutorial was built for BDCS-CE version 17.3.1 and OEHCS 0.10.2. If you are using a later version of BDCS-CE, there may be a newer version of this tutorial notebook at <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\">https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#100;a&#118;id.&#98;&#x61;&#x79;&#x61;r&#x64;@o&#114;&#97;&#99;&#108;e&#46;&#99;&#111;&#x6d;\">&#100;a&#118;id.&#98;&#x61;&#x79;&#x61;r&#x64;@o&#114;&#97;&#99;&#108;e&#46;&#99;&#111;&#x6d;</a> or <a href=\"mailto:&#x6a;&#x6f;&#x68;&#110;&#46;w&#x79;&#97;n&#x74;&#x40;o&#114;&#97;&#99;l&#101;&#x2e;c&#x6f;&#x6d;\">&#x6a;&#x6f;&#x68;&#110;&#46;w&#x79;&#97;n&#x74;&#x40;o&#114;&#97;&#99;l&#101;&#x2e;c&#x6f;&#x6d;</a></p>\n<p><strong>Contents</strong></p>\n<ul>\n  <li>About OEHCS</li>\n  <li>About Spark Streaming</li>\n  <li>Configuring Spark Streaming to work with Kafka (and csv files)</li>\n  <li>Setting up your OEHCS for access from BDCSCE</li>\n  <li>Creating a topic in OEHCS</li>\n  <li>Downloading sample data</li>\n  <li>Writing a Producer to stream data to OEHCS</li>\n  <li>Example: Consuming streaming data from OEHCS</li>\n  <li>More complex example: Spark SQL and Writing to Object Store</li>\n  <li>Next Steps</li>\n</ul>\n<p>As a reminder, the documentation for BDCS-CE can be found here: <a href=\"http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\">http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html</a></p>\n<p><strong>NOTE 1: Please ensure that you have run the &ldquo;Setting up your BDCSCE Environment&rdquo; tutorial first. That tutorial setups the environment needed by this tutorial.</strong></p>\n<p><strong>NOTE 2: Please run these paragraphs one at a time. It will not work if you try to run the entire note all at once.</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900674_-964003612","id":"20170419-090208_164595531","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:39:42+0000","dateFinished":"2017-07-31T19:39:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:586"},{"text":"%md\n# About OEHCS (Oracle Event Hub Cloud Service)\n\n\nOracle Event Hub Cloud Service combines the open source technology Apache Kafka with unique innovations from Oracle to deliver a complete platform for working with streaming data.\n\nThe Oracle Event Hub Cloud Service is a managed Platform as a Service (PaaS) cloud-based offering that provides a highly available and scalable messaging platform for loading and analyzing streaming data. You can:\n\n + Spin up multiple clusters and create topics in seconds, on demand, and then use it.\n + Scale Out or Scale In clusters or Add/Remove partitions to elastically react to varying demands of your streaming data.\n + Choose different cluster configurations depending on your needs.\n + Use open REST APIs and CLIs to manage, use, and extend the service.\n\n\nDocumentation for OEHCS can be found here: <http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html>.  General info about Apache Kafka can be found here: <https://kafka.apache.org/>.","user":"anonymous","dateUpdated":"2017-07-28T14:42:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About OEHCS (Oracle Event Hub Cloud Service)</h1>\n<p>Oracle Event Hub Cloud Service combines the open source technology Apache Kafka with unique innovations from Oracle to deliver a complete platform for working with streaming data.</p>\n<p>The Oracle Event Hub Cloud Service is a managed Platform as a Service (PaaS) cloud-based offering that provides a highly available and scalable messaging platform for loading and analyzing streaming data. You can:</p>\n<ul>\n  <li>Spin up multiple clusters and create topics in seconds, on demand, and then use it.</li>\n  <li>Scale Out or Scale In clusters or Add/Remove partitions to elastically react to varying demands of your streaming data.</li>\n  <li>Choose different cluster configurations depending on your needs.</li>\n  <li>Use open REST APIs and CLIs to manage, use, and extend the service.</li>\n</ul>\n<p>Documentation for OEHCS can be found here: <a href=\"http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html\">http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html</a>. General info about Apache Kafka can be found here: <a href=\"https://kafka.apache.org/\">https://kafka.apache.org/</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900675_-964388361","id":"20170419-090233_16831548","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:42:20+0000","dateFinished":"2017-07-28T14:42:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:587"},{"text":"%md\n# About Spark Streaming\n\nThe integration between OEHCS and BDCS-CE leverages Spark Streaming to easily process the live streams of data from OEHCS.  In particular, while Spark Streaming can work with multiple types of sources, we will be using Spark Streaming's support for Kafka as OEHCS leverages Kafka internally.\n\nDocumentation about Spark Streaming can be found here: <http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html>.  And the integration with Kafka here: <https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html>","user":"anonymous","dateUpdated":"2017-07-28T14:42:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About Spark Streaming</h1>\n<p>The integration between OEHCS and BDCS-CE leverages Spark Streaming to easily process the live streams of data from OEHCS. In particular, while Spark Streaming can work with multiple types of sources, we will be using Spark Streaming&rsquo;s support for Kafka as OEHCS leverages Kafka internally.</p>\n<p>Documentation about Spark Streaming can be found here: <a href=\"http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html\">http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html</a>. And the integration with Kafka here: <a href=\"https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\">https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900675_-964388361","id":"20170419-090743_1265713500","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:42:31+0000","dateFinished":"2017-07-28T14:42:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:588"},{"text":"%md\n# Configuring Spark Streaming to work with Kafka (and csv files)\n\nIn this tutorial, we will connect Spark Streaming running on BDCS-CE to Kafka running on OEHCS.  Spark Streaming has support for Kafka, but it is not included by default.  We need to tell Spark Streaming to add support for Kafka by marking it as a dependency.\n\nTo do so, we will edit the Spark settings for the Zeppelin Spark interpreter.\n\nFollow this procedure:\n\n + Go to the Settings tab (hint: you might want to open up a 2nd browser window so that you can refer back to these instructions)\n + Click on Notebook\n + In the Spark Interpreter section, click on Edit\n + Scroll down to the bottom of the Spark section, type in com.databricks:spark-csv_2.10:1.5.0 in the Artifact field to add a new dependency.  **This entry might already exist. If so, skip to the spark-streaming-kafka artifact**\n![csv image](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011518.jpg \"CSV Dependency\")\n + Then add this Artifact: org.apache.spark:spark-streaming-kafka_2.10:1.6.1 \n![dependency](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011553.jpg \"Dependency\")\n + Click the Save button at the bottom of the Spark section\n + Then, click the OK button to restart the Spark interpreter to pick up the new settings\n\n\nHere is an example:\n![spark dependencies](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/500/SparkDeps.gif)\n\n\nFor more details about Spark CSV support, see <https://github.com/databricks/spark-csv>.  For Spark Streaming Kafka support, see <https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html>. \n\nAs an aside, Zeppelin has a Spark Dependency interpreter (%dep) but that interpreter is depreciated and you will find that packages you list via the %dep interpreter will not be picked up if you leverage Spark SQL.  So, you should use the Settings tab to define your Spark dependencies, not the %dep interpreter.\n\n\n","user":"anonymous","dateUpdated":"2017-07-28T17:19:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Configuring Spark Streaming to work with Kafka (and csv files)</h1>\n<p>In this tutorial, we will connect Spark Streaming running on BDCS-CE to Kafka running on OEHCS. Spark Streaming has support for Kafka, but it is not included by default. We need to tell Spark Streaming to add support for Kafka by marking it as a dependency.</p>\n<p>To do so, we will edit the Spark settings for the Zeppelin Spark interpreter.</p>\n<p>Follow this procedure:</p>\n<ul>\n  <li>Go to the Settings tab (hint: you might want to open up a 2nd browser window so that you can refer back to these instructions)</li>\n  <li>Click on Notebook</li>\n  <li>In the Spark Interpreter section, click on Edit</li>\n  <li>Scroll down to the bottom of the Spark section, type in com.databricks:spark-csv_2.10:1.5.0 in the Artifact field to add a new dependency. <strong>This entry might already exist. If so, skip to the spark-streaming-kafka artifact</strong><br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011518.jpg\" alt=\"csv image\" title=\"CSV Dependency\" /></li>\n  <li>Then add this Artifact: org.apache.spark:spark-streaming-kafka_2.10:1.6.1<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011553.jpg\" alt=\"dependency\" title=\"Dependency\" /></li>\n  <li>Click the Save button at the bottom of the Spark section</li>\n  <li>Then, click the OK button to restart the Spark interpreter to pick up the new settings</li>\n</ul>\n<p>Here is an example:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/500/SparkDeps.gif\" alt=\"spark dependencies\" /></p>\n<p>For more details about Spark CSV support, see <a href=\"https://github.com/databricks/spark-csv\">https://github.com/databricks/spark-csv</a>. For Spark Streaming Kafka support, see <a href=\"https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\">https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html</a>. </p>\n<p>As an aside, Zeppelin has a Spark Dependency interpreter (%dep) but that interpreter is depreciated and you will find that packages you list via the %dep interpreter will not be picked up if you leverage Spark SQL. So, you should use the Settings tab to define your Spark dependencies, not the %dep interpreter.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900676_-966312105","id":"20170419-110833_352272515","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:42:41+0000","dateFinished":"2017-07-28T14:42:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:589"},{"text":"%md\n# Setting up your OEHCS for Access from BDCSCE\n\nTo connect your OEHCS instance with your BDCS-CE instance, we need to make sure the appropriate network access rules are created and enabled.  We also need to lookup a few settings for use later.\n\n\nFollow these steps (hint: you might want to open up a second browser window so that you can refer back to these instructions as you follow the steps):\n\n + First, lookup the **Public IP** address of your BDCS-CE server.  You will find it on the My Services - BDCSCE Service Overview page as show here:\n![Find IP](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011544.jpg \"Find IP\")\n + Next, navigate to the Service Overview page for your OEHCS Platform.  Make a note of the **Connect Descriptor**, which you will need later.\n![Connect Descriptor](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011543.jpg \"Connect Descriptor\")\n + From the OEHCS Service Overview page, navigate to the Access Rules. \n![Access Rule](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011545.jpg \"Access Rule\")\n + Create a new rule to allow your BDCS-CE server connect to OEHCS.  To do so, choose **Custom** for the source and enter the **Public IP** address of the *BDCS-CE* Server.  Set the Destination to the **kafka_KAFKA_SERVER** choice.  And enter the port as **6667**.  See the screenshot below:\n![New Rule](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011546.jpg \"New Rule\")\n\n","user":"anonymous","dateUpdated":"2017-07-28T14:44:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Setting up your OEHCS for Access from BDCSCE</h1>\n<p>To connect your OEHCS instance with your BDCS-CE instance, we need to make sure the appropriate network access rules are created and enabled. We also need to lookup a few settings for use later.</p>\n<p>Follow these steps (hint: you might want to open up a second browser window so that you can refer back to these instructions as you follow the steps):</p>\n<ul>\n  <li>First, lookup the <strong>Public IP</strong> address of your BDCS-CE server. You will find it on the My Services - BDCSCE Service Overview page as show here:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011544.jpg\" alt=\"Find IP\" title=\"Find IP\" /></li>\n  <li>Next, navigate to the Service Overview page for your OEHCS Platform. Make a note of the <strong>Connect Descriptor</strong>, which you will need later.<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011543.jpg\" alt=\"Connect Descriptor\" title=\"Connect Descriptor\" /></li>\n  <li>From the OEHCS Service Overview page, navigate to the Access Rules.<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011545.jpg\" alt=\"Access Rule\" title=\"Access Rule\" /></li>\n  <li>Create a new rule to allow your BDCS-CE server connect to OEHCS. To do so, choose <strong>Custom</strong> for the source and enter the <strong>Public IP</strong> address of the <em>BDCS-CE</em> Server. Set the Destination to the <strong>kafka_KAFKA_SERVER</strong> choice. And enter the port as <strong>6667</strong>. See the screenshot below:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011546.jpg\" alt=\"New Rule\" title=\"New Rule\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900676_-966312105","id":"20170419-183954_121707110","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:44:05+0000","dateFinished":"2017-07-28T14:44:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:590"},{"text":"%md\n# Creating a topic in OEHCS\n\nNow, we will create a new Kafka topic in OEHCS for this tutorial.  Before we do so, a quick comment about OEHCS terminology.  The \"OEHCS Platform\" is the terminology for the cluster itself.  A cluster will manage multiple topics.  We refer to individual topics as instances of \"OEHCS\".  Thus, to create a new topic you will create a new instance of \"OEHCS\" to run on the \"OEHCS Platform\" (cluster) you already created.\n\n\nFollow these instructions:\n\n + Return to My Services OEHCS Platform screen as shown below:\n![Platform](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011547.jpg \"Platform\")\n + Select Oracle Event Hub Cloud Service - Topics from the pop-up menu to the right of \"Oracle Event Hub Cloud Service - Platform\" as shown below:\n![OEHCS](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011548.jpg \"OEHCS\")\n + On the My Services Event Hub Cloud Service - Topics page, click the Create Service button as shown below:\n![OEHCS2](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011583.jpg \"OEHCS2\")\n + Create the service(Topic) by setting the Service Name to **TutorialOEHCS**, setting the Hosted On to your OEHCS Platform instance, and filling the other fields as shown below: \n![Topic1](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011549.jpg \"Topic1\")\n + Once the service(Topic) is created, click on Service(Topic) name on the Summary page to see more details, including the full topic name. Make a note of the full topic name as you will need it later.\n![Topic2](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011551.jpg \"Topic2\")\n\n\n\n\n\n\n","user":"anonymous","dateUpdated":"2017-07-31T19:23:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating a topic in OEHCS</h1>\n<p>Now, we will create a new Kafka topic in OEHCS for this tutorial. Before we do so, a quick comment about OEHCS terminology. The &ldquo;OEHCS Platform&rdquo; is the terminology for the cluster itself. A cluster will manage multiple topics. We refer to individual topics as instances of &ldquo;OEHCS&rdquo;. Thus, to create a new topic you will create a new instance of &ldquo;OEHCS&rdquo; to run on the &ldquo;OEHCS Platform&rdquo; (cluster) you already created.</p>\n<p>Follow these instructions:</p>\n<ul>\n  <li>Return to My Services OEHCS Platform screen as shown below:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011547.jpg\" alt=\"Platform\" title=\"Platform\" /></li>\n  <li>Select Oracle Event Hub Cloud Service - Topics from the pop-up menu to the right of &ldquo;Oracle Event Hub Cloud Service - Platform&rdquo; as shown below:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011548.jpg\" alt=\"OEHCS\" title=\"OEHCS\" /></li>\n  <li>On the My Services Event Hub Cloud Service - Topics page, click the Create Service button as shown below:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011583.jpg\" alt=\"OEHCS2\" title=\"OEHCS2\" /></li>\n  <li>Create the service(Topic) by setting the Service Name to <strong>TutorialOEHCS</strong>, setting the Hosted On to your OEHCS Platform instance, and filling the other fields as shown below:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011549.jpg\" alt=\"Topic1\" title=\"Topic1\" /></li>\n  <li>Once the service(Topic) is created, click on Service(Topic) name on the Summary page to see more details, including the full topic name. Make a note of the full topic name as you will need it later.<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011551.jpg\" alt=\"Topic2\" title=\"Topic2\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900676_-966312105","id":"20170419-091227_1075909783","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:23:21+0000","dateFinished":"2017-07-31T19:23:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:591"},{"title":"Set OEHCS and Object Store Parameters","text":"%spark\nz.angularBind(\"BIND_ObjectStorage_Container\", \"citibike\")\nz.angularBind(\"BIND_OEHCS_ConnectionDescriptor\", z.input(\"OEHCS_ConnectionDescriptor\",\"141.144.144.128:6667\"))\nz.angularBind(\"BIND_OEHCS_Topic\", z.input(\"OEHCS_Topic\",\"gse00010212-TutorialOEHCS\"))\n\n//save these for pyspark\nz.put(\"BIND_OEHCS_Topic\", z.angular(\"BIND_OEHCS_Topic\"))\nz.put(\"BIND_OEHCS_ConnectionDescriptor\", z.angular(\"BIND_OEHCS_ConnectionDescriptor\"))\n\n//save these for shell\nscala.tools.nsc.io.File(\"/var/lib/zeppelin/oehcs.sh\").writeAll(\n  \"export ObjectStorage_Container=\\\"\"+z.angular(\"BIND_ObjectStorage_Container\")+\"\\\"\\n\" +\n  \"export OEHCS_ConnectionDescriptor=\\\"\"+z.angular(\"BIND_OEHCS_ConnectionDescriptor\")+\"\\\"\\n\" +\n  \"export OEHCS_Topic=\\\"\"+z.angular(\"BIND_OEHCS_Topic\")+\"\\\"\\n\"\n)\nprintln(\"done\")\n","user":"anonymous","dateUpdated":"2017-07-31T19:25:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{"OEHCS_Topic":"gse00010212-TutorialOEHCS","ObjectStorage_Container":"citibike","OEHCS_ConnectionDescriptor":"141.144.29.130:6667"},"forms":{"OEHCS_ConnectionDescriptor":{"name":"OEHCS_ConnectionDescriptor","displayName":"OEHCS_ConnectionDescriptor","type":"input","defaultValue":"141.144.144.128:6667","hidden":false,"$$hashKey":"object:1196"},"OEHCS_Topic":{"name":"OEHCS_Topic","displayName":"OEHCS_Topic","type":"input","defaultValue":"gse00010212-TutorialOEHCS","hidden":false,"$$hashKey":"object:1197"}}},"apps":[],"jobName":"paragraph_1501252900677_-966696854","id":"20170427-091556_1383806412","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:25:45+0000","dateFinished":"2017-07-31T19:25:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:592"},{"text":"%md\n# Before you continue\n\nMake sure you have followed the above manual steps to add the Spark dependencies for CSV and Kafka, enable access from BDCSCE to OEHCS, create a topic in OEHCS, run the above paragraph to save your configuration settings.","user":"anonymous","dateUpdated":"2017-07-28T14:44:47+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Before you continue</h1>\n<p>Make sure you have followed the above manual steps to add the Spark dependencies for CSV and Kafka, enable access from BDCSCE to OEHCS, create a topic in OEHCS, run the above paragraph to save your configuration settings.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900677_-966696854","id":"20170427-100043_241174826","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:44:47+0000","dateFinished":"2017-07-28T14:44:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:593"},{"text":"%md\n# Downloading Sample Data\n\nThis example uses bike ride data from the New York City bike share program <https://www.citibikenyc.com/>.\n\nWe will download a month of data.  We will get our data from <https://www.citibikenyc.com/system-data>, which links us to <https://s3.amazonaws.com/tripdata/index.html>.  In particular, we will grab data for December 2016.\n\nRun the following paragraph to download data and unzip it.\n\n","user":"anonymous","dateUpdated":"2017-07-28T14:44:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Downloading Sample Data</h1>\n<p>This example uses bike ride data from the New York City bike share program <a href=\"https://www.citibikenyc.com/\">https://www.citibikenyc.com/</a>.</p>\n<p>We will download a month of data. We will get our data from <a href=\"https://www.citibikenyc.com/system-data\">https://www.citibikenyc.com/system-data</a>, which links us to <a href=\"https://s3.amazonaws.com/tripdata/index.html\">https://s3.amazonaws.com/tripdata/index.html</a>. In particular, we will grab data for December 2016.</p>\n<p>Run the following paragraph to download data and unzip it.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900677_-966696854","id":"20170427-100029_1153032330","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:44:53+0000","dateFinished":"2017-07-28T14:44:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:594"},{"title":"Shell commands to download data ","text":"%sh\n\nmkdir bikes\ncd bikes\nrm 201612-citibike-tripdata.zip\n\nFILENAME=201612-citibike-tripdata\necho \"Downloading $FILENAME.zip.  This may take a few minutes.\"\nwget https://s3.amazonaws.com/tripdata/$FILENAME.zip \nunzip $FILENAME.zip\nls -l\nhead $FILENAME.csv\n\necho \"done\"","user":"anonymous","dateUpdated":"2017-07-28T15:03:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"editOnDblClick":"false","language":"sh"},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900677_-966696854","id":"20170419-091541_1055978146","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:52:18+0000","dateFinished":"2017-07-28T14:52:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:595"},{"text":"%md\n# Writing a Producer to stream data to OEHCS\n\nThis tutorial will use a small python program to stream our bike data to OEHCS.  This program will read the bike data and play it back, either in \"real-time\" or in an accelerated fashion.  Our Python program will use the \"kafka-python\" library, as described here: <https://github.com/dpkp/kafka-python>.  \n\nOur first step is to download the kafka-python library using pip as seen in the next paragraph.  Then we will write our python program and save as a file.\n\nRun the next 2 paragraphs to continue.\n","user":"anonymous","dateUpdated":"2017-07-28T14:45:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Writing a Producer to stream data to OEHCS</h1>\n<p>This tutorial will use a small python program to stream our bike data to OEHCS. This program will read the bike data and play it back, either in &ldquo;real-time&rdquo; or in an accelerated fashion. Our Python program will use the &ldquo;kafka-python&rdquo; library, as described here: <a href=\"https://github.com/dpkp/kafka-python\">https://github.com/dpkp/kafka-python</a>. </p>\n<p>Our first step is to download the kafka-python library using pip as seen in the next paragraph. Then we will write our python program and save as a file.</p>\n<p>Run the next 2 paragraphs to continue.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900678_-965542607","id":"20170419-091249_1413883626","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:45:03+0000","dateFinished":"2017-07-28T14:45:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:596"},{"title":"Shell commands to install Python kafka libraries","text":"%sh\n#do this at least once.  With BDCS 17.2.5, you need to first setup the environment for sudo and pip as shown in Tutorial 2 Setting up the BDCS-CE Environment.\necho \"appdirs==1.4.3\nkafka-python==1.3.3\npackaging==16.8\npyparsing==2.2.0\npython-dateutil==2.6.0\nsix==1.10.0\n\" > requirements.txt\n\nsudo pip install -r requirements.txt 2>&1\necho \"done\"\n","user":"anonymous","dateUpdated":"2017-07-28T15:03:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"editOnDblClick":"false","language":"sh"},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900678_-965542607","id":"20170419-092112_1857107096","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:52:42+0000","dateFinished":"2017-07-28T14:52:47+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:597"},{"title":"Shell command to save our Python producer program to a script","text":"%sh\n\n. oehcs.sh\n\necho \"\n#!/usr/bin/env python\n\n# standard libraries\nimport os\nimport sys\nimport csv\nimport json\nfrom time import sleep\nimport datetime as dt\n\n# Quality of Life Utils\nimport dateutil.parser\n\n# kafka-python libraries\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\n\nif len(sys.argv) < 6:\n\tprint 'usage: tutorial_kafka.py [inputfile:/path/to/file.csv] [acceleration-factor:integer] [recordcount:int-0 is infinite] [starttime:YYYY-MM-DD hh:mm:ss]'\n\tsys.exit(1)\n\ninputfile = sys.argv[1]\naccelerator = float(sys.argv[2])\nrecordcount = int(sys.argv[3])\ninputstarttime = dateutil.parser.parse(' '.join(sys.argv[4:]))\n\nscriptstarttime = dt.datetime.now()\nrelativestarttimedelta = inputstarttime - scriptstarttime\n\ndef timedelta_total_seconds(timedelta):\n    return (\n        timedelta.microseconds + 0.0 +\n        (timedelta.seconds + timedelta.days * 24 * 3600) * 10 ** 6) / 10 ** 6\n\n\n# When this baby hits 88mph, you're going to see some serious stuff.\ndef delorean(accelerator, scriptstarttime):\n\tnowtime = dt.datetime.now()\n\treturn scriptstarttime + dt.timedelta(seconds=accelerator*timedelta_total_seconds(nowtime - scriptstarttime))\n\n\n# Kafka Stuff\n# put your broker hostname:port in single quotes inside those bracketse\nmykafkaservers = ['$OEHCS_ConnectionDescriptor']\nproducer = KafkaProducer(bootstrap_servers=mykafkaservers, value_serializer=lambda m: json.dumps(m).encode('utf-8'))\n\n\ni = 0\n# Reader Loop\nwith open(inputfile) as csvfile:\n\treader = csv.DictReader(csvfile)\n\tfor rec in reader:\n\t\teventtime = dateutil.parser.parse(rec['Start Time'])\n\t\tif eventtime < inputstarttime:\n\t\t\tcontinue\n\t\trelativenowtime = delorean(accelerator, scriptstarttime) + relativestarttimedelta\n\t\twhile (relativenowtime < eventtime):\n\t\t\trelativenowtime = delorean(accelerator, scriptstarttime) + relativestarttimedelta\n\t\t\t# print relativenowtime, eventtime\n\t\t\tsleep(0.1)\n\t\tprint str(i)+'/'+str(recordcount)+' sending: ', rec\n\t\tproducer.send('$OEHCS_Topic', rec)\n\t\ti += 1\n\t\tif i >= recordcount and recordcount != 0:\n\t\t\tbreak\n\" > tutorial_kafka.py\n\nls -l tutorial_kafka.py\necho \"done\"","user":"anonymous","dateUpdated":"2017-07-28T15:03:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"editOnDblClick":"false","language":"sh"},"editorHide":false,"tableHide":false},"settings":{"params":{"OEHCS_Topic":"gse00010212-TutorialOEHCS","OEHCS_Broker":"dcb-oehcs-apr19-kafka-zk-1","OEHCS_ConnectionDescriptor":"141.144.144.128:6667"},"forms":{}},"apps":[],"jobName":"paragraph_1501252900678_-965542607","id":"20170419-091305_1792407197","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:56:11+0000","dateFinished":"2017-07-28T14:56:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:598"},{"text":"%md\n# Example: Consuming streaming data from OEHCS\n\nNow that we have downloaded data and written our python producer, we are ready for our first example.  To run our example, **you will need to run 2 things**: the python producer script and a Spark Streaming consumer script.\n\nFirst, we will start with the Spark Streaming script.  **Run the next paragraph**.  **Scroll down and observe the output**, even though it says it is still running.  You will notice that it begins to report new information every 5 seconds.  After 90 seconds, it will shut itself down.\n\n**Before the Spark Streaming code shuts itself down**, move to the paragraph that invokes our python producer in a shell interpreter (it is the one right after the spark streaming script).  **Run this python producer paragraph**.  At this point, both the producer and the consumer should be running.  Observe the output of both.  \n\nAfter sending 10 records, the python producer will quit.  After 90 seconds, the Spark Streaming consumer will quit.\n\n","user":"anonymous","dateUpdated":"2017-07-28T14:45:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example: Consuming streaming data from OEHCS</h1>\n<p>Now that we have downloaded data and written our python producer, we are ready for our first example. To run our example, <strong>you will need to run 2 things</strong>: the python producer script and a Spark Streaming consumer script.</p>\n<p>First, we will start with the Spark Streaming script. <strong>Run the next paragraph</strong>. <strong>Scroll down and observe the output</strong>, even though it says it is still running. You will notice that it begins to report new information every 5 seconds. After 90 seconds, it will shut itself down.</p>\n<p><strong>Before the Spark Streaming code shuts itself down</strong>, move to the paragraph that invokes our python producer in a shell interpreter (it is the one right after the spark streaming script). <strong>Run this python producer paragraph</strong>. At this point, both the producer and the consumer should be running. Observe the output of both. </p>\n<p>After sending 10 records, the python producer will quit. After 90 seconds, the Spark Streaming consumer will quit.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900679_-965927356","id":"20170421-100334_1732241682","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:45:13+0000","dateFinished":"2017-07-28T14:45:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:599"},{"title":"Spark Streaming (Scala) Example: Kafka Consumer","text":"%spark\n\n{\n    \n\nimport _root_.kafka.serializer.StringDecoder //http://stackoverflow.com/questions/36397688/sbt-cannot-import-kafka-encoder-decoder-classes\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\n\n\n println(\"Creating new Streaming Context\")\n val ssc = new StreamingContext(sc, Seconds(5))\n \n val topic = z.angular(\"BIND_OEHCS_Topic\").toString\n println(\"topic:\"+topic)\n val topicsSet = topic.split(\",\").toSet\n \n val brokers=z.angular(\"BIND_OEHCS_ConnectionDescriptor\").toString\n println(\"brokers:\"+brokers)\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n \n println(\"Creating Kafka DStream\")\n //https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\n      \n\n println(\"Setting up operations on DStream\")    \n \n //for debugging, you can print the full contents of the first 10 rows of each batch of messages by uncommenting the following\n //messages.print()\n \n messages.foreachRDD(rdd => {\n     //our Kafka data comes in Key,Value format.  we only care about the value, so use a map to extract just the 2nd element\n     var values=rdd.map(kv => kv._2)\n     \n     //for this example, the value is a JSON string.  Let's make a DataFrame out of the JSON strings\n     var df=sqlContext.jsonRDD(values)\n     \n     var reccount = df.count()\n     //let's print out the count\n     printf(\"count = %s \\n\",reccount)\n     \n     //check to see if we have any rows...\n     if (reccount >0)  {\n        //let's print the first row\n        println(\"first row \",df.first())\n\n     } \n     \n })\n \n println(\"Starting Streaming Context\")\n ssc.start()\n\n println(\"Will now sleep for a few minutes, before stopping the StreamingContext.  At this point, you should start the producer.\")\n\n //now sleep for 1.5 minutes.  Parameter is milliseconds\n Thread.sleep(90000)\n\n //stop any active streamingcontexts.  Parameters are boolean stopSparkContext, boolean stopGracefully\n println(\"Stopping Active StreamingContext\")\n StreamingContext.getActive().map(_.stop(false,true))\n\n println(\"done\")\n\n}","user":"anonymous","dateUpdated":"2017-07-28T14:56:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900679_-965927356","id":"20170419-193925_26787468","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:56:16+0000","dateFinished":"2017-07-28T14:57:55+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:600"},{"title":"Running the Kafka Producer Shell script","text":"%sh\n#The arguments are datafile TimeSpeedupFactor RecordsToProduce StartDate StartTime  \npython ./tutorial_kafka.py bikes/201612-citibike-tripdata.csv 1 10 2016-12-01 06:00:00 2>&1\n","user":"anonymous","dateUpdated":"2017-07-28T15:03:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"editOnDblClick":"false","language":"sh"},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900679_-965927356","id":"20170419-093304_375400318","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:56:24+0000","dateFinished":"2017-07-28T14:57:16+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:601"},{"text":"%md\n# Before you continue\n\nAt this point, you have now seen a working example of data being streamed to OEHCS and read by BDCS-CE.  The operations that we performed on the incoming data were rather basic-- we just counted the incoming rows and printed out the first row of each batch.  \n\nNext, we will implement a more complex example.","user":"anonymous","dateUpdated":"2017-07-28T14:45:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Before you continue</h1>\n<p>At this point, you have now seen a working example of data being streamed to OEHCS and read by BDCS-CE. The operations that we performed on the incoming data were rather basic&ndash; we just counted the incoming rows and printed out the first row of each batch. </p>\n<p>Next, we will implement a more complex example.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900679_-965927356","id":"20170427-101453_1864348150","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:45:24+0000","dateFinished":"2017-07-28T14:45:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:602"},{"text":"%md\n# More complex example: Spark SQL and Writing to Object Store\n\nIn this example, we will demonstrate defining a Spark SQL table with our real-time data as well as writing data to the Object Store. \n\nTo run the more complex example, **you will need to**\n\n+ run the Spark paragraph below to start the consumer.  *Note: this consumer will not stop by itself but we provide commands later to stop it when needed.*\n+ run the producer paragraph below to start the producer.\n+ Observe the output of the consumer and run and re-run the SQL paragraphs to query the realtime_bike_trips table.  The consumer is using a window of 30 secods, so you should see different data approximately every 30 seconds as you run the SQL queries.\n+ run the Spark paragraph further below to stop the running StreamingContext\n+ check the ObjectStore to confirm that the streaming data was written to it\n\n\n","user":"anonymous","dateUpdated":"2017-07-31T19:38:33+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>More complex example: Spark SQL and Writing to Object Store</h1>\n<p>In this example, we will demonstrate defining a Spark SQL table with our real-time data as well as writing data to the Object Store. </p>\n<p>To run the more complex example, <strong>you will need to</strong></p>\n<ul>\n  <li>run the Spark paragraph below to start the consumer. <em>Note: this consumer will not stop by itself but we provide commands later to stop it when needed.</em></li>\n  <li>run the producer paragraph below to start the producer.</li>\n  <li>Observe the output of the consumer and run and re-run the SQL paragraphs to query the realtime_bike_trips table. The consumer is using a window of 30 secods, so you should see different data approximately every 30 seconds as you run the SQL queries.</li>\n  <li>run the Spark paragraph further below to stop the running StreamingContext</li>\n  <li>check the ObjectStore to confirm that the streaming data was written to it</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900680_-967851101","id":"20170421-100406_1372717581","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:38:33+0000","dateFinished":"2017-07-31T19:38:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:603"},{"title":"Streaming Consumer that defines a Spark SQL Table and writes to Object Store","text":"%spark\n\n{\n    \n\n import _root_.kafka.serializer.StringDecoder //http://stackoverflow.com/questions/36397688/sbt-cannot-import-kafka-encoder-decoder-classes\n import org.apache.spark.streaming._\n import org.apache.spark.streaming.kafka._\n\n val filebase = \"swift://\"+z.angular(\"BIND_ObjectStorage_Container\")+\".default/bikes_realtime\"\n\n\n println(\"Defining a placeholder empty dataframe for our SQL table structure when we don't have any realtime data\")\n\n //use a sample record to capture the column names and types\n val dummyrec=\"{'Birth Year': '1981', 'Stop Time': '2016-12-01 06:03:36', 'End Station Longitude': '-73.99061728', 'Trip Duration': '214', 'Start Station ID': '447', 'Start Station Longitude': '-73.9851615', 'End Station Latitude': '40.76669671', 'End Station Name': 'W 53 St & 10 Ave', 'Start Time': '2016-12-01 06:00:01', 'Start Station Latitude': '40.76370739', 'End Station ID': '480', 'Bike ID': '16669', 'User Type': 'Subscriber', 'Gender': '1', 'Start Station Name': '8 Ave & W 52 St'}\"\n val dummyRDD = sc.parallelize(dummyrec :: Nil)\n var dummyDf=sqlContext.jsonRDD(dummyRDD)\n //df.printSchema()\n //create a new DF with zero records (but with same column names/types)\n var emptyDF=dummyDf.filter(\"Gender = 'xxx'\")\n //emptyDF.printSchema()\n  \n  \n println(\"Creating new Streaming Context\")\n val ssc = new StreamingContext(sc, Seconds(30))\n \n\n val topic = z.angular(\"BIND_OEHCS_Topic\").toString\n println(\"topic:\"+topic)\n val topicsSet = topic.split(\",\").toSet\n \n val brokers=z.angular(\"BIND_OEHCS_ConnectionDescriptor\").toString\n println(\"brokers:\"+brokers)\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n\n\n println(\"Creating Kafka DStream\")\n //https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\n      \n\n println(\"Setting up operations on DStream\")    \n \n //for debugging, you can print the full contents of the first 10 rows of each batch of messages by uncommenting the following\n //messages.print()\n \n messages.foreachRDD(rdd => {\n     //our Kafka data comes in Key,Value format.  we only care about the value, so use a map to extract just the 2nd element\n     var values=rdd.map(kv => kv._2)\n     \n     //for this example, the value is a JSON string.  Let's make a DataFrame out of the JSON strings\n     var df=sqlContext.jsonRDD(values)\n     \n     var reccount = df.count()\n     //let's print out the count\n     //printf(\"count = %s \\n\",reccount)  This will get printed to the Spark log files, not zeppelin\n     \n     //check to see if we have any rows...\n     if (reccount >0) {\n        //let's print the first row\n        //println(\"first row \",df.first()) This will get printed to the Spark log files, not zeppelin\n        \n        //let's define a temptable\n        df.registerTempTable(\"realtime_bike_trips\")\n        \n        //let's also write this DF to Object Store...\n\n        // save in json format.  the repartition(1) ensures that we write a single output file, which makes sense since we know the output is small\n        df.repartition(1).write.format(\"json\").mode(\"append\").save(filebase)\n\n\n     } else {\n         //no records.\n         //let's register a df with no data\n         emptyDF.registerTempTable(\"realtime_bike_trips\")\n     }\n     \n     \n })\n \n println(\"Starting Streaming Context\")\n ssc.start()\n\n println(\"Note: you will need to manually stop this StreamingContext or it will continue forever.  To do so, run: StreamingContext.getActive().map(_.stop(false,true))\")\n println(\"      There is a sample paragraph below that shows you how to do this.\")\n\n println(\"Start of the Consumer done. Go ahead and start the producer if you have not already. With both the Consumer and Producer running, run the Spark SQL queries below and you should see different data every 30 seconds \")\n\n}","user":"anonymous","dateUpdated":"2017-07-31T19:36:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900680_-967851101","id":"20170421-100601_289995941","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:36:21+0000","dateFinished":"2017-07-31T19:36:21+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:604"},{"title":"Producer for the more complex example","text":"%sh\n#The arguments are datafile TimeSpeedupFactor RecordsToProduce StartDate StartTime  \n\n# for the more complex example, we are running a timespeedup factor of 10 to push data faster through the system\n\npython ./tutorial_kafka.py bikes/201612-citibike-tripdata.csv 10 300 2016-12-01 06:00:00 2>&1\n","user":"anonymous","dateUpdated":"2017-07-31T19:38:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"editOnDblClick":"false","language":"sh"},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900680_-967851101","id":"20170421-103000_112276404","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:30:07+0000","dateFinished":"2017-07-31T19:32:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:605"},{"title":"SQL to query realtime_bike_trips","text":"%sql\nselect * from realtime_bike_trips","user":"anonymous","dateUpdated":"2017-07-31T19:37:24+0000","config":{"colWidth":8,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Bike ID","index":0,"aggr":"sum"}],"values":[{"name":"Birth Year","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Bike ID","index":0,"aggr":"sum"},"yAxis":{"name":"Birth Year","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900681_-968235850","id":"20170421-094901_1826207284","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:37:24+0000","dateFinished":"2017-07-31T19:37:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:606"},{"title":"More SQL with realtime_bike_trips","text":"%sql\nselect count(*) cnt, min(`Start Time`) minTime,\nmax(`Start Time`) maxTime from realtime_bike_trips","user":"anonymous","dateUpdated":"2017-07-31T19:33:09+0000","config":{"colWidth":4,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"cnt","index":0,"aggr":"sum"}],"values":[{"name":"minTime","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"cnt","index":0,"aggr":"sum"},"yAxis":{"name":"minTime","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900681_-968235850","id":"20170421-104947_1434606148","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:33:09+0000","dateFinished":"2017-07-31T19:33:09+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:607"},{"title":"Spark code to stop the running Spark Streaming consumer ","text":"%spark\n{\n    import org.apache.spark.streaming._\n    println(\"Stopping any active StreamingContext.  May take a minute.\")\n    StreamingContext.getActive().map(_.stop(false,true))\n    println(\"done\")\n}","user":"anonymous","dateUpdated":"2017-07-31T19:37:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501252900681_-968235850","id":"20170419-115308_1919654685","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:37:28+0000","dateFinished":"2017-07-31T19:38:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:608"},{"title":"Shell command to list the Object Store contents","text":"%sh\n. oehcs.sh\necho Storage Container = $ObjectStorage_Container\n\necho \"Observe the timestamps of the files.  You should see files with today's timestamps added by the more complex streaming example\"\n\nhadoop fs -ls swift://$ObjectStorage_Container.default/bikes_realtime 2>&1","user":"anonymous","dateUpdated":"2017-07-28T15:04:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"editOnDblClick":"false","language":"sh"},"editorHide":false,"tableHide":false},"settings":{"params":{"container":"citibike"},"forms":{}},"apps":[],"jobName":"paragraph_1501252900682_-967081603","id":"20170421-105528_683135750","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T15:02:00+0000","dateFinished":"2017-07-28T15:02:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:609"},{"text":"%md\n# Next Steps\n\nLearn more about:\n\n + Oracle Event Hub Cloud Service: <http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html>\n + Spark Streaming: <http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html>\n + Apache Kafka: <https://kafka.apache.org/>\n + Spark Streaming and Kafka Integration: <https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html>\n \nRun the Demonstration Citi Bike Live Map with OEHCS and Spark Streaming\n","user":"anonymous","dateUpdated":"2017-07-31T19:37:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Next Steps</h1>\n<p>Learn more about:</p>\n<ul>\n  <li>Oracle Event Hub Cloud Service: <a href=\"http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html\">http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html</a></li>\n  <li>Spark Streaming: <a href=\"http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html\">http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html</a></li>\n  <li>Apache Kafka: <a href=\"https://kafka.apache.org/\">https://kafka.apache.org/</a></li>\n  <li>Spark Streaming and Kafka Integration: <a href=\"https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\">https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html</a></li>\n</ul>\n<p>Run the Demonstration Citi Bike Live Map with OEHCS and Spark Streaming</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900682_-967081603","id":"20170421-095706_230718086","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-31T19:37:41+0000","dateFinished":"2017-07-31T19:37:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:610"},{"text":"%md\n### Change Log\nJuly 28, 2017 - Updated for BDCSCE 17.3.1-20","user":"anonymous","dateUpdated":"2017-07-28T14:46:09+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>July 28, 2017 - Updated for BDCSCE 17.3.1-20</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501252900683_-967466352","id":"20170419-121108_1129923637","dateCreated":"2017-07-28T14:41:40+0000","dateStarted":"2017-07-28T14:46:09+0000","dateFinished":"2017-07-28T14:46:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:611"},{"text":"%md\n","user":"anonymous","dateUpdated":"2017-07-28T14:46:09+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501253169069_1666522974","id":"20170728-144609_433937768","dateCreated":"2017-07-28T14:46:09+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:612"}],"name":"Tutorial 9 Working with OEHCS and Spark Streaming","id":"2CPDMBZG4","angularObjects":{"2CQVFQFWU:shared_process":[],"2CRFREMNU:shared_process":[{"name":"BIND_ObjectStorage_Container","object":"citibike","noteId":"2CPDMBZG4"},{"name":"BIND_OEHCS_ConnectionDescriptor","object":"141.144.29.130:6667","noteId":"2CPDMBZG4"},{"name":"BIND_OEHCS_Topic","object":"gse00010212-TutorialOEHCS","noteId":"2CPDMBZG4"}],"2CPRZVQKR:shared_process":[],"2CQWU46XZ:shared_process":[],"2CPKCF4R9:shared_process":[],"2CR7Y2CFK:shared_process":[],"2CQQM64RT:shared_process":[],"2CQX3UW3S:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}