{"paragraphs":[{"text":"%md\n#Tutorial 9 Working with OEHCS and Spark Streaming\n\nThis tutorial was built for BDCS-CE version 17.2.5 and OEHCS 0.10.2.  If you are using a later version of BDCS-CE, please look for a newer version of this tutorial notebook at <https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake>.  Questions and feedback about the tutorial: <david.bayard@oracle.com> or <john.wyant@oracle.com>\n\n**Contents**\n+ About OEHCS\n+ About Spark Streaming\n+ Configuring Spark Streaming to work with Kafka (and csv files)\n+ Setting up your OEHCS for access from BDCSCE\n+ Creating a topic in OEHCS\n+ Downloading sample data\n+ Writing a Producer to stream data to OEHCS\n+ Example: Consuming streaming data from OEHCS\n+ More complex example: Spark SQL and Writing to Object Store\n+ Next Steps\n+ Bonus: Pyspark consumer example\n\n\nAs a reminder, the documentation for BDCS-CE can be found here: <http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html>\n\n**NOTE 1: Please ensure that you have run the \"Setting up your BDCSCE Environment\" tutorial first.  That tutorial setups the environment needed by this tutorial.**\n\n**NOTE 2: Please run these paragraphs one at a time.  It will not work if you try to run the entire note all at once.**\n","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 4:59:48 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429178_-1530747372","id":"20170419-090208_164595531","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Tutorial 9 Working with OEHCS and Spark Streaming</h1>\n<p>This tutorial was built for BDCS-CE version 17.2.5 and OEHCS 0.10.2.  If you are using a later version of BDCS-CE, please look for a newer version of this tutorial notebook at <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\">https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake</a>.  Questions and feedback about the tutorial: <a href=\"&#109;&#x61;&#x69;&#108;&#116;&#111;&#x3a;&#100;&#x61;&#118;&#x69;&#x64;&#46;&#x62;&#x61;&#x79;&#x61;&#114;&#100;&#x40;&#x6f;&#114;&#x61;&#x63;&#108;&#x65;&#46;&#x63;&#x6f;&#x6d;\">&#x64;a&#x76;&#105;&#x64;&#46;&#x62;&#97;&#121;&#97;&#x72;&#100;&#64;&#x6f;&#114;&#97;&#x63;&#108;&#x65;&#46;&#99;&#111;&#109;</a> or <a href=\"&#109;&#x61;&#x69;&#x6c;&#x74;&#x6f;&#x3a;&#x6a;&#111;&#104;&#110;&#x2e;&#x77;&#x79;&#x61;&#x6e;&#x74;&#64;&#111;&#x72;&#x61;&#x63;&#x6c;&#101;&#x2e;&#x63;&#111;&#109;\">&#106;&#x6f;&#104;&#110;&#46;&#x77;&#x79;&#97;&#x6e;&#x74;&#x40;&#x6f;&#114;&#97;&#99;&#108;&#x65;&#x2e;&#99;&#x6f;&#109;</a></p>\n<p><strong>Contents</strong></p>\n<ul>\n<li>About OEHCS</li>\n<li>About Spark Streaming</li>\n<li>Configuring Spark Streaming to work with Kafka (and csv files)</li>\n<li>Setting up your OEHCS for access from BDCSCE</li>\n<li>Creating a topic in OEHCS</li>\n<li>Downloading sample data</li>\n<li>Writing a Producer to stream data to OEHCS</li>\n<li>Example: Consuming streaming data from OEHCS</li>\n<li>More complex example: Spark SQL and Writing to Object Store</li>\n<li>Next Steps</li>\n<li>Bonus: Pyspark consumer example</li>\n</ul>\n<p>As a reminder, the documentation for BDCS-CE can be found here: <a href=\"http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\">http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html</a></p>\n<p><strong>NOTE 1: Please ensure that you have run the &ldquo;Setting up your BDCSCE Environment&rdquo; tutorial first.  That tutorial setups the environment needed by this tutorial.</strong></p>\n<p><strong>NOTE 2: Please run these paragraphs one at a time.  It will not work if you try to run the entire note all at once.</strong></p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 4:59:46 PM","dateFinished":"Jun 19, 2017 4:59:46 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:209"},{"text":"%md\n#About OEHCS (Oracle Event Hub Cloud Service)\n\n\nOracle Event Hub Cloud Service combines the open source technology Apache Kafka with unique innovations from Oracle to deliver a complete platform for working with streaming data.\n\nThe Oracle Event Hub Cloud Service is a managed Platform as a Service (PaaS) cloud-based offering that provides a highly available and scalable messaging platform for loading and analyzing streaming data. You can:\n\n + Spin up multiple clusters and create topics in seconds, on demand, and then use it.\n + Scale Out or Scale In clusters or Add/Remove partitions to elastically react to varying demands of your streaming data.\n + Choose different cluster configurations depending on your needs.\n + Use open REST APIs and CLIs to manage, use, and extend the service.\n\n\nDocumentation for OEHCS can be found here: <http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html>.  General info about Apache Kafka can be found here: <https://kafka.apache.org/>.","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 3:59:25 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429179_-1531132121","id":"20170419-090233_16831548","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>About OEHCS (Oracle Event Hub Cloud Service)</h1>\n<p>Oracle Event Hub Cloud Service combines the open source technology Apache Kafka with unique innovations from Oracle to deliver a complete platform for working with streaming data.</p>\n<p>The Oracle Event Hub Cloud Service is a managed Platform as a Service (PaaS) cloud-based offering that provides a highly available and scalable messaging platform for loading and analyzing streaming data. You can:</p>\n<ul>\n<li>Spin up multiple clusters and create topics in seconds, on demand, and then use it.</li>\n<li>Scale Out or Scale In clusters or Add/Remove partitions to elastically react to varying demands of your streaming data.</li>\n<li>Choose different cluster configurations depending on your needs.</li>\n<li>Use open REST APIs and CLIs to manage, use, and extend the service.</li>\n</ul>\n<p>Documentation for OEHCS can be found here: <a href=\"http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html\">http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html</a>.  General info about Apache Kafka can be found here: <a href=\"https://kafka.apache.org/\">https://kafka.apache.org/</a>.</p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 3:59:21 PM","dateFinished":"Jun 19, 2017 3:59:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:210"},{"text":"%md\n#About Spark Streaming\n\nThe integration between OEHCS and BDCS-CE leverages Spark Streaming to easily process the live streams of data from OEHCS.  In particular, while Spark Streaming can work with multiple types of sources, we will be using Spark Streaming's support for Kafka as OEHCS leverages Kafka internally.\n\nDocumentation about Spark Streaming can be found here: <http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html>.  And the integration with Kafka here: <https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html>","dateUpdated":"Jun 19, 2017 3:59:43 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429179_-1531132121","id":"20170419-090743_1265713500","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>About Spark Streaming</h1>\n<p>The integration between OEHCS and BDCS-CE leverages Spark Streaming to easily process the live streams of data from OEHCS.  In particular, while Spark Streaming can work with multiple types of sources, we will be using Spark Streaming's support for Kafka as OEHCS leverages Kafka internally.</p>\n<p>Documentation about Spark Streaming can be found here: <a href=\"http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html\">http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html</a>.  And the integration with Kafka here: <a href=\"https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\">https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html</a></p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:211"},{"text":"%md\n#Configuring Spark Streaming to work with Kafka (and csv files)\n\nIn this tutorial, we will connect Spark Streaming running on BDCS-CE to Kafka running on OEHCS.  Spark Streaming has support for Kafka, but it is not included by default.  We need to tell Spark Streaming to add support for Kafka by marking it as a dependency.\n\nTo do so, we will edit the Spark settings for the Zeppelin Spark interpreter.\n\nFollow this procedure:\n + Go to the Settings tab (hint: you might want to open up a 2nd browser window so that you can refer back to these instructions)\n + Click on Notebook\n + In the Spark Interpreter section, click on Edit\n + Scroll down to the bottom of the Spark section, type in com.databricks:spark-csv_2.10:1.5.0 in the Artifact field to add a new dependency.  **This entry might already exist. If so, skip to the spark-streaming-kafka artifact**\n![csv image](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011518.jpg \"CSV Dependency\")\n + Then add this Artifact: org.apache.spark:spark-streaming-kafka_2.10:1.6.1 \n![dependency](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011553.jpg \"Dependency\")\n + Click the Save button at the bottom of the Spark section\n + Then, click the OK button to restart the Spark interpreter to pick up the new settings\n\n\nHere is an example:\n![spark dependencies](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/500/SparkDeps.gif)\n\n\nFor more details about Spark CSV support, see <https://github.com/databricks/spark-csv>.  For Spark Streaming Kafka support, see <https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html>. \n\nAs an aside, Zeppelin has a Spark Dependency interpreter (%dep) but that interpreter is depreciated and you will find that packages you list via the %dep interpreter will not be picked up if you leverage Spark SQL.  So, you should use the Settings tab to define your Spark dependencies, not the %dep interpreter.\n\n\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:27:43 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429179_-1531132121","id":"20170419-110833_352272515","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Configuring Spark Streaming to work with Kafka (and csv files)</h1>\n<p>In this tutorial, we will connect Spark Streaming running on BDCS-CE to Kafka running on OEHCS.  Spark Streaming has support for Kafka, but it is not included by default.  We need to tell Spark Streaming to add support for Kafka by marking it as a dependency.</p>\n<p>To do so, we will edit the Spark settings for the Zeppelin Spark interpreter.</p>\n<p>Follow this procedure:</p>\n<ul>\n<li>Go to the Settings tab (hint: you might want to open up a 2nd browser window so that you can refer back to these instructions)</li>\n<li>Click on Notebook</li>\n<li>In the Spark Interpreter section, click on Edit</li>\n<li>Scroll down to the bottom of the Spark section, type in com.databricks:spark-csv_2.10:1.5.0 in the Artifact field to add a new dependency.  <strong>This entry might already exist. If so, skip to the spark-streaming-kafka artifact</strong>\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011518.jpg\" alt=\"csv image\" title=\"CSV Dependency\" /></li>\n<li>Then add this Artifact: org.apache.spark:spark-streaming-kafka_2.10:1.6.1\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011553.jpg\" alt=\"dependency\" title=\"Dependency\" /></li>\n<li>Click the Save button at the bottom of the Spark section</li>\n<li>Then, click the OK button to restart the Spark interpreter to pick up the new settings</li>\n</ul>\n<p>Here is an example:\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/500/SparkDeps.gif\" alt=\"spark dependencies\" /></p>\n<p>For more details about Spark CSV support, see <a href=\"https://github.com/databricks/spark-csv\">https://github.com/databricks/spark-csv</a>.  For Spark Streaming Kafka support, see <a href=\"https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\">https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html</a>.</p>\n<p>As an aside, Zeppelin has a Spark Dependency interpreter (%dep) but that interpreter is depreciated and you will find that packages you list via the %dep interpreter will not be picked up if you leverage Spark SQL.  So, you should use the Settings tab to define your Spark dependencies, not the %dep interpreter.</p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 20, 2017 9:27:40 AM","dateFinished":"Jun 20, 2017 9:27:40 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:212","focus":true},{"text":"%md\n#Setting up your OEHCS for Access from BDCSCE\n\nTo connect your OEHCS instance with your BDCS-CE instance, we need to make sure the appropriate network access rules are created and enabled.  We also need to lookup a few settings for use later.\n\n\nFollow these steps (hint: you might want to open up a second browser window so that you can refer back to these instructions as you follow the steps):\n + First, lookup the **Public IP** address of your BDCS-CE server.  You will find it on the My Services - BDCSCE Service Overview page as show here:\n![Find IP](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011544.jpg \"Find IP\")\n + Next, navigate to the Service Overview page for your OEHCS Platform.  Make a note of the **Connect Descriptor**, which you will need later.\n![Connect Descriptor](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011543.jpg \"Connect Descriptor\")\n + From the OEHCS Service Overview page, navigate to the Access Rules. \n![Access Rule](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011545.jpg \"Access Rule\")\n + Create a new rule to allow your BDCS-CE server connect to OEHCS.  To do so, choose **Custom** for the source and enter the **Public IP** address of the *BDCS-CE* Server.  Set the Destination to the **kafka_KAFKA_SERVER** choice.  And enter the port as **6667**.  See the screenshot below:\n![New Rule](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011546.jpg \"New Rule\")\n\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:27:52 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429179_-1531132121","id":"20170419-183954_121707110","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Setting up your OEHCS for Access from BDCSCE</h1>\n<p>To connect your OEHCS instance with your BDCS-CE instance, we need to make sure the appropriate network access rules are created and enabled.  We also need to lookup a few settings for use later.</p>\n<p>Follow these steps (hint: you might want to open up a second browser window so that you can refer back to these instructions as you follow the steps):</p>\n<ul>\n<li>First, lookup the <strong>Public IP</strong> address of your BDCS-CE server.  You will find it on the My Services - BDCSCE Service Overview page as show here:\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011544.jpg\" alt=\"Find IP\" title=\"Find IP\" /></li>\n<li>Next, navigate to the Service Overview page for your OEHCS Platform.  Make a note of the <strong>Connect Descriptor</strong>, which you will need later.\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011543.jpg\" alt=\"Connect Descriptor\" title=\"Connect Descriptor\" /></li>\n<li>From the OEHCS Service Overview page, navigate to the Access Rules.\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011545.jpg\" alt=\"Access Rule\" title=\"Access Rule\" /></li>\n<li>Create a new rule to allow your BDCS-CE server connect to OEHCS.  To do so, choose <strong>Custom</strong> for the source and enter the <strong>Public IP</strong> address of the <em>BDCS-CE</em> Server.  Set the Destination to the <strong>kafka_KAFKA_SERVER</strong> choice.  And enter the port as <strong>6667</strong>.  See the screenshot below:\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011546.jpg\" alt=\"New Rule\" title=\"New Rule\" /></li>\n</ul>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 20, 2017 9:27:49 AM","dateFinished":"Jun 20, 2017 9:27:49 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:213","focus":true},{"text":"%md\n#Creating a topic in OEHCS\n\nNow, we will create a new Kafka topic in OEHCS for this tutorial.  Before we do so, a quick comment about OEHCS terminology.  The \"OEHCS Platform\" is the terminology for the cluster itself.  A cluster will manage multiple topics.  We refer to individual topics as instances of \"OEHCS\".  Thus, to create a new topic you will create a new instance of \"OEHCS\" to run on the \"OEHCS Platform\" (cluster) you already created.\n\n\nFollow these instructions:\n + Return to My Services OEHCS Platform screen as shown below:\n![Platform](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011547.jpg \"Platform\")\n + Select Oracle Event Hub Cloud Service from the pop-up menu to the right of \"Oracle Event Hub Cloud Service - Platform\" as shown below:\n![OEHCS](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011548.jpg \"OEHCS\")\n + On the My Services Event Hub Cloud Service page, click the Create Service button as shown below:\n![OEHCS2](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011583.jpg \"OEHCS2\")\n + Create the service(Topic) by setting the Service Name to **TutorialOEHCS**, setting the Hosted On to your OEHCS Platform instance, and filling the other fields as shown below: \n![Topic1](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011549.jpg \"Topic1\")\n + Once the service(Topic) is created, click on Service(Topic) name on the Summary page to see more details, including the full topic name. Make a note of the full topic name as you will need it later.\n![Topic2](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011551.jpg \"Topic2\")\n\n\n\n\n\n\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:28:04 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429179_-1531132121","id":"20170419-091227_1075909783","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Creating a topic in OEHCS</h1>\n<p>Now, we will create a new Kafka topic in OEHCS for this tutorial.  Before we do so, a quick comment about OEHCS terminology.  The &ldquo;OEHCS Platform&rdquo; is the terminology for the cluster itself.  A cluster will manage multiple topics.  We refer to individual topics as instances of &ldquo;OEHCS&rdquo;.  Thus, to create a new topic you will create a new instance of &ldquo;OEHCS&rdquo; to run on the &ldquo;OEHCS Platform&rdquo; (cluster) you already created.</p>\n<p>Follow these instructions:</p>\n<ul>\n<li>Return to My Services OEHCS Platform screen as shown below:\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011547.jpg\" alt=\"Platform\" title=\"Platform\" /></li>\n<li>Select Oracle Event Hub Cloud Service from the pop-up menu to the right of &ldquo;Oracle Event Hub Cloud Service - Platform&rdquo; as shown below:\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011548.jpg\" alt=\"OEHCS\" title=\"OEHCS\" /></li>\n<li>On the My Services Event Hub Cloud Service page, click the Create Service button as shown below:\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011583.jpg\" alt=\"OEHCS2\" title=\"OEHCS2\" /></li>\n<li>Create the service(Topic) by setting the Service Name to <strong>TutorialOEHCS</strong>, setting the Hosted On to your OEHCS Platform instance, and filling the other fields as shown below:\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011549.jpg\" alt=\"Topic1\" title=\"Topic1\" /></li>\n<li>Once the service(Topic) is created, click on Service(Topic) name on the Summary page to see more details, including the full topic name. Make a note of the full topic name as you will need it later.\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011551.jpg\" alt=\"Topic2\" title=\"Topic2\" /></li>\n</ul>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 20, 2017 9:28:01 AM","dateFinished":"Jun 20, 2017 9:28:01 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:214","focus":true},{"title":"Set OEHCS and Object Store Parameters","text":"%spark\nz.angularBind(\"BIND_ObjectStorage_Container\", \"citibike\")\nz.angularBind(\"BIND_OEHCS_ConnectionDescriptor\", z.input(\"OEHCS_ConnectionDescriptor\",\"141.144.144.128:6667\"))\nz.angularBind(\"BIND_OEHCS_Topic\", z.input(\"OEHCS_Topic\",\"gse00010212-TutorialOEHCS\"))\n\n//save these for pyspark\nz.put(\"BIND_OEHCS_Topic\", z.angular(\"BIND_OEHCS_Topic\"))\nz.put(\"BIND_OEHCS_ConnectionDescriptor\", z.angular(\"BIND_OEHCS_ConnectionDescriptor\"))\n\n//save these for shell\nscala.tools.nsc.io.File(\"/var/lib/zeppelin/oehcs.sh\").writeAll(\n  \"export ObjectStorage_Container=\\\"\"+z.angular(\"BIND_ObjectStorage_Container\")+\"\\\"\\n\" +\n  \"export OEHCS_ConnectionDescriptor=\\\"\"+z.angular(\"BIND_OEHCS_ConnectionDescriptor\")+\"\\\"\\n\" +\n  \"export OEHCS_Topic=\\\"\"+z.angular(\"BIND_OEHCS_Topic\")+\"\\\"\\n\"\n)\nprintln(\"done\")\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:28:37 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"OEHCS_Topic":"gse00010212-TutorialOEHCS","ObjectStorage_Container":"citibike","OEHCS_ConnectionDescriptor":"141.144.144.128:6667"},"forms":{"OEHCS_ConnectionDescriptor":{"name":"OEHCS_ConnectionDescriptor","displayName":"OEHCS_ConnectionDescriptor","type":"input","defaultValue":"141.144.144.128:6667","hidden":false},"OEHCS_Topic":{"name":"OEHCS_Topic","displayName":"OEHCS_Topic","type":"input","defaultValue":"gse00010212-TutorialOEHCS","hidden":false}}},"jobName":"paragraph_1497880429180_-1533055866","id":"20170427-091556_1383806412","result":{"code":"SUCCESS","type":"TEXT","msg":"done\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 20, 2017 9:28:37 AM","dateFinished":"Jun 20, 2017 9:28:37 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:215","focus":true},{"text":"%md\n#Before you continue\n\nMake sure you have followed the above manual steps to add the Spark dependencies for CSV and Kafka, enable access from BDCSCE to OEHCS, create a topic in OEHCS, run the above paragraph to save your configuration settings.","dateUpdated":"Jun 19, 2017 9:53:49 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429180_-1533055866","id":"20170427-100043_241174826","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Before you continue</h1>\n<p>Make sure you have followed the above manual steps to add the Spark dependencies for CSV and Kafka, enable access from BDCSCE to OEHCS, create a topic in OEHCS, run the above paragraph to save your configuration settings.</p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216"},{"text":"%md\n#Downloading Sample Data\n\nThis example uses bike ride data from the New York City bike share program <https://www.citibikenyc.com/>.\n\nWe will download a month of data.  We will get our data from <https://www.citibikenyc.com/system-data>, which links us to <https://s3.amazonaws.com/tripdata/index.html>.  In particular, we will grab data for December 2016.\n\nRun the following paragraph to download data and unzip it.\n\n","dateUpdated":"Jun 19, 2017 9:53:49 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429180_-1533055866","id":"20170427-100029_1153032330","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Downloading Sample Data</h1>\n<p>This example uses bike ride data from the New York City bike share program <a href=\"https://www.citibikenyc.com/\">https://www.citibikenyc.com/</a>.</p>\n<p>We will download a month of data.  We will get our data from <a href=\"https://www.citibikenyc.com/system-data\">https://www.citibikenyc.com/system-data</a>, which links us to <a href=\"https://s3.amazonaws.com/tripdata/index.html\">https://s3.amazonaws.com/tripdata/index.html</a>.  In particular, we will grab data for December 2016.</p>\n<p>Run the following paragraph to download data and unzip it.</p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:217"},{"title":"Shell commands to download data ","text":"%sh\n\nmkdir bikes\ncd bikes\nrm 201612-citibike-tripdata.zip\n\nFILENAME=201612-citibike-tripdata\necho \"Downloading $FILENAME.zip.  This may take a few minutes.\"\nwget https://s3.amazonaws.com/tripdata/$FILENAME.zip \nunzip $FILENAME.zip\nls -l\nhead $FILENAME.csv\n\necho \"done\"","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 4:37:33 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429180_-1533055866","id":"20170419-091541_1055978146","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 4:37:33 PM","dateFinished":"Jun 19, 2017 4:37:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:218","errorMessage":"","focus":true},{"text":"%md\n#Writing a Producer to stream data to OEHCS\n\nThis tutorial will use a small python program to stream our bike data to OEHCS.  This program will read the bike data and play it back, either in \"real-time\" or in an accelerated fashion.  Our Python program will use the \"kafka-python\" library, as described here: <https://github.com/dpkp/kafka-python>.  \n\nOur first step is to download the kafka-python library using pip as seen in the next paragraph.  Then we will write our python program and save as a file.\n\nRun the next 2 paragraphs to continue.\n","dateUpdated":"Jun 19, 2017 9:53:49 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429180_-1533055866","id":"20170419-091249_1413883626","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Writing a Producer to stream data to OEHCS</h1>\n<p>This tutorial will use a small python program to stream our bike data to OEHCS.  This program will read the bike data and play it back, either in &ldquo;real-time&rdquo; or in an accelerated fashion.  Our Python program will use the &ldquo;kafka-python&rdquo; library, as described here: <a href=\"https://github.com/dpkp/kafka-python\">https://github.com/dpkp/kafka-python</a>.</p>\n<p>Our first step is to download the kafka-python library using pip as seen in the next paragraph.  Then we will write our python program and save as a file.</p>\n<p>Run the next 2 paragraphs to continue.</p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:219"},{"title":"Shell commands to install Python kafka libraries","text":"%sh\n#do this at least once.  With BDCS 17.2.5, you need to first setup the environment for sudo and pip as shown in Tutorial 2 Setting up the BDCS-CE Environment.\necho \"appdirs==1.4.3\nkafka-python==1.3.3\npackaging==16.8\npyparsing==2.2.0\npython-dateutil==2.6.0\nsix==1.10.0\n\" > requirements.txt\n\nsudo pip install -r requirements.txt 2>&1\necho \"done\"\n","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 4:38:34 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429180_-1533055866","id":"20170419-092112_1857107096","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 4:38:35 PM","dateFinished":"Jun 19, 2017 4:38:38 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:220","errorMessage":"","focus":true},{"title":"Shell command to save our Python producer program to a script","text":"%sh\n\n. oehcs.sh\n\necho \"\n#!/usr/bin/env python\n\n# standard libraries\nimport os\nimport sys\nimport csv\nimport json\nfrom time import sleep\nimport datetime as dt\n\n# Quality of Life Utils\nimport dateutil.parser\n\n# kafka-python libraries\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\n\nif len(sys.argv) < 6:\n\tprint 'usage: tutorial_kafka.py [inputfile:/path/to/file.csv] [acceleration-factor:integer] [recordcount:int-0 is infinite] [starttime:YYYY-MM-DD hh:mm:ss]'\n\tsys.exit(1)\n\ninputfile = sys.argv[1]\naccelerator = float(sys.argv[2])\nrecordcount = int(sys.argv[3])\ninputstarttime = dateutil.parser.parse(' '.join(sys.argv[4:]))\n\nscriptstarttime = dt.datetime.now()\nrelativestarttimedelta = inputstarttime - scriptstarttime\n\ndef timedelta_total_seconds(timedelta):\n    return (\n        timedelta.microseconds + 0.0 +\n        (timedelta.seconds + timedelta.days * 24 * 3600) * 10 ** 6) / 10 ** 6\n\n\n# When this baby hits 88mph, you're going to see some serious stuff.\ndef delorean(accelerator, scriptstarttime):\n\tnowtime = dt.datetime.now()\n\treturn scriptstarttime + dt.timedelta(seconds=accelerator*timedelta_total_seconds(nowtime - scriptstarttime))\n\n\n# Kafka Stuff\n# put your broker hostname:port in single quotes inside those bracketse\nmykafkaservers = ['$OEHCS_ConnectionDescriptor']\nproducer = KafkaProducer(bootstrap_servers=mykafkaservers, value_serializer=lambda m: json.dumps(m).encode('utf-8'))\n\n\ni = 0\n# Reader Loop\nwith open(inputfile) as csvfile:\n\treader = csv.DictReader(csvfile)\n\tfor rec in reader:\n\t\teventtime = dateutil.parser.parse(rec['Start Time'])\n\t\tif eventtime < inputstarttime:\n\t\t\tcontinue\n\t\trelativenowtime = delorean(accelerator, scriptstarttime) + relativestarttimedelta\n\t\twhile (relativenowtime < eventtime):\n\t\t\trelativenowtime = delorean(accelerator, scriptstarttime) + relativestarttimedelta\n\t\t\t# print relativenowtime, eventtime\n\t\t\tsleep(0.1)\n\t\tprint str(i)+'/'+str(recordcount)+' sending: ', rec\n\t\tproducer.send('$OEHCS_Topic', rec)\n\t\ti += 1\n\t\tif i >= recordcount and recordcount != 0:\n\t\t\tbreak\n\" > tutorial_kafka.py\n\nls -l tutorial_kafka.py\necho \"done\"","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:29:34 AM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"OEHCS_Topic":"gse00010212-TutorialOEHCS","OEHCS_Broker":"dcb-oehcs-apr19-kafka-zk-1","OEHCS_ConnectionDescriptor":"141.144.144.128:6667"},"forms":{}},"jobName":"paragraph_1497880429181_-1533440615","id":"20170419-091305_1792407197","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 4:38:52 PM","dateFinished":"Jun 19, 2017 4:38:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:221","errorMessage":"","focus":true},{"text":"%md\n#Example: Consuming streaming data from OEHCS\n\nNow that we have downloaded data and written our python producer, we are ready for our first example.  To run our example, **you will need to run 2 things**: the python producer script and a Spark Streaming consumer script.\n\nFirst, we will start with the Spark Streaming script.  **Run the next paragraph**.  **Scroll down and observe the output**, even though it says it is still running.  You will notice that it begins to report new information every 5 seconds.  After 90 seconds, it will shut itself down.\n\n**Before the Spark Streaming code shuts itself down**, move to the paragraph that invokes our python producer in a shell interpreter (it is the one right after the spark streaming script).  **Run this python producer paragraph**.  At this point, both the producer and the consumer should be running.  Observe the output of both.  \n\nAfter sending 10 records, the python producer will quit.  After 90 seconds, the Spark Streaming consumer will quit.\n\n","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:10:22 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429181_-1533440615","id":"20170421-100334_1732241682","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example: Consuming streaming data from OEHCS</h1>\n<p>Now that we have downloaded data and written our python producer, we are ready for our first example.  To run our example, <strong>you will need to run 2 things</strong>: the python producer script and a Spark Streaming consumer script.</p>\n<p>First, we will start with the Spark Streaming script.  <strong>Run the next paragraph</strong>.  <strong>Scroll down and observe the output</strong>, even though it says it is still running.  You will notice that it begins to report new information every 5 seconds.  After 90 seconds, it will shut itself down.</p>\n<p><strong>Before the Spark Streaming code shuts itself down</strong>, move to the paragraph that invokes our python producer in a shell interpreter (it is the one right after the spark streaming script).  <strong>Run this python producer paragraph</strong>.  At this point, both the producer and the consumer should be running.  Observe the output of both.</p>\n<p>After sending 10 records, the python producer will quit.  After 90 seconds, the Spark Streaming consumer will quit.</p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:10:17 PM","dateFinished":"Jun 19, 2017 5:10:17 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:222"},{"title":"Spark Streaming (Scala) Example: Kafka Consumer","text":"%spark\n\n{\n    \n\nimport _root_.kafka.serializer.StringDecoder //http://stackoverflow.com/questions/36397688/sbt-cannot-import-kafka-encoder-decoder-classes\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\n\n\n println(\"Creating new Streaming Context\")\n val ssc = new StreamingContext(sc, Seconds(5))\n \n val topic = z.angular(\"BIND_OEHCS_Topic\").toString\n println(\"topic:\"+topic)\n val topicsSet = topic.split(\",\").toSet\n \n val brokers=z.angular(\"BIND_OEHCS_ConnectionDescriptor\").toString\n println(\"brokers:\"+brokers)\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n \n println(\"Creating Kafka DStream\")\n //https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\n      \n\n println(\"Setting up operations on DStream\")    \n \n //for debugging, you can print the full contents of the first 10 rows of each batch of messages by uncommenting the following\n //messages.print()\n \n messages.foreachRDD(rdd => {\n     //our Kafka data comes in Key,Value format.  we only care about the value, so use a map to extract just the 2nd element\n     var values=rdd.map(kv => kv._2)\n     \n     //for this example, the value is a JSON string.  Let's make a DataFrame out of the JSON strings\n     var df=sqlContext.jsonRDD(values)\n     \n     var reccount = df.count()\n     //let's print out the count\n     printf(\"count = %s \\n\",reccount)\n     \n     //check to see if we have any rows...\n     if (reccount >0)  {\n        //let's print the first row\n        println(\"first row \",df.first())\n\n     } \n     \n })\n \n println(\"Starting Streaming Context\")\n ssc.start()\n\n println(\"Will now sleep for a few minutes, before stopping the StreamingContext.  At this point, you should start the producer.\")\n\n //now sleep for 1.5 minutes.  Parameter is milliseconds\n Thread.sleep(90000)\n\n //stop any active streamingcontexts.  Parameters are boolean stopSparkContext, boolean stopGracefully\n println(\"Stopping Active StreamingContext\")\n StreamingContext.getActive().map(_.stop(false,true))\n\n println(\"done\")\n\n}","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:06:47 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429181_-1533440615","id":"20170419-193925_26787468","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:06:47 PM","dateFinished":"Jun 19, 2017 5:08:25 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:223","errorMessage":"","focus":true},{"title":"Running the Kafka Producer Shell script","text":"%sh\n#The arguments are datafile TimeSpeedupFactor RecordsToProduce StartDate StartTime  \npython ./tutorial_kafka.py bikes/201612-citibike-tripdata.csv 1 10 2016-12-01 06:00:00 2>&1\n","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:07:29 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429181_-1533440615","id":"20170419-093304_375400318","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:07:29 PM","dateFinished":"Jun 19, 2017 5:08:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224","errorMessage":"","focus":true},{"text":"%md\n#Before you continue\n\nAt this point, you have now seen a working example of data being streamed to OEHCS and read by BDCS-CE.  The operations that we performed on the incoming data were rather basic-- we just counted the incoming rows and printed out the first row of each batch.  \n\nNext, we will implement a more complex example.","dateUpdated":"Jun 19, 2017 9:53:49 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429182_-1532286368","id":"20170427-101453_1864348150","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Before you continue</h1>\n<p>At this point, you have now seen a working example of data being streamed to OEHCS and read by BDCS-CE.  The operations that we performed on the incoming data were rather basic&ndash; we just counted the incoming rows and printed out the first row of each batch.</p>\n<p>Next, we will implement a more complex example.</p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:225"},{"text":"%md\n#More complex example: Spark SQL and Writing to Object Store\n\nIn this example, we will demonstrate defining a Spark SQL table with our real-time data as well as writing data to the Object Store. \n\nTo run the more complex example, **you will need to**\n+ run the Spark paragraph below to start the consumer.  *Note: this consumer will not stop by itself but we provide commands later to stop it when needed.*\n+ run the producer paragraph below to start the producer.\n+ Observe the output of the consumer and run and re-run the SQL paragraphs to query the realtime_bike_trips table.  The consumer is using a window of 30 secods, so you should see different data approximately every 30 seconds as you run the SQL queries.\n+ run the Spark paragraph further below to stop the running StreamingContext\n+ check the ObjectStore to confirm that the streaming data was written to it\n\n\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:30:31 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429182_-1532286368","id":"20170421-100406_1372717581","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>More complex example: Spark SQL and Writing to Object Store</h1>\n<p>In this example, we will demonstrate defining a Spark SQL table with our real-time data as well as writing data to the Object Store.</p>\n<p>To run the more complex example, <strong>you will need to</strong></p>\n<ul>\n<li>run the Spark paragraph below to start the consumer.  <em>Note: this consumer will not stop by itself but we provide commands later to stop it when needed.</em></li>\n<li>run the producer paragraph below to start the producer.</li>\n<li>Observe the output of the consumer and run and re-run the SQL paragraphs to query the realtime_bike_trips table.  The consumer is using a window of 30 secods, so you should see different data approximately every 30 seconds as you run the SQL queries.</li>\n<li>run the Spark paragraph further below to stop the running StreamingContext</li>\n<li>check the ObjectStore to confirm that the streaming data was written to it</li>\n</ul>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 20, 2017 9:30:10 AM","dateFinished":"Jun 20, 2017 9:30:10 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:226","focus":true},{"title":"Streaming Consumer that defines a Spark SQL Table and writes to Object Store","text":"%spark\n\n{\n    \n\n import _root_.kafka.serializer.StringDecoder //http://stackoverflow.com/questions/36397688/sbt-cannot-import-kafka-encoder-decoder-classes\n import org.apache.spark.streaming._\n import org.apache.spark.streaming.kafka._\n\n val filebase = \"swift://\"+z.angular(\"BIND_ObjectStorage_Container\")+\".default/bikes_realtime\"\n\n\n println(\"Defining a placeholder empty dataframe for our SQL table structure when we don't have any realtime data\")\n\n //use a sample record to capture the column names and types\n val dummyrec=\"{'Birth Year': '1981', 'Stop Time': '2016-12-01 06:03:36', 'End Station Longitude': '-73.99061728', 'Trip Duration': '214', 'Start Station ID': '447', 'Start Station Longitude': '-73.9851615', 'End Station Latitude': '40.76669671', 'End Station Name': 'W 53 St & 10 Ave', 'Start Time': '2016-12-01 06:00:01', 'Start Station Latitude': '40.76370739', 'End Station ID': '480', 'Bike ID': '16669', 'User Type': 'Subscriber', 'Gender': '1', 'Start Station Name': '8 Ave & W 52 St'}\"\n val dummyRDD = sc.parallelize(dummyrec :: Nil)\n var dummyDf=sqlContext.jsonRDD(dummyRDD)\n //df.printSchema()\n //create a new DF with zero records (but with same column names/types)\n var emptyDF=dummyDf.filter(\"Gender = 'xxx'\")\n //emptyDF.printSchema()\n  \n  \n println(\"Creating new Streaming Context\")\n val ssc = new StreamingContext(sc, Seconds(30))\n \n\n val topic = z.angular(\"BIND_OEHCS_Topic\").toString\n println(\"topic:\"+topic)\n val topicsSet = topic.split(\",\").toSet\n \n val brokers=z.angular(\"BIND_OEHCS_ConnectionDescriptor\").toString\n println(\"brokers:\"+brokers)\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n\n\n println(\"Creating Kafka DStream\")\n //https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\n      \n\n println(\"Setting up operations on DStream\")    \n \n //for debugging, you can print the full contents of the first 10 rows of each batch of messages by uncommenting the following\n //messages.print()\n \n messages.foreachRDD(rdd => {\n     //our Kafka data comes in Key,Value format.  we only care about the value, so use a map to extract just the 2nd element\n     var values=rdd.map(kv => kv._2)\n     \n     //for this example, the value is a JSON string.  Let's make a DataFrame out of the JSON strings\n     var df=sqlContext.jsonRDD(values)\n     \n     var reccount = df.count()\n     //let's print out the count\n     printf(\"count = %s \\n\",reccount)\n     \n     //check to see if we have any rows...\n     if (reccount >0) {\n        //let's print the first row\n        println(\"first row \",df.first())\n        \n        //let's define a temptable\n        df.registerTempTable(\"realtime_bike_trips\")\n        \n        //let's also write this DF to Object Store...\n\n        // save in json format.  the repartition(1) ensures that we write a single output file, which makes sense since we know the output is small\n        df.repartition(1).write.format(\"json\").mode(\"append\").save(filebase)\n\n\n     } else {\n         //no records.\n         //let's register a df with no data\n         emptyDF.registerTempTable(\"realtime_bike_trips\")\n     }\n     \n     \n })\n \n println(\"Starting Streaming Context\")\n ssc.start()\n\n println(\"Note: you will need to manually stop this StreamingContext or it will continue forever.  To do so, run: StreamingContext.getActive().map(_.stop(false,true))\")\n println(\"      There is a sample paragraph below that shows you how to do this.\")\n\n println(\"done, but you'll see new output written every 30 seconds until you stop it.  Go ahead and start the producer if you have not already.\")\n\n}","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:17:55 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429182_-1532286368","id":"20170421-100601_289995941","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:17:55 PM","dateFinished":"Jun 19, 2017 5:17:55 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:227","errorMessage":"","focus":true},{"title":"Producer for the more complex example","text":"%sh\n#The arguments are datafile TimeSpeedupFactor RecordsToProduce StartDate StartTime  \n\n# for the more complex example, we are running a timespeedup factor of 10 to push data faster through the system\n\npython ./tutorial_kafka.py bikes/201612-citibike-tripdata.csv 10 300 2016-12-01 06:00:00 2>&1\n","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:18:04 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429182_-1532286368","id":"20170421-103000_112276404","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:18:04 PM","dateFinished":"Jun 19, 2017 5:20:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:228","errorMessage":"","focus":true},{"title":"SQL to query realtime_bike_trips","text":"%sql\nselect * from realtime_bike_trips","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:18:33 PM","config":{"colWidth":8,"editorMode":"ace/mode/sql","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Bike ID","index":0,"aggr":"sum"}],"values":[{"name":"Birth Year","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Bike ID","index":0,"aggr":"sum"},"yAxis":{"name":"Birth Year","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429188_-1449950103","id":"20170421-094901_1826207284","result":{"code":"SUCCESS","type":"TABLE","msg":"Bike ID\tBirth Year\tEnd Station ID\tEnd Station Latitude\tEnd Station Longitude\tEnd Station Name\tGender\tStart Station ID\tStart Station Latitude\tStart Station Longitude\tStart Station Name\tStart Time\tStop Time\tTrip Duration\tUser Type\n20578\t1964\t3141\t40.76500525\t-73.95818491\t1 Ave & E 68 St\t1\t490\t40.751551\t-73.993934\t8 Ave & W 33 St\t2016-12-01 06:00:08\t2016-12-01 06:27:10\t1622\tSubscriber\n25033\t1958\t497\t40.73704984\t-73.99009296\tE 17 St & Broadway\t1\t3236\t40.75898481399634\t-73.99379968643188\tW 42 St & Dyer Ave\t2016-12-01 06:00:10\t2016-12-01 06:22:39\t1349\tSubscriber\n22604\t1992\t297\t40.734232\t-73.986923\tE 15 St & 3 Ave\t1\t394\t40.72521311\t-73.97768752\tE 9 St & Avenue C\t2016-12-01 06:00:11\t2016-12-01 06:07:59\t467\tSubscriber\n24035\t1958\t527\t40.744023\t-73.976056\tE 33 St & 2 Ave\t1\t492\t40.75019995\t-73.99093085\tW 33 St & 7 Ave\t2016-12-01 06:00:21\t2016-12-01 06:05:54\t333\tSubscriber\n24295\t1966\t3224\t40.73997354103409\t-74.00513872504234\tW 13 St & Hudson St\t1\t3161\t40.7801839724239\t-73.97728532552719\tW 76 St & Columbus Ave\t2016-12-01 06:00:30\t2016-12-01 06:25:04\t1473\tSubscriber\n21229\t1979\t518\t40.74780373\t-73.9734419\tE 39 St & 2 Ave\t1\t468\t40.7652654\t-73.98192338\tBroadway & W 55 St\t2016-12-01 06:00:33\t2016-12-01 06:14:10\t817\tSubscriber\n25056\t1991\t461\t40.73587678\t-73.98205027\tE 20 St & 2 Ave\t1\t3141\t40.76500525\t-73.95818491\t1 Ave & E 68 St\t2016-12-01 06:01:07\t2016-12-01 06:14:23\t796\tSubscriber\n22465\t1974\t327\t40.7153379\t-74.01658354\tVesey Pl & River Terrace\t1\t482\t40.73935542\t-73.99931783\tW 15 St & 7 Ave\t2016-12-01 06:01:32\t2016-12-01 06:16:59\t927\tSubscriber\n23329\t1960\t402\t40.7403432\t-73.98955109\tBroadway & E 22 St\t1\t326\t40.72953837\t-73.98426726\tE 11 St & 1 Ave\t2016-12-01 06:01:36\t2016-12-01 06:09:51\t494\tSubscriber\n23018\t1966\t499\t40.76915505\t-73.98191841\tBroadway & W 60 St\t1\t3233\t40.75724567911726\t-73.97805914282799\tE 48 St & 5 Ave\t2016-12-01 06:01:41\t2016-12-01 06:09:22\t461\tSubscriber\n19287\t1985\t536\t40.74144387\t-73.97536082\t1 Ave & E 30 St\t2\t174\t40.7381765\t-73.97738662\tE 25 St & 1 Ave\t2016-12-01 06:01:53\t2016-12-01 06:04:20\t147\tSubscriber\n24311\t1966\t477\t40.75640548\t-73.9900262\tW 41 St & 8 Ave\t1\t459\t40.746745\t-74.007756\tW 20 St & 11 Ave\t2016-12-01 06:02:02\t2016-12-01 06:12:00\t597\tSubscriber\n22268\t1973\t388\t40.749717753\t-74.002950346\tW 26 St & 10 Ave\t0\t3152\t40.76873687\t-73.96119945\t3 Ave & E 71 St\t2016-12-01 06:02:07\t2016-12-01 06:33:23\t1876\tSubscriber\n25835\t1987\t324\t40.689888\t-73.981013\tDeKalb Ave & Hudson Ave\t1\t241\t40.68981035\t-73.97493121\tDeKalb Ave & S Portland Ave\t2016-12-01 06:02:35\t2016-12-01 06:04:45\t130\tSubscriber\n19371\t1985\t324\t40.689888\t-73.981013\tDeKalb Ave & Hudson Ave\t1\t270\t40.69308257\t-73.97178913\tAdelphi St & Myrtle Ave\t2016-12-01 06:02:58\t2016-12-01 06:06:30\t212\tSubscriber\n23530\t1970\t352\t40.76340613\t-73.97722479\tW 56 St & 6 Ave\t1\t477\t40.75640548\t-73.9900262\tW 41 St & 8 Ave\t2016-12-01 06:03:02\t2016-12-01 06:10:39\t457\tSubscriber\n22114\t1977\t456\t40.7597108\t-73.97402311\tE 53 St & Madison Ave\t1\t477\t40.75640548\t-73.9900262\tW 41 St & 8 Ave\t2016-12-01 06:03:03\t2016-12-01 06:10:06\t423\tSubscriber\n22972\t1984\t2000\t40.70255088\t-73.98940236\tFront St & Washington St\t1\t420\t40.68764484\t-73.96968902\tClermont Ave & Lafayette Ave\t2016-12-01 06:03:03\t2016-12-01 06:14:53\t709\tSubscriber\n17243\t1972\t477\t40.75640548\t-73.9900262\tW 41 St & 8 Ave\t1\t3236\t40.75898481399634\t-73.99379968643188\tW 42 St & Dyer Ave\t2016-12-01 06:04:02\t2016-12-01 06:06:44\t162\tSubscriber\n22983\t1996\t2012\t40.739445\t-73.976806\tE 27 St & 1 Ave\t2\t519\t40.751873\t-73.977706\tPershing Square North\t2016-12-01 06:04:03\t2016-12-01 06:12:23\t500\tSubscriber\n16669\t1981\t480\t40.76669671\t-73.99061728\tW 53 St & 10 Ave\t1\t447\t40.76370739\t-73.9851615\t8 Ave & W 52 St\t2016-12-01 06:00:01\t2016-12-01 06:03:36\t214\tSubscriber\n25077\t1992\t387\t40.71273266\t-74.0046073\tCentre St & Chambers St\t1\t507\t40.73912601\t-73.97973776\tE 25 St & 2 Ave\t2016-12-01 06:00:13\t2016-12-01 06:14:22\t848\tSubscriber\n22680\t1975\t468\t40.7652654\t-73.98192338\tBroadway & W 55 St\t1\t3233\t40.75724567911726\t-73.97805914282799\tE 48 St & 5 Ave\t2016-12-01 06:00:16\t2016-12-01 06:05:28\t312\tSubscriber\n27180\t1980\t337\t40.7037992\t-74.00838676\tOld Slip & Front St\t1\t264\t40.70706456\t-74.00731853\tMaiden Ln & Pearl St\t2016-12-01 06:00:52\t2016-12-01 06:03:04\t131\tSubscriber\n26385\t1950\t264\t40.70706456\t-74.00731853\tMaiden Ln & Pearl St\t1\t3255\t40.7505853470215\t-73.9946848154068\t8 Ave & W 31 St\t2016-12-01 06:01:00\t2016-12-01 06:19:39\t1119\tSubscriber\n21243\t1959\t536\t40.74144387\t-73.97536082\t1 Ave & E 30 St\t1\t477\t40.75640548\t-73.9900262\tW 41 St & 8 Ave\t2016-12-01 06:01:05\t2016-12-01 06:11:17\t611\tSubscriber\n24112\t1974\t497\t40.73704984\t-73.99009296\tE 17 St & Broadway\t1\t3255\t40.7505853470215\t-73.9946848154068\t8 Ave & W 31 St\t2016-12-01 06:01:13\t2016-12-01 06:10:42\t569\tSubscriber\n25619\t1970\t366\t40.693261\t-73.968896\tClinton Ave & Myrtle Ave\t2\t366\t40.693261\t-73.968896\tClinton Ave & Myrtle Ave\t2016-12-01 06:01:16\t2016-12-01 06:42:29\t2473\tSubscriber\n23455\t1997\t440\t40.75255434\t-73.97282625\tE 45 St & 3 Ave\t1\t529\t40.7575699\t-73.99098507\tW 42 St & 8 Ave\t2016-12-01 06:01:19\t2016-12-01 06:07:21\t362\tSubscriber\n25537\t1985\t527\t40.744023\t-73.976056\tE 33 St & 2 Ave\t1\t492\t40.75019995\t-73.99093085\tW 33 St & 7 Ave\t2016-12-01 06:01:29\t2016-12-01 06:07:58\t388\tSubscriber\n24973\t1956\t514\t40.76087502\t-74.00277668\t12 Ave & W 40 St\t1\t3255\t40.7505853470215\t-73.9946848154068\t8 Ave & W 31 St\t2016-12-01 06:02:12\t2016-12-01 06:11:22\t549\tSubscriber\n19071\t1989\t3132\t40.76350532\t-73.97109243\tE 59 St & Madison Ave\t1\t461\t40.73587678\t-73.98205027\tE 20 St & 2 Ave\t2016-12-01 06:02:19\t2016-12-01 06:17:02\t883\tSubscriber\n17928\t1976\t324\t40.689888\t-73.981013\tDeKalb Ave & Hudson Ave\t1\t416\t40.68753406\t-73.97265183\tCumberland St & Lafayette Ave\t2016-12-01 06:02:27\t2016-12-01 06:05:50\t203\tSubscriber\n17389\t1966\t527\t40.744023\t-73.976056\tE 33 St & 2 Ave\t1\t490\t40.751551\t-73.993934\t8 Ave & W 33 St\t2016-12-01 06:02:44\t2016-12-01 06:13:51\t666\tSubscriber\n23610\t1957\t456\t40.7597108\t-73.97402311\tE 53 St & Madison Ave\t1\t423\t40.76584941\t-73.98690506\tW 54 St & 9 Ave\t2016-12-01 06:02:48\t2016-12-01 06:08:02\t314\tSubscriber\n19465\t1971\t3172\t40.7785669\t-73.97754961\tW 74 St & Columbus Ave\t1\t357\t40.73261787\t-73.99158043\tE 11 St & Broadway\t2016-12-01 06:02:58\t2016-12-01 06:31:50\t1731\tSubscriber\n22961\t1974\t486\t40.7462009\t-73.98855723\tBroadway & W 29 St\t1\t326\t40.72953837\t-73.98426726\tE 11 St & 1 Ave\t2016-12-01 06:03:27\t2016-12-01 06:12:31\t543\tSubscriber\n27034\t1984\t458\t40.751396\t-74.005226\t11 Ave & W 27 St\t1\t405\t40.739323\t-74.008119\tWashington St & Gansevoort St\t2016-12-01 06:03:57\t2016-12-01 06:10:20\t382\tSubscriber\n","comment":"","msgTable":[[{"key":"Birth Year","value":"20578"},{"key":"Birth Year","value":"1964"},{"key":"Birth Year","value":"3141"},{"key":"Birth Year","value":"40.76500525"},{"key":"Birth Year","value":"-73.95818491"},{"key":"Birth Year","value":"1 Ave & E 68 St"},{"key":"Birth Year","value":"1"},{"key":"Birth Year","value":"490"},{"key":"Birth Year","value":"40.751551"},{"key":"Birth Year","value":"-73.993934"},{"key":"Birth Year","value":"8 Ave & W 33 St"},{"key":"Birth Year","value":"2016-12-01 06:00:08"},{"key":"Birth Year","value":"2016-12-01 06:27:10"},{"key":"Birth Year","value":"1622"},{"key":"Birth Year","value":"Subscriber"}],[{"key":"End Station ID","value":"25033"},{"key":"End Station ID","value":"1958"},{"key":"End Station ID","value":"497"},{"key":"End Station ID","value":"40.73704984"},{"key":"End Station ID","value":"-73.99009296"},{"key":"End Station ID","value":"E 17 St & Broadway"},{"key":"End Station ID","value":"1"},{"key":"End Station ID","value":"3236"},{"key":"End Station ID","value":"40.75898481399634"},{"key":"End Station ID","value":"-73.99379968643188"},{"key":"End Station ID","value":"W 42 St & Dyer Ave"},{"key":"End Station ID","value":"2016-12-01 06:00:10"},{"key":"End Station ID","value":"2016-12-01 06:22:39"},{"key":"End Station ID","value":"1349"},{"key":"End Station ID","value":"Subscriber"}],[{"key":"End Station Latitude","value":"22604"},{"key":"End Station Latitude","value":"1992"},{"key":"End Station Latitude","value":"297"},{"key":"End Station Latitude","value":"40.734232"},{"key":"End Station Latitude","value":"-73.986923"},{"key":"End Station Latitude","value":"E 15 St & 3 Ave"},{"key":"End Station Latitude","value":"1"},{"key":"End Station Latitude","value":"394"},{"key":"End Station Latitude","value":"40.72521311"},{"key":"End Station Latitude","value":"-73.97768752"},{"key":"End Station Latitude","value":"E 9 St & Avenue C"},{"key":"End Station Latitude","value":"2016-12-01 06:00:11"},{"key":"End Station Latitude","value":"2016-12-01 06:07:59"},{"key":"End Station Latitude","value":"467"},{"key":"End Station Latitude","value":"Subscriber"}],[{"key":"End Station Longitude","value":"24035"},{"key":"End Station Longitude","value":"1958"},{"key":"End Station Longitude","value":"527"},{"key":"End Station Longitude","value":"40.744023"},{"key":"End Station Longitude","value":"-73.976056"},{"key":"End Station Longitude","value":"E 33 St & 2 Ave"},{"key":"End Station Longitude","value":"1"},{"key":"End Station Longitude","value":"492"},{"key":"End Station Longitude","value":"40.75019995"},{"key":"End Station Longitude","value":"-73.99093085"},{"key":"End Station Longitude","value":"W 33 St & 7 Ave"},{"key":"End Station Longitude","value":"2016-12-01 06:00:21"},{"key":"End Station Longitude","value":"2016-12-01 06:05:54"},{"key":"End Station Longitude","value":"333"},{"key":"End Station Longitude","value":"Subscriber"}],[{"key":"End Station Name","value":"24295"},{"key":"End Station Name","value":"1966"},{"key":"End Station Name","value":"3224"},{"key":"End Station Name","value":"40.73997354103409"},{"key":"End Station Name","value":"-74.00513872504234"},{"key":"End Station Name","value":"W 13 St & Hudson St"},{"key":"End Station Name","value":"1"},{"key":"End Station Name","value":"3161"},{"key":"End Station Name","value":"40.7801839724239"},{"key":"End Station Name","value":"-73.97728532552719"},{"key":"End Station Name","value":"W 76 St & Columbus Ave"},{"key":"End Station Name","value":"2016-12-01 06:00:30"},{"key":"End Station Name","value":"2016-12-01 06:25:04"},{"key":"End Station Name","value":"1473"},{"key":"End Station Name","value":"Subscriber"}],[{"key":"Gender","value":"21229"},{"key":"Gender","value":"1979"},{"key":"Gender","value":"518"},{"key":"Gender","value":"40.74780373"},{"key":"Gender","value":"-73.9734419"},{"key":"Gender","value":"E 39 St & 2 Ave"},{"key":"Gender","value":"1"},{"key":"Gender","value":"468"},{"key":"Gender","value":"40.7652654"},{"key":"Gender","value":"-73.98192338"},{"key":"Gender","value":"Broadway & W 55 St"},{"key":"Gender","value":"2016-12-01 06:00:33"},{"key":"Gender","value":"2016-12-01 06:14:10"},{"key":"Gender","value":"817"},{"key":"Gender","value":"Subscriber"}],[{"key":"Start Station ID","value":"25056"},{"key":"Start Station ID","value":"1991"},{"key":"Start Station ID","value":"461"},{"key":"Start Station ID","value":"40.73587678"},{"key":"Start Station ID","value":"-73.98205027"},{"key":"Start Station ID","value":"E 20 St & 2 Ave"},{"key":"Start Station ID","value":"1"},{"key":"Start Station ID","value":"3141"},{"key":"Start Station ID","value":"40.76500525"},{"key":"Start Station ID","value":"-73.95818491"},{"key":"Start Station ID","value":"1 Ave & E 68 St"},{"key":"Start Station ID","value":"2016-12-01 06:01:07"},{"key":"Start Station ID","value":"2016-12-01 06:14:23"},{"key":"Start Station ID","value":"796"},{"key":"Start Station ID","value":"Subscriber"}],[{"key":"Start Station Latitude","value":"22465"},{"key":"Start Station Latitude","value":"1974"},{"key":"Start Station Latitude","value":"327"},{"key":"Start Station Latitude","value":"40.7153379"},{"key":"Start Station Latitude","value":"-74.01658354"},{"key":"Start Station Latitude","value":"Vesey Pl & River Terrace"},{"key":"Start Station Latitude","value":"1"},{"key":"Start Station Latitude","value":"482"},{"key":"Start Station Latitude","value":"40.73935542"},{"key":"Start Station Latitude","value":"-73.99931783"},{"key":"Start Station Latitude","value":"W 15 St & 7 Ave"},{"key":"Start Station Latitude","value":"2016-12-01 06:01:32"},{"key":"Start Station Latitude","value":"2016-12-01 06:16:59"},{"key":"Start Station Latitude","value":"927"},{"key":"Start Station Latitude","value":"Subscriber"}],[{"key":"Start Station Longitude","value":"23329"},{"key":"Start Station Longitude","value":"1960"},{"key":"Start Station Longitude","value":"402"},{"key":"Start Station Longitude","value":"40.7403432"},{"key":"Start Station Longitude","value":"-73.98955109"},{"key":"Start Station Longitude","value":"Broadway & E 22 St"},{"key":"Start Station Longitude","value":"1"},{"key":"Start Station Longitude","value":"326"},{"key":"Start Station Longitude","value":"40.72953837"},{"key":"Start Station Longitude","value":"-73.98426726"},{"key":"Start Station Longitude","value":"E 11 St & 1 Ave"},{"key":"Start Station Longitude","value":"2016-12-01 06:01:36"},{"key":"Start Station Longitude","value":"2016-12-01 06:09:51"},{"key":"Start Station Longitude","value":"494"},{"key":"Start Station Longitude","value":"Subscriber"}],[{"key":"Start Station Name","value":"23018"},{"key":"Start Station Name","value":"1966"},{"key":"Start Station Name","value":"499"},{"key":"Start Station Name","value":"40.76915505"},{"key":"Start Station Name","value":"-73.98191841"},{"key":"Start Station Name","value":"Broadway & W 60 St"},{"key":"Start Station Name","value":"1"},{"key":"Start Station Name","value":"3233"},{"key":"Start Station Name","value":"40.75724567911726"},{"key":"Start Station Name","value":"-73.97805914282799"},{"key":"Start Station Name","value":"E 48 St & 5 Ave"},{"key":"Start Station Name","value":"2016-12-01 06:01:41"},{"key":"Start Station Name","value":"2016-12-01 06:09:22"},{"key":"Start Station Name","value":"461"},{"key":"Start Station Name","value":"Subscriber"}],[{"key":"Start Time","value":"19287"},{"key":"Start Time","value":"1985"},{"key":"Start Time","value":"536"},{"key":"Start Time","value":"40.74144387"},{"key":"Start Time","value":"-73.97536082"},{"key":"Start Time","value":"1 Ave & E 30 St"},{"key":"Start Time","value":"2"},{"key":"Start Time","value":"174"},{"key":"Start Time","value":"40.7381765"},{"key":"Start Time","value":"-73.97738662"},{"key":"Start Time","value":"E 25 St & 1 Ave"},{"key":"Start Time","value":"2016-12-01 06:01:53"},{"key":"Start Time","value":"2016-12-01 06:04:20"},{"key":"Start Time","value":"147"},{"key":"Start Time","value":"Subscriber"}],[{"key":"Stop Time","value":"24311"},{"key":"Stop Time","value":"1966"},{"key":"Stop Time","value":"477"},{"key":"Stop Time","value":"40.75640548"},{"key":"Stop Time","value":"-73.9900262"},{"key":"Stop Time","value":"W 41 St & 8 Ave"},{"key":"Stop Time","value":"1"},{"key":"Stop Time","value":"459"},{"key":"Stop Time","value":"40.746745"},{"key":"Stop Time","value":"-74.007756"},{"key":"Stop Time","value":"W 20 St & 11 Ave"},{"key":"Stop Time","value":"2016-12-01 06:02:02"},{"key":"Stop Time","value":"2016-12-01 06:12:00"},{"key":"Stop Time","value":"597"},{"key":"Stop Time","value":"Subscriber"}],[{"key":"Trip Duration","value":"22268"},{"key":"Trip Duration","value":"1973"},{"key":"Trip Duration","value":"388"},{"key":"Trip Duration","value":"40.749717753"},{"key":"Trip Duration","value":"-74.002950346"},{"key":"Trip Duration","value":"W 26 St & 10 Ave"},{"key":"Trip Duration","value":"0"},{"key":"Trip Duration","value":"3152"},{"key":"Trip Duration","value":"40.76873687"},{"key":"Trip Duration","value":"-73.96119945"},{"key":"Trip Duration","value":"3 Ave & E 71 St"},{"key":"Trip Duration","value":"2016-12-01 06:02:07"},{"key":"Trip Duration","value":"2016-12-01 06:33:23"},{"key":"Trip Duration","value":"1876"},{"key":"Trip Duration","value":"Subscriber"}],[{"key":"User Type","value":"25835"},{"key":"User Type","value":"1987"},{"key":"User Type","value":"324"},{"key":"User Type","value":"40.689888"},{"key":"User Type","value":"-73.981013"},{"key":"User Type","value":"DeKalb Ave & Hudson Ave"},{"key":"User Type","value":"1"},{"key":"User Type","value":"241"},{"key":"User Type","value":"40.68981035"},{"key":"User Type","value":"-73.97493121"},{"key":"User Type","value":"DeKalb Ave & S Portland Ave"},{"key":"User Type","value":"2016-12-01 06:02:35"},{"key":"User Type","value":"2016-12-01 06:04:45"},{"key":"User Type","value":"130"},{"key":"User Type","value":"Subscriber"}],[{"value":"19371"},{"value":"1985"},{"value":"324"},{"value":"40.689888"},{"value":"-73.981013"},{"value":"DeKalb Ave & Hudson Ave"},{"value":"1"},{"value":"270"},{"value":"40.69308257"},{"value":"-73.97178913"},{"value":"Adelphi St & Myrtle Ave"},{"value":"2016-12-01 06:02:58"},{"value":"2016-12-01 06:06:30"},{"value":"212"},{"value":"Subscriber"}],[{"value":"23530"},{"value":"1970"},{"value":"352"},{"value":"40.76340613"},{"value":"-73.97722479"},{"value":"W 56 St & 6 Ave"},{"value":"1"},{"value":"477"},{"value":"40.75640548"},{"value":"-73.9900262"},{"value":"W 41 St & 8 Ave"},{"value":"2016-12-01 06:03:02"},{"value":"2016-12-01 06:10:39"},{"value":"457"},{"value":"Subscriber"}],[{"value":"22114"},{"value":"1977"},{"value":"456"},{"value":"40.7597108"},{"value":"-73.97402311"},{"value":"E 53 St & Madison Ave"},{"value":"1"},{"value":"477"},{"value":"40.75640548"},{"value":"-73.9900262"},{"value":"W 41 St & 8 Ave"},{"value":"2016-12-01 06:03:03"},{"value":"2016-12-01 06:10:06"},{"value":"423"},{"value":"Subscriber"}],[{"value":"22972"},{"value":"1984"},{"value":"2000"},{"value":"40.70255088"},{"value":"-73.98940236"},{"value":"Front St & Washington St"},{"value":"1"},{"value":"420"},{"value":"40.68764484"},{"value":"-73.96968902"},{"value":"Clermont Ave & Lafayette Ave"},{"value":"2016-12-01 06:03:03"},{"value":"2016-12-01 06:14:53"},{"value":"709"},{"value":"Subscriber"}],[{"value":"17243"},{"value":"1972"},{"value":"477"},{"value":"40.75640548"},{"value":"-73.9900262"},{"value":"W 41 St & 8 Ave"},{"value":"1"},{"value":"3236"},{"value":"40.75898481399634"},{"value":"-73.99379968643188"},{"value":"W 42 St & Dyer Ave"},{"value":"2016-12-01 06:04:02"},{"value":"2016-12-01 06:06:44"},{"value":"162"},{"value":"Subscriber"}],[{"value":"22983"},{"value":"1996"},{"value":"2012"},{"value":"40.739445"},{"value":"-73.976806"},{"value":"E 27 St & 1 Ave"},{"value":"2"},{"value":"519"},{"value":"40.751873"},{"value":"-73.977706"},{"value":"Pershing Square North"},{"value":"2016-12-01 06:04:03"},{"value":"2016-12-01 06:12:23"},{"value":"500"},{"value":"Subscriber"}],[{"value":"16669"},{"value":"1981"},{"value":"480"},{"value":"40.76669671"},{"value":"-73.99061728"},{"value":"W 53 St & 10 Ave"},{"value":"1"},{"value":"447"},{"value":"40.76370739"},{"value":"-73.9851615"},{"value":"8 Ave & W 52 St"},{"value":"2016-12-01 06:00:01"},{"value":"2016-12-01 06:03:36"},{"value":"214"},{"value":"Subscriber"}],[{"value":"25077"},{"value":"1992"},{"value":"387"},{"value":"40.71273266"},{"value":"-74.0046073"},{"value":"Centre St & Chambers St"},{"value":"1"},{"value":"507"},{"value":"40.73912601"},{"value":"-73.97973776"},{"value":"E 25 St & 2 Ave"},{"value":"2016-12-01 06:00:13"},{"value":"2016-12-01 06:14:22"},{"value":"848"},{"value":"Subscriber"}],[{"value":"22680"},{"value":"1975"},{"value":"468"},{"value":"40.7652654"},{"value":"-73.98192338"},{"value":"Broadway & W 55 St"},{"value":"1"},{"value":"3233"},{"value":"40.75724567911726"},{"value":"-73.97805914282799"},{"value":"E 48 St & 5 Ave"},{"value":"2016-12-01 06:00:16"},{"value":"2016-12-01 06:05:28"},{"value":"312"},{"value":"Subscriber"}],[{"value":"27180"},{"value":"1980"},{"value":"337"},{"value":"40.7037992"},{"value":"-74.00838676"},{"value":"Old Slip & Front St"},{"value":"1"},{"value":"264"},{"value":"40.70706456"},{"value":"-74.00731853"},{"value":"Maiden Ln & Pearl St"},{"value":"2016-12-01 06:00:52"},{"value":"2016-12-01 06:03:04"},{"value":"131"},{"value":"Subscriber"}],[{"value":"26385"},{"value":"1950"},{"value":"264"},{"value":"40.70706456"},{"value":"-74.00731853"},{"value":"Maiden Ln & Pearl St"},{"value":"1"},{"value":"3255"},{"value":"40.7505853470215"},{"value":"-73.9946848154068"},{"value":"8 Ave & W 31 St"},{"value":"2016-12-01 06:01:00"},{"value":"2016-12-01 06:19:39"},{"value":"1119"},{"value":"Subscriber"}],[{"value":"21243"},{"value":"1959"},{"value":"536"},{"value":"40.74144387"},{"value":"-73.97536082"},{"value":"1 Ave & E 30 St"},{"value":"1"},{"value":"477"},{"value":"40.75640548"},{"value":"-73.9900262"},{"value":"W 41 St & 8 Ave"},{"value":"2016-12-01 06:01:05"},{"value":"2016-12-01 06:11:17"},{"value":"611"},{"value":"Subscriber"}],[{"value":"24112"},{"value":"1974"},{"value":"497"},{"value":"40.73704984"},{"value":"-73.99009296"},{"value":"E 17 St & Broadway"},{"value":"1"},{"value":"3255"},{"value":"40.7505853470215"},{"value":"-73.9946848154068"},{"value":"8 Ave & W 31 St"},{"value":"2016-12-01 06:01:13"},{"value":"2016-12-01 06:10:42"},{"value":"569"},{"value":"Subscriber"}],[{"value":"25619"},{"value":"1970"},{"value":"366"},{"value":"40.693261"},{"value":"-73.968896"},{"value":"Clinton Ave & Myrtle Ave"},{"value":"2"},{"value":"366"},{"value":"40.693261"},{"value":"-73.968896"},{"value":"Clinton Ave & Myrtle Ave"},{"value":"2016-12-01 06:01:16"},{"value":"2016-12-01 06:42:29"},{"value":"2473"},{"value":"Subscriber"}],[{"value":"23455"},{"value":"1997"},{"value":"440"},{"value":"40.75255434"},{"value":"-73.97282625"},{"value":"E 45 St & 3 Ave"},{"value":"1"},{"value":"529"},{"value":"40.7575699"},{"value":"-73.99098507"},{"value":"W 42 St & 8 Ave"},{"value":"2016-12-01 06:01:19"},{"value":"2016-12-01 06:07:21"},{"value":"362"},{"value":"Subscriber"}],[{"value":"25537"},{"value":"1985"},{"value":"527"},{"value":"40.744023"},{"value":"-73.976056"},{"value":"E 33 St & 2 Ave"},{"value":"1"},{"value":"492"},{"value":"40.75019995"},{"value":"-73.99093085"},{"value":"W 33 St & 7 Ave"},{"value":"2016-12-01 06:01:29"},{"value":"2016-12-01 06:07:58"},{"value":"388"},{"value":"Subscriber"}],[{"value":"24973"},{"value":"1956"},{"value":"514"},{"value":"40.76087502"},{"value":"-74.00277668"},{"value":"12 Ave & W 40 St"},{"value":"1"},{"value":"3255"},{"value":"40.7505853470215"},{"value":"-73.9946848154068"},{"value":"8 Ave & W 31 St"},{"value":"2016-12-01 06:02:12"},{"value":"2016-12-01 06:11:22"},{"value":"549"},{"value":"Subscriber"}],[{"value":"19071"},{"value":"1989"},{"value":"3132"},{"value":"40.76350532"},{"value":"-73.97109243"},{"value":"E 59 St & Madison Ave"},{"value":"1"},{"value":"461"},{"value":"40.73587678"},{"value":"-73.98205027"},{"value":"E 20 St & 2 Ave"},{"value":"2016-12-01 06:02:19"},{"value":"2016-12-01 06:17:02"},{"value":"883"},{"value":"Subscriber"}],[{"value":"17928"},{"value":"1976"},{"value":"324"},{"value":"40.689888"},{"value":"-73.981013"},{"value":"DeKalb Ave & Hudson Ave"},{"value":"1"},{"value":"416"},{"value":"40.68753406"},{"value":"-73.97265183"},{"value":"Cumberland St & Lafayette Ave"},{"value":"2016-12-01 06:02:27"},{"value":"2016-12-01 06:05:50"},{"value":"203"},{"value":"Subscriber"}],[{"value":"17389"},{"value":"1966"},{"value":"527"},{"value":"40.744023"},{"value":"-73.976056"},{"value":"E 33 St & 2 Ave"},{"value":"1"},{"value":"490"},{"value":"40.751551"},{"value":"-73.993934"},{"value":"8 Ave & W 33 St"},{"value":"2016-12-01 06:02:44"},{"value":"2016-12-01 06:13:51"},{"value":"666"},{"value":"Subscriber"}],[{"value":"23610"},{"value":"1957"},{"value":"456"},{"value":"40.7597108"},{"value":"-73.97402311"},{"value":"E 53 St & Madison Ave"},{"value":"1"},{"value":"423"},{"value":"40.76584941"},{"value":"-73.98690506"},{"value":"W 54 St & 9 Ave"},{"value":"2016-12-01 06:02:48"},{"value":"2016-12-01 06:08:02"},{"value":"314"},{"value":"Subscriber"}],[{"value":"19465"},{"value":"1971"},{"value":"3172"},{"value":"40.7785669"},{"value":"-73.97754961"},{"value":"W 74 St & Columbus Ave"},{"value":"1"},{"value":"357"},{"value":"40.73261787"},{"value":"-73.99158043"},{"value":"E 11 St & Broadway"},{"value":"2016-12-01 06:02:58"},{"value":"2016-12-01 06:31:50"},{"value":"1731"},{"value":"Subscriber"}],[{"value":"22961"},{"value":"1974"},{"value":"486"},{"value":"40.7462009"},{"value":"-73.98855723"},{"value":"Broadway & W 29 St"},{"value":"1"},{"value":"326"},{"value":"40.72953837"},{"value":"-73.98426726"},{"value":"E 11 St & 1 Ave"},{"value":"2016-12-01 06:03:27"},{"value":"2016-12-01 06:12:31"},{"value":"543"},{"value":"Subscriber"}],[{"value":"27034"},{"value":"1984"},{"value":"458"},{"value":"40.751396"},{"value":"-74.005226"},{"value":"11 Ave & W 27 St"},{"value":"1"},{"value":"405"},{"value":"40.739323"},{"value":"-74.008119"},{"value":"Washington St & Gansevoort St"},{"value":"2016-12-01 06:03:57"},{"value":"2016-12-01 06:10:20"},{"value":"382"},{"value":"Subscriber"}]],"columnNames":[{"name":"Bike ID","index":0,"aggr":"sum"},{"name":"Birth Year","index":1,"aggr":"sum"},{"name":"End Station ID","index":2,"aggr":"sum"},{"name":"End Station Latitude","index":3,"aggr":"sum"},{"name":"End Station Longitude","index":4,"aggr":"sum"},{"name":"End Station Name","index":5,"aggr":"sum"},{"name":"Gender","index":6,"aggr":"sum"},{"name":"Start Station ID","index":7,"aggr":"sum"},{"name":"Start Station Latitude","index":8,"aggr":"sum"},{"name":"Start Station Longitude","index":9,"aggr":"sum"},{"name":"Start Station Name","index":10,"aggr":"sum"},{"name":"Start Time","index":11,"aggr":"sum"},{"name":"Stop Time","index":12,"aggr":"sum"},{"name":"Trip Duration","index":13,"aggr":"sum"},{"name":"User Type","index":14,"aggr":"sum"}],"rows":[["20578","1964","3141","40.76500525","-73.95818491","1 Ave & E 68 St","1","490","40.751551","-73.993934","8 Ave & W 33 St","2016-12-01 06:00:08","2016-12-01 06:27:10","1622","Subscriber"],["25033","1958","497","40.73704984","-73.99009296","E 17 St & Broadway","1","3236","40.75898481399634","-73.99379968643188","W 42 St & Dyer Ave","2016-12-01 06:00:10","2016-12-01 06:22:39","1349","Subscriber"],["22604","1992","297","40.734232","-73.986923","E 15 St & 3 Ave","1","394","40.72521311","-73.97768752","E 9 St & Avenue C","2016-12-01 06:00:11","2016-12-01 06:07:59","467","Subscriber"],["24035","1958","527","40.744023","-73.976056","E 33 St & 2 Ave","1","492","40.75019995","-73.99093085","W 33 St & 7 Ave","2016-12-01 06:00:21","2016-12-01 06:05:54","333","Subscriber"],["24295","1966","3224","40.73997354103409","-74.00513872504234","W 13 St & Hudson St","1","3161","40.7801839724239","-73.97728532552719","W 76 St & Columbus Ave","2016-12-01 06:00:30","2016-12-01 06:25:04","1473","Subscriber"],["21229","1979","518","40.74780373","-73.9734419","E 39 St & 2 Ave","1","468","40.7652654","-73.98192338","Broadway & W 55 St","2016-12-01 06:00:33","2016-12-01 06:14:10","817","Subscriber"],["25056","1991","461","40.73587678","-73.98205027","E 20 St & 2 Ave","1","3141","40.76500525","-73.95818491","1 Ave & E 68 St","2016-12-01 06:01:07","2016-12-01 06:14:23","796","Subscriber"],["22465","1974","327","40.7153379","-74.01658354","Vesey Pl & River Terrace","1","482","40.73935542","-73.99931783","W 15 St & 7 Ave","2016-12-01 06:01:32","2016-12-01 06:16:59","927","Subscriber"],["23329","1960","402","40.7403432","-73.98955109","Broadway & E 22 St","1","326","40.72953837","-73.98426726","E 11 St & 1 Ave","2016-12-01 06:01:36","2016-12-01 06:09:51","494","Subscriber"],["23018","1966","499","40.76915505","-73.98191841","Broadway & W 60 St","1","3233","40.75724567911726","-73.97805914282799","E 48 St & 5 Ave","2016-12-01 06:01:41","2016-12-01 06:09:22","461","Subscriber"],["19287","1985","536","40.74144387","-73.97536082","1 Ave & E 30 St","2","174","40.7381765","-73.97738662","E 25 St & 1 Ave","2016-12-01 06:01:53","2016-12-01 06:04:20","147","Subscriber"],["24311","1966","477","40.75640548","-73.9900262","W 41 St & 8 Ave","1","459","40.746745","-74.007756","W 20 St & 11 Ave","2016-12-01 06:02:02","2016-12-01 06:12:00","597","Subscriber"],["22268","1973","388","40.749717753","-74.002950346","W 26 St & 10 Ave","0","3152","40.76873687","-73.96119945","3 Ave & E 71 St","2016-12-01 06:02:07","2016-12-01 06:33:23","1876","Subscriber"],["25835","1987","324","40.689888","-73.981013","DeKalb Ave & Hudson Ave","1","241","40.68981035","-73.97493121","DeKalb Ave & S Portland Ave","2016-12-01 06:02:35","2016-12-01 06:04:45","130","Subscriber"],["19371","1985","324","40.689888","-73.981013","DeKalb Ave & Hudson Ave","1","270","40.69308257","-73.97178913","Adelphi St & Myrtle Ave","2016-12-01 06:02:58","2016-12-01 06:06:30","212","Subscriber"],["23530","1970","352","40.76340613","-73.97722479","W 56 St & 6 Ave","1","477","40.75640548","-73.9900262","W 41 St & 8 Ave","2016-12-01 06:03:02","2016-12-01 06:10:39","457","Subscriber"],["22114","1977","456","40.7597108","-73.97402311","E 53 St & Madison Ave","1","477","40.75640548","-73.9900262","W 41 St & 8 Ave","2016-12-01 06:03:03","2016-12-01 06:10:06","423","Subscriber"],["22972","1984","2000","40.70255088","-73.98940236","Front St & Washington St","1","420","40.68764484","-73.96968902","Clermont Ave & Lafayette Ave","2016-12-01 06:03:03","2016-12-01 06:14:53","709","Subscriber"],["17243","1972","477","40.75640548","-73.9900262","W 41 St & 8 Ave","1","3236","40.75898481399634","-73.99379968643188","W 42 St & Dyer Ave","2016-12-01 06:04:02","2016-12-01 06:06:44","162","Subscriber"],["22983","1996","2012","40.739445","-73.976806","E 27 St & 1 Ave","2","519","40.751873","-73.977706","Pershing Square North","2016-12-01 06:04:03","2016-12-01 06:12:23","500","Subscriber"],["16669","1981","480","40.76669671","-73.99061728","W 53 St & 10 Ave","1","447","40.76370739","-73.9851615","8 Ave & W 52 St","2016-12-01 06:00:01","2016-12-01 06:03:36","214","Subscriber"],["25077","1992","387","40.71273266","-74.0046073","Centre St & Chambers St","1","507","40.73912601","-73.97973776","E 25 St & 2 Ave","2016-12-01 06:00:13","2016-12-01 06:14:22","848","Subscriber"],["22680","1975","468","40.7652654","-73.98192338","Broadway & W 55 St","1","3233","40.75724567911726","-73.97805914282799","E 48 St & 5 Ave","2016-12-01 06:00:16","2016-12-01 06:05:28","312","Subscriber"],["27180","1980","337","40.7037992","-74.00838676","Old Slip & Front St","1","264","40.70706456","-74.00731853","Maiden Ln & Pearl St","2016-12-01 06:00:52","2016-12-01 06:03:04","131","Subscriber"],["26385","1950","264","40.70706456","-74.00731853","Maiden Ln & Pearl St","1","3255","40.7505853470215","-73.9946848154068","8 Ave & W 31 St","2016-12-01 06:01:00","2016-12-01 06:19:39","1119","Subscriber"],["21243","1959","536","40.74144387","-73.97536082","1 Ave & E 30 St","1","477","40.75640548","-73.9900262","W 41 St & 8 Ave","2016-12-01 06:01:05","2016-12-01 06:11:17","611","Subscriber"],["24112","1974","497","40.73704984","-73.99009296","E 17 St & Broadway","1","3255","40.7505853470215","-73.9946848154068","8 Ave & W 31 St","2016-12-01 06:01:13","2016-12-01 06:10:42","569","Subscriber"],["25619","1970","366","40.693261","-73.968896","Clinton Ave & Myrtle Ave","2","366","40.693261","-73.968896","Clinton Ave & Myrtle Ave","2016-12-01 06:01:16","2016-12-01 06:42:29","2473","Subscriber"],["23455","1997","440","40.75255434","-73.97282625","E 45 St & 3 Ave","1","529","40.7575699","-73.99098507","W 42 St & 8 Ave","2016-12-01 06:01:19","2016-12-01 06:07:21","362","Subscriber"],["25537","1985","527","40.744023","-73.976056","E 33 St & 2 Ave","1","492","40.75019995","-73.99093085","W 33 St & 7 Ave","2016-12-01 06:01:29","2016-12-01 06:07:58","388","Subscriber"],["24973","1956","514","40.76087502","-74.00277668","12 Ave & W 40 St","1","3255","40.7505853470215","-73.9946848154068","8 Ave & W 31 St","2016-12-01 06:02:12","2016-12-01 06:11:22","549","Subscriber"],["19071","1989","3132","40.76350532","-73.97109243","E 59 St & Madison Ave","1","461","40.73587678","-73.98205027","E 20 St & 2 Ave","2016-12-01 06:02:19","2016-12-01 06:17:02","883","Subscriber"],["17928","1976","324","40.689888","-73.981013","DeKalb Ave & Hudson Ave","1","416","40.68753406","-73.97265183","Cumberland St & Lafayette Ave","2016-12-01 06:02:27","2016-12-01 06:05:50","203","Subscriber"],["17389","1966","527","40.744023","-73.976056","E 33 St & 2 Ave","1","490","40.751551","-73.993934","8 Ave & W 33 St","2016-12-01 06:02:44","2016-12-01 06:13:51","666","Subscriber"],["23610","1957","456","40.7597108","-73.97402311","E 53 St & Madison Ave","1","423","40.76584941","-73.98690506","W 54 St & 9 Ave","2016-12-01 06:02:48","2016-12-01 06:08:02","314","Subscriber"],["19465","1971","3172","40.7785669","-73.97754961","W 74 St & Columbus Ave","1","357","40.73261787","-73.99158043","E 11 St & Broadway","2016-12-01 06:02:58","2016-12-01 06:31:50","1731","Subscriber"],["22961","1974","486","40.7462009","-73.98855723","Broadway & W 29 St","1","326","40.72953837","-73.98426726","E 11 St & 1 Ave","2016-12-01 06:03:27","2016-12-01 06:12:31","543","Subscriber"],["27034","1984","458","40.751396","-74.005226","11 Ave & W 27 St","1","405","40.739323","-74.008119","Washington St & Gansevoort St","2016-12-01 06:03:57","2016-12-01 06:10:20","382","Subscriber"]]},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:18:33 PM","dateFinished":"Jun 19, 2017 5:18:37 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:229"},{"title":"More SQL with realtime_bike_trips","text":"%sql\nselect count(*) cnt, min(`Start Time`) minTime,\nmax(`Start Time`) maxTime from realtime_bike_trips","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:13:33 PM","config":{"colWidth":4,"editorMode":"ace/mode/sql","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"cnt","index":0,"aggr":"sum"}],"values":[{"name":"minTime","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"cnt","index":0,"aggr":"sum"},"yAxis":{"name":"minTime","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429188_-1449950103","id":"20170421-104947_1434606148","result":{"code":"SUCCESS","type":"TABLE","msg":"cnt\tminTime\tmaxTime\n45\t2016-12-01 06:04:18\t2016-12-01 06:09:16\n","comment":"","msgTable":[[{"key":"minTime","value":"45"},{"key":"minTime","value":"2016-12-01 06:04:18"},{"key":"minTime","value":"2016-12-01 06:09:16"}]],"columnNames":[{"name":"cnt","index":0,"aggr":"sum"},{"name":"minTime","index":1,"aggr":"sum"},{"name":"maxTime","index":2,"aggr":"sum"}],"rows":[["45","2016-12-01 06:04:18","2016-12-01 06:09:16"]]},"dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:13:33 PM","dateFinished":"Jun 19, 2017 5:13:35 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:230"},{"title":"Spark code to stop the running Spark Streaming consumer ","text":"%spark\n{\n    import org.apache.spark.streaming._\n    println(\"Stopping any active StreamingContext.  May take a minute.\")\n    StreamingContext.getActive().map(_.stop(false,true))\n    println(\"done\")\n}","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:27:16 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429188_-1449950103","id":"20170419-115308_1919654685","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:27:16 PM","dateFinished":"Jun 19, 2017 5:27:16 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:231","errorMessage":"","focus":true},{"title":"Shell command to list the Object Store contents","text":"%sh\n. oehcs.sh\necho Storage Container = $ObjectStorage_Container\n\necho \"Observe the timestamps of the files.  You should see files with today's timestamps added by the more complex streaming example\"\n\nhadoop fs -ls swift://$ObjectStorage_Container.default/bikes_realtime 2>&1","authenticationInfo":{},"dateUpdated":"Jun 19, 2017 5:26:54 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"container":"citibike"},"forms":{}},"jobName":"paragraph_1497880429188_-1449950103","id":"20170421-105528_683135750","dateCreated":"Jun 19, 2017 9:53:49 AM","dateStarted":"Jun 19, 2017 5:26:54 PM","dateFinished":"Jun 19, 2017 5:26:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:232","errorMessage":"","focus":true},{"text":"%md\n#Next Steps\n\nLearn more about:\n + Oracle Event Hub Cloud Service: <http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html>\n + Spark Streaming: <http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html>\n + Apache Kafka: <https://kafka.apache.org/>\n + Spark Streaming and Kafka Integration: <https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html>\n \nRun the Demonstration Citi Bike Live Map with OEHCS and Spark Streaming\n","dateUpdated":"Jun 20, 2017 9:31:56 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429188_-1449950103","id":"20170421-095706_230718086","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Next Steps</h1>\n<p>Learn more about:</p>\n<ul>\n<li>Oracle Event Hub Cloud Service: <a href=\"http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html\">http://docs.oracle.com/cloud/latest/event-hub-cloud/index.html</a></li>\n<li>Spark Streaming: <a href=\"http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html\">http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html</a></li>\n<li>Apache Kafka: <a href=\"https://kafka.apache.org/\">https://kafka.apache.org/</a></li>\n<li>Spark Streaming and Kafka Integration: <a href=\"https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\">https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html</a></li>\n</ul>\n<p>Run the Demonstration Citi Bike Live Map with OEHCS and Spark Streaming</p>\n"},"dateCreated":"Jun 19, 2017 9:53:49 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:233","authenticationInfo":{},"dateFinished":"Jun 20, 2017 9:31:51 AM","dateStarted":"Jun 20, 2017 9:31:51 AM","focus":true},{"title":"Bonus: Python Spark Streaming Consumer Example","text":"%pyspark\n\n\nimport json\n\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kafka import KafkaUtils\n\nssc = StreamingContext(sc, 10)\n\ntopic = z.get('BIND_OEHCS_Topic')\n\nbrokers = z.get('BIND_OEHCS_ConnectionDescriptor')\n\ndirectKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\"metadata.broker.list\": brokers})\n\ndef process(rdd):\n    try:\n        df = sqlContext.jsonRDD(rdd.map(lambda (_, value): value))\n        print 'count '+str(df.count())\n\n        # df.write.parquet('swift://Journey2.default/bikes', mode='append')\n    except ValueError:\n        pass\n\ndirectKafkaStream.foreachRDD(process)\n\nssc.start()\n\n\n# when you want this to stop, run the spark paragraph to stop any running StreamingContexts.  The one with: StreamingContext.getActive().map(_.stop(false,true))","dateUpdated":"Jun 19, 2017 9:53:49 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429188_-1449950103","id":"20170419-111151_5005124","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Jun 19, 2017 9:53:49 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:234"},{"dateUpdated":"Jun 19, 2017 9:53:49 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497880429188_-1449950103","id":"20170419-121108_1129923637","dateCreated":"Jun 19, 2017 9:53:49 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:235"}],"name":" Tutorial 9 Working with OEHCS and Spark Streaming","id":"2CMRRRYE2","angularObjects":{"2CKYUH771":[],"2CHQCE7WB":[],"2CM73VNZC":[],"2CJ9M7X3E":[],"2CM9MFPDC":[{"name":"BIND_ObjectStorage_Container","object":"citibike","noteId":"2CMRRRYE2"},{"name":"BIND_OEHCS_ConnectionDescriptor","object":"141.144.29.130:6667","noteId":"2CMRRRYE2"},{"name":"BIND_OEHCS_Topic","object":"gse00010212-TutorialOEHCS","noteId":"2CMRRRYE2"}],"2CKUFFGB5":[],"2CH76629W":[],"2CJ6JMU49":[],"2CMM8Y2GW":[]},"config":{"looknfeel":"default"},"info":{}}