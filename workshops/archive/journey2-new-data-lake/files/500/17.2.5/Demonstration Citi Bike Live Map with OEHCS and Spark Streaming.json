{"paragraphs":[{"text":"%md\n#Citi Bike Live Map Demonstration\n\nThis demonstration was built for BDCS-CE version 17.2.5.  If you are using a later version of BDCS-CE, please look for a newer version of this tutorial notebook at <https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake>.  Questions and feedback about the tutorial: <david.bayard@oracle.com> or <john.wyant@oracle.com>\n\n**Contents**\n+ OEHCS Setup\n+ Preparing Bike Data for Streaming\n+ Writing a Producer to stream data to OEHCS\n+ Running the Live Map demonstration\n+ Next Steps\n\n\nAs a reminder, the documentation for BDCS-CE can be found here: <http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html>\n\n**NOTE: Please ensure that you have run the \"Working with Spark Interpreter\" and \"Working with OEHCS and Spark Streaming\" Tutorials first.**\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:14:12 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269102_-68466977","id":"20170417-194925_2070687465","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Citi Bike Live Map Demonstration</h1>\n<p>This demonstration was built for BDCS-CE version 17.2.5.  If you are using a later version of BDCS-CE, please look for a newer version of this tutorial notebook at <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\">https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake</a>.  Questions and feedback about the tutorial: <a href=\"&#x6d;a&#x69;&#x6c;&#116;&#111;&#58;&#100;&#97;&#x76;&#x69;&#x64;&#x2e;&#x62;&#97;&#121;&#x61;&#x72;&#x64;&#64;&#111;&#114;&#97;&#x63;&#x6c;&#101;&#x2e;&#99;&#111;&#x6d;\">&#100;&#97;&#x76;&#x69;&#x64;&#46;&#98;&#x61;&#121;&#x61;&#x72;&#100;&#64;&#111;&#x72;&#97;&#x63;&#108;&#x65;&#x2e;&#99;&#x6f;&#x6d;</a> or <a href=\"&#x6d;&#x61;&#105;&#108;&#x74;&#111;&#58;&#106;&#x6f;&#104;&#110;&#x2e;&#x77;&#121;&#x61;&#x6e;&#x74;&#64;&#111;&#114;&#97;&#99;&#x6c;&#x65;&#x2e;&#x63;&#x6f;&#x6d;\">&#x6a;&#111;&#x68;&#x6e;&#46;&#x77;&#x79;&#x61;&#110;&#116;&#64;&#111;&#x72;ac&#108;&#101;&#x2e;&#99;&#111;&#x6d;</a></p>\n<p><strong>Contents</strong></p>\n<ul>\n<li>OEHCS Setup</li>\n<li>Preparing Bike Data for Streaming</li>\n<li>Writing a Producer to stream data to OEHCS</li>\n<li>Running the Live Map demonstration</li>\n<li>Next Steps</li>\n</ul>\n<p>As a reminder, the documentation for BDCS-CE can be found here: <a href=\"http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\">http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html</a></p>\n<p><strong>NOTE: Please ensure that you have run the &ldquo;Working with Spark Interpreter&rdquo; and &ldquo;Working with OEHCS and Spark Streaming&rdquo; Tutorials first.</strong></p>\n"},"dateCreated":"Jun 19, 2017 4:47:49 PM","dateStarted":"Jun 20, 2017 9:14:09 AM","dateFinished":"Jun 20, 2017 9:14:09 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:209","focus":true},{"text":"%md\n#OEHCS Setup\n\nThis demonstration will use Oracle Event Hub Cloud Service (OEHCS) and Spark Streaming.  Be sure that you have completed the tutorial \"Working with OEHCS and Spark Streaming\" before running this demonstration.  That tutorial will help you setup connectivity to OEHCS, create a Kafka topic, and configure the Spark Streaming Kafka dependency.  You will need to enter the OEHCS Connection Descriptor and OEHCS Topic in the paragraph below and run it.","dateUpdated":"Jun 19, 2017 4:47:49 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269102_-68466977","id":"20170503-140655_2083705382","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>OEHCS Setup</h1>\n<p>This demonstration will use Oracle Event Hub Cloud Service (OEHCS) and Spark Streaming.  Be sure that you have completed the tutorial &ldquo;Working with OEHCS and Spark Streaming&rdquo; before running this demonstration.  That tutorial will help you setup connectivity to OEHCS, create a Kafka topic, and configure the Spark Streaming Kafka dependency.  You will need to enter the OEHCS Connection Descriptor and OEHCS Topic in the paragraph below and run it.</p>\n"},"dateCreated":"Jun 19, 2017 4:47:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:210"},{"title":"Parameters","text":"%spark\nz.angularBind(\"BIND_ObjectStorage_Container\", \"citibike\")\nz.angularBind(\"BIND_OEHCS_ConnectionDescriptor\", z.input(\"OEHCS_ConnectionDescriptor\",\"141.144.144.128:6667\"))\nz.angularBind(\"BIND_OEHCS_Topic\", z.input(\"OEHCS_Topic\",\"gse00010212-TutorialOEHCS\"))\n\n                         \nscala.tools.nsc.io.File(\"/var/lib/zeppelin/bikes_part3.sh\").writeAll(\n  \"export ObjectStorage_Container=\\\"\"+z.angular(\"BIND_ObjectStorage_Container\")+\"\\\"\\n\" +\n  \"export OEHCS_ConnectionDescriptor=\\\"\"+z.angular(\"BIND_OEHCS_ConnectionDescriptor\")+\"\\\"\\n\" +\n  \"export OEHCS_Topic=\\\"\"+z.angular(\"BIND_OEHCS_Topic\")+\"\\\"\\n\"\n)\nprintln(\"done\")\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:25:45 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"Top N":"10","OEHCS_Topic":"gse00010212-TutorialOEHCS","dayOfWeek":"All","ObjectStorage_Container":"citibike","OEHCS_ConnectionDescriptor":"141.144.144.128:6667","hourOfDay":"All","David":"rocks"},"forms":{"OEHCS_ConnectionDescriptor":{"name":"OEHCS_ConnectionDescriptor","displayName":"OEHCS_ConnectionDescriptor","type":"input","defaultValue":"141.144.144.128:6667","hidden":false},"OEHCS_Topic":{"name":"OEHCS_Topic","displayName":"OEHCS_Topic","type":"input","defaultValue":"gse00010212-TutorialOEHCS","hidden":false}}},"jobName":"paragraph_1497905269102_-68466977","id":"20170426-101707_919098031","result":{"code":"SUCCESS","type":"TEXT","msg":"done\n"},"dateCreated":"Jun 19, 2017 4:47:49 PM","dateStarted":"Jun 20, 2017 9:25:45 AM","dateFinished":"Jun 20, 2017 9:25:45 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:211","focus":true},{"text":"%md\n#Preparing Bike Data for Streaming\n\nWe will take our bike trip data and wrangle it into a format that will be easier to use with streaming.  Specifically, our current data provides 1 row of data which has both the start and end times of a bike trip.  We will wrangle the data into a new file such that the start of the trip is its own row and the end of the trip is also its own row.  And we will sort our new file by the appropriate event time for that row (start or end).  This will make it easier for our Kafka producer program to stream the data for us.  We will use Spark SQL to make the wrangling easy.  Finally, we'll write our new data to a container in the Object Store.\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 8:57:12 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269102_-68466977","id":"20170421-162727_412866557","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Preparing Bike Data for Streaming</h1>\n<p>We will take our bike trip data and wrangle it into a format that will be easier to use with streaming.  Specifically, our current data provides 1 row of data which has both the start and end times of a bike trip.  We will wrangle the data into a new file such that the start of the trip is its own row and the end of the trip is also its own row.  And we will sort our new file by the appropriate event time for that row (start or end).  This will make it easier for our Kafka producer program to stream the data for us.  We will use Spark SQL to make the wrangling easy.  Finally, we'll write our new data to a container in the Object Store.</p>\n"},"dateCreated":"Jun 19, 2017 4:47:49 PM","dateStarted":"Jun 20, 2017 8:57:09 AM","dateFinished":"Jun 20, 2017 8:57:09 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:212","focus":true},{"title":"Spark code to prepare data","text":"%spark\n\n//a previous tutorial placed the csv file into your Object Store citibike container\n//notice the use of the swift://CONTAINER.defaut/ syntax\n\nval df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"swift://citibike.default/201612-citibike-tripdata.csv\")\n\n//cache the data frame for performance\ndf.cache()\n\n\nprintln(\"Here is the schema detected from the CSV\")\ndf.printSchema()\nprintln(\"..\")\n\nprintln(\"# of rows: %s\".format(\n  df.count() \n)) \nprintln(\"..\")\n\ndf.registerTempTable(\"bike_trips_temp\")\n\n\nprintln(\"Wrangling the existing data into a new dataframe\")\n// create a new DataFrame that creates separate rows for start and end events\nval df = sqlContext.sql(s\"\"\"select b.`Start Time` EventTime, \"Pickup\" EventType, \n    case when b.gender=1 then 'Male' when b.gender=2 then 'Female' else 'unknown' end GenderStr, b.* from bike_trips_temp b\nunion all\nselect b.`Stop Time` EventTime, \"Dropoff\" EventType, \n    case when b.gender=1 then 'Male' when b.gender=2 then 'Female' else 'unknown' end GenderStr, b.* from bike_trips_temp b\norder by EventTime\"\"\")\n\nprintln(\"Writing new data to Object Store\")\n//write the new data frame out as a csv file\ndf.repartition(1).write.format(\"com.databricks.spark.csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"swift://\"+z.angular(\"BIND_ObjectStorage_Container\")+\".default/bike_events\")\n\nprintln(\"done\")\n","authenticationInfo":{},"dateUpdated":"Jun 20, 2017 9:25:54 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269103_-68851725","id":"20170421-163037_1592342947","dateCreated":"Jun 19, 2017 4:47:49 PM","dateStarted":"Jun 20, 2017 8:54:54 AM","dateFinished":"Jun 20, 2017 8:56:53 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:213","errorMessage":"","focus":true},{"title":"Shell script to get a local copy of our new datafile","text":"%sh\n# This script copies the new dataset we created to the local file system, where our producer script is expecting it to reside.\n\n. bikes_part3.sh\necho Object Store Container = $ObjectStorage_Container\n\n\nhadoop fs -ls swift://$ObjectStorage_Container.default/bike_events 2>&1\n\nmkdir bikes\ncd bikes\nrm bike_events.csv\n\nhadoop fs -get swift://$ObjectStorage_Container.default/bike_events/part-00000 bike_events.csv 2>&1\n\nls -l bike_events.csv\nhead bike_events.csv\n\necho \"done\"\n\n","dateUpdated":"Jun 20, 2017 8:57:45 AM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269103_-68851725","id":"20170421-171627_570636554","dateCreated":"Jun 19, 2017 4:47:49 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:214","authenticationInfo":{},"dateFinished":"Jun 20, 2017 8:58:03 AM","dateStarted":"Jun 20, 2017 8:57:45 AM","focus":true},{"text":"%md\n#Writing a Producer to stream data to OEHCS\n\nFor this demonstration, we will replay historical bike pickup and dropoff data back in \"real-time\".  In other words, if we start the clock at 8am and our first bike is picked up at 8:00:14, our producer program will send the pickup event 14 seconds after it is ready to begin.  If the next bike event is at 8:00:25, the producer will wait 11 more seconds before sending that event.  We also have a \"time acceleration\" parameter we can use to speed up how fast our producer replays the historical data.\n\nOur producer program is written in python.  It will stream to OEHCS via the \"kafka-python\" library, as described here: <https://github.com/dpkp/kafka-python>\n","dateUpdated":"Jun 19, 2017 4:47:49 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269103_-68851725","id":"20170421-163756_1232378663","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Writing a Producer to stream data to OEHCS</h1>\n<p>For this demonstration, we will replay historical bike pickup and dropoff data back in &ldquo;real-time&rdquo;.  In other words, if we start the clock at 8am and our first bike is picked up at 8:00:14, our producer program will send the pickup event 14 seconds after it is ready to begin.  If the next bike event is at 8:00:25, the producer will wait 11 more seconds before sending that event.  We also have a &ldquo;time acceleration&rdquo; parameter we can use to speed up how fast our producer replays the historical data.</p>\n<p>Our producer program is written in python.  It will stream to OEHCS via the &ldquo;kafka-python&rdquo; library, as described here: <a href=\"https://github.com/dpkp/kafka-python\">https://github.com/dpkp/kafka-python</a></p>\n"},"dateCreated":"Jun 19, 2017 4:47:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:215"},{"title":"Shell command to save our Python program to a script","text":"%sh\n. bikes_part3.sh\n\necho \"\n#!/usr/bin/env python\n\n# standard libraries\nimport os\nimport sys\nimport csv\nimport json\nfrom time import sleep\nimport datetime as dt\n\n# Quality of Life Utils\nimport dateutil.parser\n\n# kafka-python libraries\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\n\nif len(sys.argv) < 6:\n\tprint 'usage: tutorial_kafka.py [inputfile:/path/to/file.csv] [acceleration-factor:integer] [recordcount:int-0 is infinite] [starttime:YYYY-MM-DD hh:mm:ss]'\n\tsys.exit(1)\n\ninputfile = sys.argv[1]\naccelerator = float(sys.argv[2])\nrecordcount = int(sys.argv[3])\ninputstarttime = dateutil.parser.parse(' '.join(sys.argv[4:]))\n\nscriptstarttime = dt.datetime.now()\nrelativestarttimedelta = inputstarttime - scriptstarttime\n\ndef timedelta_total_seconds(timedelta):\n    return (\n        timedelta.microseconds + 0.0 +\n        (timedelta.seconds + timedelta.days * 24 * 3600) * 10 ** 6) / 10 ** 6\n\n\n# When this baby hits 88mph, you're going to see some serious stuff.\ndef delorean(accelerator, scriptstarttime):\n\tnowtime = dt.datetime.now()\n\treturn scriptstarttime + dt.timedelta(seconds=accelerator*timedelta_total_seconds(nowtime - scriptstarttime))\n\n\n# Kafka Stuff\n# put your broker hostname:port in single quotes inside those bracketse\nmykafkaservers = ['$OEHCS_ConnectionDescriptor']\nproducer = KafkaProducer(bootstrap_servers=mykafkaservers, value_serializer=lambda m: json.dumps(m).encode('utf-8'))\n\n\ni = 0\n# Reader Loop\nwith open(inputfile) as csvfile:\n\treader = csv.DictReader(csvfile)\n\tfor rec in reader:\n\t\teventtime = dateutil.parser.parse(rec['EventTime'])\n\t\tif eventtime < inputstarttime:\n\t\t\tcontinue\n\t\trelativenowtime = delorean(accelerator, scriptstarttime) + relativestarttimedelta\n\t\twhile (relativenowtime < eventtime):\n\t\t\trelativenowtime = delorean(accelerator, scriptstarttime) + relativestarttimedelta\n\t\t\t# print relativenowtime, eventtime\n\t\t\tsleep(0.1)\n\t\t#print str(i)+'/'+str(recordcount)+' sending: ', rec\n\t\tprint str(i)+'/'+str(recordcount)\n\t\tproducer.send('$OEHCS_Topic', rec)\n\t\ti += 1\n\t\tif i >= recordcount and recordcount != 0:\n\t\t\tbreak\n\" > citibike_kafka.py\n\nls -l citibike_kafka.py\necho \"done\"","dateUpdated":"Jun 20, 2017 8:59:27 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"OEHCS_Topic":"gse00010212-TutorialOEHCS","OEHCS_ConnectionDescriptor":"141.144.144.128:6667"},"forms":{}},"jobName":"paragraph_1497905269103_-68851725","id":"20170421-163807_533257721","dateCreated":"Jun 19, 2017 4:47:49 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216","authenticationInfo":{},"dateFinished":"Jun 20, 2017 8:59:27 AM","dateStarted":"Jun 20, 2017 8:59:27 AM","focus":true},{"text":"%md\n#Running the Live Map demonstration\n\nAssuming you have run the above paragraphs, we are now ready to run our Live Map demonstration.  This demonstration will show a live map showing the latest pickups and dropoffs.  Pickups will be shown with green markers with a green line indicating the direction where the bike will eventually be dropped off (since we are replaying historical data, we conviently know the final dropoff location!).  Dropoff are shown in red with the line indicating where the bike came from.  Longer lines indicate longer trips.\n\n\nTo do run the demo,\n + **Run the Spark Streaming paragraph below**.  This will start a Spark Streaming session with a 5 second window.  As new data arrives, it will update the map (below).  This session will run for a few minutes before stopping itself.\n + Be sure to **also run the Producer paragraph below** otherwise there will be no data for the Spark Streaming session to see.  \n + With both the Spark Streaming and Producer running, **watch the output of those paragraphs as well as the map paragraph** to see changes.\n\n \nThe code editors for the next three paragraphs are hidden by default to make it simpler to run, but definitely show the code editors to see how the code is written.\n","dateUpdated":"Jun 20, 2017 9:03:16 AM","config":{"colWidth":12,"editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269104_-58463505","id":"20170421-164116_456910782","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Running the Live Map demonstration</h1>\n<p>Assuming you have run the above paragraphs, we are now ready to run our Live Map demonstration.  This demonstration will show a live map showing the latest pickups and dropoffs.  Pickups will be shown with green markers with a green line indicating the direction where the bike will eventually be dropped off (since we are replaying historical data, we conviently know the final dropoff location!).  Dropoff are shown in red with the line indicating where the bike came from.  Longer lines indicate longer trips.</p>\n<p>To do run the demo,</p>\n<ul>\n<li><strong>Run the Spark Streaming paragraph below</strong>.  This will start a Spark Streaming session with a 5 second window.  As new data arrives, it will update the map (below).  This session will run for a few minutes before stopping itself.</li>\n<li>Be sure to <strong>also run the Producer paragraph below</strong> otherwise there will be no data for the Spark Streaming session to see.</li>\n<li>With both the Spark Streaming and Producer running, <strong>watch the output of those paragraphs as well as the map paragraph</strong> to see changes.</li>\n</ul>\n<p>The code editors for the next three paragraphs are hidden by default to make it simpler to run, but definitely show the code editors to see how the code is written.</p>\n"},"dateCreated":"Jun 19, 2017 4:47:49 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:217","authenticationInfo":{},"dateFinished":"Jun 20, 2017 9:03:15 AM","dateStarted":"Jun 20, 2017 9:03:15 AM","focus":true},{"title":"Spark Streaming Live Map code","text":"%spark\n// Be sure to have run Part 2 of this tutorial first (and in the same zeppelin spark session).  This code assumes that bike_trips from Part 2 is a registered Spark SQL table\n\n//Define a class for the structure of the data we will be passing to the map javascript code\ncase class BikeEvents(eventtype: String, station: String, description: String, lat: Double, lon: Double, midwaylat: Double, midwaylon: Double)\n  //TODO: what other fields?  \n  // maybe midwaylat,lon (for lines)\n  //want to also keep startlat, startlon, endlat, endlon\n\nz.angularUnbind(\"bikeevents\")\n\n// println(\"topic:\"+z.angular(\"OEHCS_Topic\"))\n{\n    \n\nimport _root_.kafka.serializer.StringDecoder //http://stackoverflow.com/questions/36397688/sbt-cannot-import-kafka-encoder-decoder-classes\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\n\n\n\n println(\"Creating new Streaming Context\")\n val ssc = new StreamingContext(sc, Seconds(5))\n \n\n val topic = z.angular(\"BIND_OEHCS_Topic\").toString\n println(\"topic:\"+topic)\n val topicsSet = topic.split(\",\").toSet\n \n val brokers=z.angular(\"BIND_OEHCS_ConnectionDescriptor\").toString\n println(\"brokers:\"+brokers)\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n \n println(\"Creating Kafka DStream\")\n //https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\n\n\n\n println(\"Setting up operations on DStream\")    \n \n //for debugging, you can print the full contents of the first 10 rows of each batch of messages by uncommenting the following\n //messages.print()\n \n \n messages.foreachRDD(rdd => {\n     //our Kafka data comes in Key,Value format.  we only care about the value, so use a map to extract just the 2nd element\n     var values=rdd.map(kv => kv._2)\n     \n     //for this example, the value is a JSON string.  Let's make a DataFrame out of the JSON strings\n     var df=sqlContext.jsonRDD(values)\n     \n     var reccount = df.count()\n     //let's print out the count\n     printf(\"count = %s \\n\",reccount)\n     \n     //check to see if we have any rows...\n     if (reccount >0)  {\n        //let's print the first row\n        //println(\"first row \",df.first())\n    \n        \n        //Map the DF into an Array of BikeEvents. \n        var eventStr = if (\"EventType\"==\"Pickup\") \"Start\" else \"End\"\n        \n        var items = df.map(b => \n            BikeEvents(b.getAs[String](\"EventType\"), b.getAs[String](eventStr +\" Station Name\"), \"Gender: \"+ b.getAs[String](\"GenderStr\") + \" Birth Year:\"+b.getAs[String](\"Birth Year\"), b.getAs[String](eventStr +\" Station Latitude\").toDouble, b.getAs[String](eventStr +\" Station Longitude\").toDouble,\n            (b.getAs[String](\"Start Station Latitude\").toDouble+b.getAs[String](\"End Station Latitude\").toDouble)/2, (b.getAs[String](\"Start Station Longitude\").toDouble+b.getAs[String](\"End Station Longitude\").toDouble)/2)\n        )\n        //println(\"first items row \",items.first())\n           \n        //Bind the BikeEvents to an Angular UI variable named bikeevents\n        z.angularBind(\"bikeevents\", items.collect()) \n     } \n     \n })\n \n println(\"Starting Streaming Context\")\n ssc.start()\n\n \n println(\"Will now sleep for a few minutes, before stopping the StreamingContext\")\n\n //now sleep for a few minutes.  Parameter is milliseconds\n Thread.sleep(120000)\n\n //stop any active streamingcontexts.  Parameters are boolean stopSparkContext, boolean stopGracefully\n println(\"Stopping Active StreamingContext\")\n StreamingContext.getActive().map(_.stop(false,true))\n\n println(\"done\")\n\n} \n\n//hint: if the code does not appear to work but you don't see any errors, check the zeppelin spark log file at\n///u01/bdcsce/data/var/log/zeppelin/zeppelin-interpreter-spark-zeppelin-*.log","dateUpdated":"Jun 20, 2017 9:04:06 AM","config":{"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269104_-58463505","id":"20170421-164148_1484226247","dateCreated":"Jun 19, 2017 4:47:49 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:218","authenticationInfo":{},"dateFinished":"Jun 20, 2017 9:06:15 AM","dateStarted":"Jun 20, 2017 9:04:06 AM","focus":true},{"title":"Producer for the Live Map","text":"%sh\n#The arguments are datafile TimeSpeedupFactor RecordsToProduce StartDate StartTime  \n\n# for the more complex example, we are running a timespeedup factor of 5 to push data faster through the system\n\necho \"The producer will loop through the datafile until it finds the selected StartDate.  This can take awhile depending on the date of the month you select.\"\necho \"As an example, starting on the 5th can take 90 seconds before the first data is sent.  Starting on the 10th can take 3 minutes. Starting on the 20th can take 6 minutes.  Starting on the 30th can take 9 minutes.\"\n\n#and I think zeppelin has a default timeout of 10 minutes before it will kill a running shell interpreter\n#if you want to run this for a date in the end of the month, I suggest you make a new datafile that omits earlier data.\n# Example to make a smaller file that starts with Dec 24th. (dec24th starts on the 1366742th line-- seen by looking at the file in vi)\n# head -n 1  bikes/bike_events.csv > bikes/bike_events_startdec24.csv\n# tail -n +1366742 bikes/bike_events.csv >> bikes/bike_events_startdec24.csv\n# python ./citibike_kafka.py bikes/bike_events_startdec24.csv 20 300 2016-12-25 07:00:00 2>&1\n\necho \"..\"\necho \"..\"\necho \"Launching Producer for 2016-12-01 07:00:00 with a time acceleration factor of 5\"\npython ./citibike_kafka.py bikes/bike_events.csv 5 400 2016-12-01 07:00:00 2>&1\necho \"done\"\n\n","dateUpdated":"Jun 20, 2017 9:04:12 AM","config":{"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269104_-58463505","id":"20170421-172032_481813195","dateCreated":"Jun 19, 2017 4:47:49 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:219","authenticationInfo":{},"dateFinished":"Jun 20, 2017 9:05:27 AM","dateStarted":"Jun 20, 2017 9:04:12 AM","focus":true},{"title":"HTML to display the live map","text":"%angular\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.0.3/leaflet.css\" />\n<div id=\"map\" style=\"height: 700px; width: 100%\"></div>\n\n<script type=\"text/javascript\">\n\n\n//based on https://gist.github.com/granturing/a09aed4a302a7367be92, https://community.hortonworks.com/articles/90320/add-leaflet-map-to-zeppelin-notebook.html, etc\nfunction initMap() {\n    //open up a map around NYC at zoom level 13\n    var map = L.map('map', {preferCanvas: true}).setView([40.75, -73.99], 13);\n    //the preferCanvas was needed so that polylines and circles would show right\n\n\n    //define the background tile layer using OpenStreet maps.  Leaflet can work with other providers, too\n    L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {\n        attribution: 'Map data &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors',\n        maxZoom: 16,\n        minZoom: 11\n    }).addTo(map);\n\n    //define a new LayerGroup which will hold our markers\n    var startMarkers = L.layerGroup().addTo(map);\n    var endMarkers = L.layerGroup().addTo(map);\n\n    var overlayMaps = {\n    \"Top Pickup Stations\": startMarkers,\n    \"Top Dropoff Stations\": endMarkers\n    };\n    \n    //add a Control to the map to let the user click the Marker layer off and on\n    L.control.layers(null, overlayMaps).addTo(map);\n    \n    // keep track of our markers, so we can remove them later\n    var markers = new Array();\n    \n    // setup a custom icon for markers. See https://github.com/pointhi/leaflet-color-markers\n    var greenIcon = new L.Icon({\n      iconUrl: 'https://cdn.rawgit.com/pointhi/leaflet-color-markers/master/img/marker-icon-green.png',\n      shadowUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.7/images/marker-shadow.png',\n      iconSize: [12, 20],\n      iconAnchor: [6, 20],\n      popupAnchor: [1, -17],\n      shadowSize: [20, 20]\n    });\n    \n    var redIcon = new L.Icon({\n      iconUrl: 'https://cdn.rawgit.com/pointhi/leaflet-color-markers/master/img/marker-icon-red.png',\n      shadowUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.7/images/marker-shadow.png',\n      iconSize: [12, 20],\n      iconAnchor: [6, 20],\n      popupAnchor: [1, -17],\n      shadowSize: [20, 20]\n    });\n\n    var el = angular.element($('#map').parent('.ng-scope'));\n    angular.element(el).ready(function() {\n        \n        //listen for changes to the angular variable called stations\n        window.locationWatcher = el.scope().compiledScope.$watch('bikeevents', function(newValue, oldValue) {\n             //geoMarkers.clearLayers(); -- this did not work for me, so I use the for loop below to delete old markers\n            for(i=0;i<markers.length;i++) {\n               startMarkers.removeLayer(markers[i]);\n               endMarkers.removeLayer(markers[i]);\n               map.removeLayer(markers[i]);\n            }  \n            //now empty our array and start again\n            markers = new Array();\n            \n            //loop through each entry in our stations variable and add it as a marker\n            angular.forEach(newValue, function(bikes) {\n                var marker = L.marker([bikes.lat, bikes.lon], {icon: greenIcon})\n                    .bindPopup(\"<b>\" + bikes.station + \"</b><br>\" + bikes.description)\n                var latlngs = [\n                  [bikes.lat, bikes.lon],\n                  [bikes.midwaylat, bikes.midwaylon]\n                ];\n                \n                if (bikes.eventtype==\"Dropoff\") {\n                    marker.setIcon(redIcon)\n                    marker.addTo(endMarkers)\n                    var polyline = L.polyline(latlngs, {color: 'red'}).addTo(endMarkers);\n                    markers.push(polyline);\n                } else {\n                    marker.addTo(startMarkers);  \n                    var polyline = L.polyline(latlngs, {color: 'green'}).addTo(startMarkers);\n                    markers.push(polyline);\n                }\n                markers.push(marker);   \n                \n                \n            });\n        })\n    });\n}\n\nif (window.locationWatcher) {\n    // clear existing watcher otherwise we'll have duplicates\n    window.locationWatcher();\n}\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log('Loading Leaflet library');\n    var sc = document.createElement('script');\n    sc.type = 'text/javascript';\n    sc.src = 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.0.3/leaflet.js';\n    sc.onload = initMap;\n    sc.onerror = function(err) { alert(err); }\n    document.getElementsByTagName('head')[0].appendChild(sc);\n}\n</script>\n\n","dateUpdated":"Jun 19, 2017 4:47:49 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269104_-58463505","id":"20170417-195006_1776067804","result":{"code":"SUCCESS","type":"ANGULAR","msg":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.0.3/leaflet.css\" />\n<div id=\"map\" style=\"height: 700px; width: 100%\"></div>\n\n<script type=\"text/javascript\">\n\n\n//based on https://gist.github.com/granturing/a09aed4a302a7367be92, https://community.hortonworks.com/articles/90320/add-leaflet-map-to-zeppelin-notebook.html, etc\nfunction initMap() {\n    //open up a map around NYC at zoom level 13\n    var map = L.map('map', {preferCanvas: true}).setView([40.75, -73.99], 13);\n    //the preferCanvas was needed so that polylines and circles would show right\n\n\n    //define the background tile layer using OpenStreet maps.  Leaflet can work with other providers, too\n    L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {\n        attribution: 'Map data &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors',\n        maxZoom: 16,\n        minZoom: 11\n    }).addTo(map);\n\n    //define a new LayerGroup which will hold our markers\n    var startMarkers = L.layerGroup().addTo(map);\n    var endMarkers = L.layerGroup().addTo(map);\n\n    var overlayMaps = {\n    \"Top Pickup Stations\": startMarkers,\n    \"Top Dropoff Stations\": endMarkers\n    };\n    \n    //add a Control to the map to let the user click the Marker layer off and on\n    L.control.layers(null, overlayMaps).addTo(map);\n    \n    // keep track of our markers, so we can remove them later\n    var markers = new Array();\n    \n    // setup a custom icon for markers. See https://github.com/pointhi/leaflet-color-markers\n    var greenIcon = new L.Icon({\n      iconUrl: 'https://cdn.rawgit.com/pointhi/leaflet-color-markers/master/img/marker-icon-green.png',\n      shadowUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.7/images/marker-shadow.png',\n      iconSize: [12, 20],\n      iconAnchor: [6, 20],\n      popupAnchor: [1, -17],\n      shadowSize: [20, 20]\n    });\n    \n    var redIcon = new L.Icon({\n      iconUrl: 'https://cdn.rawgit.com/pointhi/leaflet-color-markers/master/img/marker-icon-red.png',\n      shadowUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.7/images/marker-shadow.png',\n      iconSize: [12, 20],\n      iconAnchor: [6, 20],\n      popupAnchor: [1, -17],\n      shadowSize: [20, 20]\n    });\n\n    var el = angular.element($('#map').parent('.ng-scope'));\n    angular.element(el).ready(function() {\n        \n        //listen for changes to the angular variable called stations\n        window.locationWatcher = el.scope().compiledScope.$watch('bikeevents', function(newValue, oldValue) {\n             //geoMarkers.clearLayers(); -- this did not work for me, so I use the for loop below to delete old markers\n            for(i=0;i<markers.length;i++) {\n               startMarkers.removeLayer(markers[i]);\n               endMarkers.removeLayer(markers[i]);\n               map.removeLayer(markers[i]);\n            }  \n            //now empty our array and start again\n            markers = new Array();\n            \n            //loop through each entry in our stations variable and add it as a marker\n            angular.forEach(newValue, function(bikes) {\n                var marker = L.marker([bikes.lat, bikes.lon], {icon: greenIcon})\n                    .bindPopup(\"<b>\" + bikes.station + \"</b><br>\" + bikes.description)\n                var latlngs = [\n                  [bikes.lat, bikes.lon],\n                  [bikes.midwaylat, bikes.midwaylon]\n                ];\n                \n                if (bikes.eventtype==\"Dropoff\") {\n                    marker.setIcon(redIcon)\n                    marker.addTo(endMarkers)\n                    var polyline = L.polyline(latlngs, {color: 'red'}).addTo(endMarkers);\n                    markers.push(polyline);\n                } else {\n                    marker.addTo(startMarkers);  \n                    var polyline = L.polyline(latlngs, {color: 'green'}).addTo(startMarkers);\n                    markers.push(polyline);\n                }\n                markers.push(marker);   \n                \n                \n            });\n        })\n    });\n}\n\nif (window.locationWatcher) {\n    // clear existing watcher otherwise we'll have duplicates\n    window.locationWatcher();\n}\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log('Loading Leaflet library');\n    var sc = document.createElement('script');\n    sc.type = 'text/javascript';\n    sc.src = 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.0.3/leaflet.js';\n    sc.onload = initMap;\n    sc.onerror = function(err) { alert(err); }\n    document.getElementsByTagName('head')[0].appendChild(sc);\n}\n</script>"},"dateCreated":"Jun 19, 2017 4:47:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:220","focus":true},{"text":"%md\n#Next Steps\n\nThis concludes our NYC Citi Bikes demonstration (for now).  \n\n\n\nYou've seen how to:\n + Load Citi Bike data to the Object Store\n + Define a Spark SQL table on the data\n + Run various SQL queries and display the output\n + Show results on a Map\n + Stream bike data and create a live map\n\n\nStay tuned for additional parts of this demonstration coming soon.\n","dateUpdated":"Jun 19, 2017 4:47:49 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269104_-58463505","id":"20170417-195150_467395081","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Next Steps</h1>\n<p>This concludes our NYC Citi Bikes demonstration (for now).</p>\n<p>You've seen how to:</p>\n<ul>\n<li>Load Citi Bike data to the Object Store</li>\n<li>Define a Spark SQL table on the data</li>\n<li>Run various SQL queries and display the output</li>\n<li>Show results on a Map</li>\n<li>Stream bike data and create a live map</li>\n</ul>\n<p>Stay tuned for additional parts of this demonstration coming soon.</p>\n"},"dateCreated":"Jun 19, 2017 4:47:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:221"},{"dateUpdated":"Jun 19, 2017 4:47:49 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497905269104_-58463505","id":"20170503-152817_1702690082","dateCreated":"Jun 19, 2017 4:47:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:222"}],"name":"Demonstration Citi Bike Live Map with OEHCS and Spark Streaming","id":"2CJ7GXVXN","angularObjects":{"2CKYUH771":[],"2CHQCE7WB":[],"2CM73VNZC":[],"2CJ9M7X3E":[],"2CM9MFPDC":[{"name":"BIND_ObjectStorage_Container","object":"citibike","noteId":"2CJ7GXVXN"},{"name":"BIND_OEHCS_ConnectionDescriptor","object":"141.144.29.130:6667","noteId":"2CJ7GXVXN"},{"name":"BIND_OEHCS_Topic","object":"gse00010212-TutorialOEHCS","noteId":"2CJ7GXVXN"}],"2CKUFFGB5":[],"2CH76629W":[],"2CJ6JMU49":[],"2CMM8Y2GW":[]},"config":{"looknfeel":"default"},"info":{}}