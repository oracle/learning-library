{"paragraphs":[{"text":"%md\n# Alluxio (BDFS)\n\nhttps://docs.oracle.com/en/cloud/paas/big-data-compute-cloud/csspc/big-data-file-system-bdfs.html\nand\nhttps://www.alluxio.org/docs/master/en/index.html\nand\nhttp://www.alluxio.org/docs/master/en/Configuration-Settings.html\n\nNote: if running BDC on OCI (as compared to OCI-Classic), BDFS will not work until version 18.2.2\n","dateUpdated":"2018-03-08T14:42:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Alluxio (BDFS)</h1>\n<p><a href=\"https://docs.oracle.com/en/cloud/paas/big-data-compute-cloud/csspc/big-data-file-system-bdfs.html\">https://docs.oracle.com/en/cloud/paas/big-data-compute-cloud/csspc/big-data-file-system-bdfs.html</a><br/>and<br/><a href=\"https://www.alluxio.org/docs/master/en/index.html\">https://www.alluxio.org/docs/master/en/index.html</a><br/>and<br/><a href=\"http://www.alluxio.org/docs/master/en/Configuration-Settings.html\">http://www.alluxio.org/docs/master/en/Configuration-Settings.html</a></p>\n<p>Note: if running BDC on OCI (as compared to OCI-Classic), BDFS will not work until version 18.2.2</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520514503300_1298225371","id":"20170801-182316_1493031966","dateCreated":"2018-03-08T13:08:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1585","user":"anonymous","dateFinished":"2018-03-08T14:42:57+0000","dateStarted":"2018-03-08T14:42:57+0000"},{"title":"Display the alluxio command line help...","text":"%sh\nalluxio fs\n\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Usage: java AlluxioShell\n       [cat <path>]                                         Prints the file's contents to the console.                                                     \n       [checksum <Alluxio path>]                            Calculates the md5 checksum of a file in the Alluxio filesystem.                               \n       [chgrp [-R] <group> <path>]                          Changes the group of a file or directory specified by args. Specify -R to change the group recursively.\n       [chmod [-R] <mode> <path>]                           Changes the permission of a file or directory specified by args. Specify -R to change the permission recursively.\n       [chown [-R] <owner> <path>]                          Changes the owner of a file or directory specified by args. Specify -R to change the owner recursively.\n       [copyFromLocal <src> <remoteDst>]                    Copies a file or a directory from local filesystem to Alluxio filesystem.                      \n       [copyToLocal <src> <localDst>]                       Copies a file or a directory from the Alluxio filesystem to the local filesystem.              \n       [count <path>]                                       Displays the number of files and directories matching the specified prefix.                    \n       [cp [-R] <src> <dst>]                                Copies a file or a directory in the Alluxio filesystem. The -R flag is needed to copy directories.\n       [createLineage <inputFile1,...> <outputFile1,...> [<cmd_arg1> <cmd_arg2> ...]]   Creates a lineage.                                                                             \n       [deleteLineage <lineageId> <cascade(true|false)>]    Deletes a lineage. If cascade is specified as true, dependent lineages will also be deleted.   \n       [du <path>]                                          Displays the size of the specified file or directory.                                          \n       [fileInfo <path>]                                    Displays all block info for the specified file.                                                \n       [free <path>]                                        Frees the space occupied by a file or a directory in Alluxio.                                  \n       [getCapacityBytes]                                   Gets the capacity of the Alluxio file system.                                                  \n       [getUsedBytes]                                       Gets number of bytes used in the Alluxio file system.                                          \n       [leader]                                             Prints the current leader master host name.                                                    \n       [listLineages]                                       Lists all lineages.                                                                            \n       [load <path>]                                        Loads a file or directory in Alluxio space, makes it resident in memory.                       \n       [loadMetadata <path>]                                Loads metadata for the given Alluxio path from the under file system.                          \n       [location <path>]                                    Displays the list of hosts storing the specified file.                                         \n       [ls [-R] [-f] <path>]                                Displays information for all files and directories directly under the specified path. Specify -R to display files and directories recursively. Specify -f to force loading files in the directory.\n       [mkdir <path1> [path2] ... [pathn]]                  Creates the specified directories, including any parent directories that are required.         \n       [mount [-readonly] [-shared] [-P <properties file name>] <alluxioPath> <ufsURI>]   Mounts a UFS path onto an Alluxio path.                                                        \n       [mv <src> <dst>]                                     Renames a file or directory.                                                                   \n       [persist <alluxioPath1> [alluxioPath2] ... [alluxioPathn]]   Persists files or directories currently stored only in Alluxio to the UnderFileSystem.         \n       [pin <path>]                                         Pins the given file or directory in memory (works recursively for directories). Pinned files are never evicted from memory, unless TTL is set.\n       [report <path>]                                      Reports to the master that a file is lost.                                                     \n       [rm [-R] <path>]                                     Removes the specified file. Specify -R to remove file or directory recursively.                \n       [setTtl <path> <time to live(in milliseconds)>]      Sets a new TTL value for the file at path.                                                     \n       [tail -c <number of bytes> <path>]                   Prints the file's last n bytes (by default, 1KB) to the console.                               \n       [touch <path>]                                       Creates a 0 byte file. The file will be written to the under file system.                      \n       [unmount <alluxioPath>]                              Unmounts an Alluxio path.                                                                      \n       [unpin <path>]                                       Unpins the given file or folder from memory (works recursively for a directory).               \n       [unsetTtl <path>]                                    Unsets the TTL value for the given path.                                                       \n"},{"type":"TEXT","data":"ExitValue: 255"}]},"apps":[],"jobName":"paragraph_1520514503301_1297840622","id":"20180203-175838_1702158189","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1586"},{"title":"An example of listing the alluxio (BDFS) file system","text":"%sh\nalluxio fs ls -R -f /citibike/\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1.00B     02-03-2018 15:54:03:180  Directory      /citibike/raw\n130.33MB  02-03-2018 15:54:03:181  In Memory      /citibike/raw/201612-citibike-tripdata.csv\n1.00B     02-03-2018 18:51:31:613  Directory      /citibike/modified\n130.33MB  02-03-2018 18:51:31:630  In Memory      /citibike/modified/201612-citibike-tripdata.nh.csv\n"}]},"apps":[],"jobName":"paragraph_1520514503302_1298994869","id":"20180203-175907_116962826","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1587"},{"title":"Explicitly load the data we want to work with into BDFS","text":"%sh\nalluxio fs load /citibike/modified/201612-citibike-tripdata.nh.csv\nalluxio fs ls -R /citibike","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1.00B     02-03-2018 15:54:03:180  Directory      /citibike/raw\n130.33MB  02-03-2018 15:54:03:181  In Memory      /citibike/raw/201612-citibike-tripdata.csv\n1.00B     02-03-2018 18:51:31:613  Directory      /citibike/modified\n130.33MB  02-03-2018 18:51:31:630  In Memory      /citibike/modified/201612-citibike-tripdata.nh.csv\n"}]},"apps":[],"jobName":"paragraph_1520514503302_1298994869","id":"20180203-184548_766276135","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1588"},{"title":"An example of listing the alluxio files system using hadoop fs","text":"%sh\nhadoop fs -ls swift://journeyC.default/citibike/modified\n# use the below LOGGER setting to avoid lots of INFO logging from alluxio by default\nexport HADOOP_ROOT_LOGGER=WARN\nhadoop fs -ls bdfs://localhost:19998/citibike/modified","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 1 items\n-rw-rw-rw-   1  136661199 2018-02-03 18:50 swift://journeyC.default/citibike/modified/201612-citibike-tripdata.nh.csv\nFound 1 items\n-rw-rw-rw-   3  136661199 2018-02-03 18:51 bdfs://localhost:19998/citibike/modified/201612-citibike-tripdata.nh.csv\n"}]},"apps":[],"jobName":"paragraph_1520514503302_1298994869","id":"20180203-190920_2025214639","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1589"},{"title":"An example of using alluxio (BDFS) versus standard object store (swift)","text":"%spark\n\n// If you get this error message:\n// java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n// Then go to the Settings tab, then click on Notebook.  Then restart the Notebook.  This will restart your SparkContext\n\n//val swift_df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"swift://journeyC.default/citibike/raw/201612-citibike-tripdata.csv\")\nval swift_df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"swift://journeyC.default/citibike/modified/201612-citibike-tripdata.nh.csv\")\n\n//val bdfs_df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"bdfs://localhost:19998/citibike/raw/201612-citibike-tripdata.csv\")\nval bdfs_df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"bdfs://localhost:19998/citibike/modified/201612-citibike-tripdata.nh.csv\")\n\n\n{\nvar t0 = System.nanoTime()\nprintln(\"# of rows: %s\".format(\n  swift_df.count() \n)) \nvar t1 = System.nanoTime()\nprintln(\"Swift Count Elapsed time: \" + (t1 - t0)/1000000000 + \"s\")\nprintln(\"..\")\n\nt0 = System.nanoTime()\nprintln(\"# of rows: %s\".format(\n  bdfs_df.count() \n)) \nt1 = System.nanoTime()\nprintln(\"BDFS Count Elapsed time: \" + (t1 - t0)/1000000000 + \"s\")\nprintln(\"..\")\n}\n\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nswift_df: org.apache.spark.sql.DataFrame = [528: string, 2016-12-01 00:00:04: string ... 13 more fields]\n\nbdfs_df: org.apache.spark.sql.DataFrame = [528: int, 2016-12-01 00:00:04: timestamp ... 13 more fields]\n# of rows: 812191\nSwift Count Elapsed time: 7s\n..\n# of rows: 812191\nBDFS Count Elapsed time: 1s\n..\n"}]},"apps":[],"jobName":"paragraph_1520514503303_1298610120","id":"20180203-180003_597806132","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1590"},{"text":"%md\n# In order to use hive, we need to adjust an Alluxio parameter\n\nThis is needed in 18.1.2.\n\n1.edit /u01/bdcsce/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ALLUXIO/package/templates/alluxio-site.template , and add\nalluxio.underfs.object.store.mount.shared.publicly=true\n2.restart alluxio via ambari\n\nSee https://www.alluxio.org/docs/master/en/Configuring-Alluxio-with-Swift.html\n\n\n\n\nThese instructions did not work in 18.1.2.  You need to edit the alluxio-site.template file directly:\n1.In ambari, navigate to Alluxio, then Configs.\n2.Expand the custom alluixio-site section\n3.Click Add Property...\n4.Add this:\nalluxio.underfs.object.store.mount.shared.publicly=true\n5.Save the configuration\n6.Restart alluxio.\n\n\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>In order to use hive, we need to adjust an Alluxio parameter</h1>\n<p>This is needed in 18.1.2.</p>\n<p>1.edit /u01/bdcsce/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ALLUXIO/package/templates/alluxio-site.template , and add<br/>alluxio.underfs.object.store.mount.shared.publicly=true<br/>2.restart alluxio via ambari</p>\n<p>See <a href=\"https://www.alluxio.org/docs/master/en/Configuring-Alluxio-with-Swift.html\">https://www.alluxio.org/docs/master/en/Configuring-Alluxio-with-Swift.html</a></p>\n<p>These instructions did not work in 18.1.2. You need to edit the alluxio-site.template file directly:<br/>1.In ambari, navigate to Alluxio, then Configs.<br/>2.Expand the custom alluixio-site section<br/>3.Click Add Property&hellip;<br/>4.Add this:<br/>alluxio.underfs.object.store.mount.shared.publicly=true<br/>5.Save the configuration<br/>6.Restart alluxio.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520514503303_1298610120","id":"20180203-181546_1001224896","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1591"},{"text":"%md\n# In order to use Spark Thrift..\n\nNeeded in 18.1.2\n\n1.Go to Ambari..Spark2...Configs\n\n2.Expand the Advanced spark2-env\n\nedit spark_thrift_cmd_opts and add :/usr/hdp/current/zeppelin-server/lib/guava-15.0.jar to the --driver-class-path section AND to the --conf \"spark.executor.extraClassPath=...\" section.  This is easiest if you copy the value into a notepad and change it and copy it back.  If you haven't changed it, in 18.1.2, you'll probably end up with this value:\n\n--master yarn --driver-class-path /opt/oracle/bdcsce/current/lib/hadoop-openstack.jar:/opt/oracle/bdcsce/current/lib/joss-bdcsce.jar:/opt/oracle/bdcsce/current/lib/ojdbc7.jar:/opt/oracle/bdcsce/current/lib/spoccs-hdfs-zeppelin-0.5.1.jar:/opt/oracle/bdcsce/current/lib/stocator-bdcsce.jar:/opt/oracle/bdcsce/current/lib/oci-hdfs-full-1.2.5.jar:/u01/bdcsce/opt/alluxio/core/client/target/alluxio-core-client-1.3.0-jar-with-dependencies.jar:/u01/bdcsce/opt/alluxio/conf/:/usr/hdp/current/spark-client/conf:/usr/hdp/current/zeppelin-server/lib/guava-15.0.jar --conf \"spark.executor.extraClassPath=/opt/oracle/bdcsce/current/lib/hadoop-openstack.jar:/opt/oracle/bdcsce/current/lib/joss-bdcsce.jar:/opt/oracle/bdcsce/current/lib/ojdbc7.jar:/opt/oracle/bdcsce/current/lib/spoccs-hdfs-zeppelin-0.5.1.jar:/opt/oracle/bdcsce/current/lib/stocator-bdcsce.jar:/u01/bdcsce/opt/alluxio/core/client/target/alluxio-core-client-1.3.0-jar-with-dependencies.jar:/u01/bdcsce/opt/alluxio/conf/:/usr/hdp/current/spark-client/conf:/usr/hdp/current/zeppelin-server/lib/guava-15.0.jar\" --jars /opt/oracle/bdcsce/current/lib/hadoop-openstack-spoc.jar,/opt/oracle/bdcsce/current/lib/joss-bdcsce.jar,/opt/oracle/bdcsce/current/lib/ojdbc7.jar,/opt/oracle/bdcsce/current/lib/spoccs-hdfs-zeppelin-0.5.1.jar,/opt/oracle/bdcsce/current/lib/stocator-bdcsce.jar,/opt/oracle/bdcsce/current/lib/oci-hdfs-full-1.2.5.jar,/u01/bdcsce/opt/alluxio/core/client/target/alluxio-core-client-1.3.0-jar-with-dependencies.jar,/u01/bdcsce/usr/hdp/current/zeppelin-server/lib/guava-15.0.jar --driver-java-options -Dspark.local.dir=/data/var/tmp\n\n3.save and restart Spark2\n\n\n\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>In order to use Spark Thrift..</h1>\n<p>Needed in 18.1.2</p>\n<p>1.Go to Ambari..Spark2&hellip;Configs</p>\n<p>2.Expand the Advanced spark2-env</p>\n<p>edit spark_thrift_cmd_opts and add :/usr/hdp/current/zeppelin-server/lib/guava-15.0.jar to the &ndash;driver-class-path section AND to the &ndash;conf &ldquo;spark.executor.extraClassPath=&hellip;&rdquo; section. This is easiest if you copy the value into a notepad and change it and copy it back. If you haven&rsquo;t changed it, in 18.1.2, you&rsquo;ll probably end up with this value:</p>\n<p>&ndash;master yarn &ndash;driver-class-path /opt/oracle/bdcsce/current/lib/hadoop-openstack.jar:/opt/oracle/bdcsce/current/lib/joss-bdcsce.jar:/opt/oracle/bdcsce/current/lib/ojdbc7.jar:/opt/oracle/bdcsce/current/lib/spoccs-hdfs-zeppelin-0.5.1.jar:/opt/oracle/bdcsce/current/lib/stocator-bdcsce.jar:/opt/oracle/bdcsce/current/lib/oci-hdfs-full-1.2.5.jar:/u01/bdcsce/opt/alluxio/core/client/target/alluxio-core-client-1.3.0-jar-with-dependencies.jar:/u01/bdcsce/opt/alluxio/conf/:/usr/hdp/current/spark-client/conf:/usr/hdp/current/zeppelin-server/lib/guava-15.0.jar &ndash;conf &ldquo;spark.executor.extraClassPath=/opt/oracle/bdcsce/current/lib/hadoop-openstack.jar:/opt/oracle/bdcsce/current/lib/joss-bdcsce.jar:/opt/oracle/bdcsce/current/lib/ojdbc7.jar:/opt/oracle/bdcsce/current/lib/spoccs-hdfs-zeppelin-0.5.1.jar:/opt/oracle/bdcsce/current/lib/stocator-bdcsce.jar:/u01/bdcsce/opt/alluxio/core/client/target/alluxio-core-client-1.3.0-jar-with-dependencies.jar:/u01/bdcsce/opt/alluxio/conf/:/usr/hdp/current/spark-client/conf:/usr/hdp/current/zeppelin-server/lib/guava-15.0.jar&rdquo; &ndash;jars /opt/oracle/bdcsce/current/lib/hadoop-openstack-spoc.jar,/opt/oracle/bdcsce/current/lib/joss-bdcsce.jar,/opt/oracle/bdcsce/current/lib/ojdbc7.jar,/opt/oracle/bdcsce/current/lib/spoccs-hdfs-zeppelin-0.5.1.jar,/opt/oracle/bdcsce/current/lib/stocator-bdcsce.jar,/opt/oracle/bdcsce/current/lib/oci-hdfs-full-1.2.5.jar,/u01/bdcsce/opt/alluxio/core/client/target/alluxio-core-client-1.3.0-jar-with-dependencies.jar,/u01/bdcsce/usr/hdp/current/zeppelin-server/lib/guava-15.0.jar &ndash;driver-java-options -Dspark.local.dir=/data/var/tmp</p>\n<p>3.save and restart Spark2</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520514503303_1298610120","id":"20180205-203015_400513135","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1592"},{"title":"Create an external hive table against BDFS (alluxio)","text":"%sh\n\n/u01/bdcsce/opt/alluxio/bin/alluxio fs chmod 777 /citibike/modified/\n\nhive  <<EOF\nDROP TABLE bike_trips_objectstore_bdfs;\n\nCREATE external TABLE bike_trips_objectstore_bdfs ( \nTripDuration int,\nStartTime timestamp,\nStopTime timestamp,\nStartStationID string,\nStartStationName string,\nStartStationLatitude string,\nStartStationLongitude string,\nEndStationID string,\nEndStationName string,\nEndStationLatitude string,\nEndStationLongitude string,\nBikeID int,\nUserType string,\nBirthYear int, \nGender int\n) \nROW FORMAT delimited \nFIELDS TERMINATED BY ',' \nlocation 'bdfs://localhost:19998/citibike/modified/';\n\n\nexit;\n\nEOF\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":150.3,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Changed permission of /citibike/modified to 777\nWARNING: Use \"yarn jar\" to launch YARN applications.\n\nLogging initialized using configuration in file:/etc/hive/2.4.2.0-258/0/hive-log4j.properties\nhive> DROP TABLE bike_trips_objectstore_bdfs;\nOK\nTime taken: 0.948 seconds\nhive> \n    > CREATE external TABLE bike_trips_objectstore_bdfs ( \n    > TripDuration int,\n    > StartTime timestamp,\n    > StopTime timestamp,\n    > StartStationID string,\n    > StartStationName string,\n    > StartStationLatitude string,\n    > StartStationLongitude string,\n    > EndStationID string,\n    > EndStationName string,\n    > EndStationLatitude string,\n    > EndStationLongitude string,\n    > BikeID int,\n    > UserType string,\n    > BirthYear int, \n    > Gender int\n    > ) \n    > ROW FORMAT delimited \n    > FIELDS TERMINATED BY ',' \n    > location 'bdfs://localhost:19998/citibike/modified/';\nOK\nTime taken: 0.809 seconds\nhive> \n    > \n    > exit;\n"}]},"apps":[],"jobName":"paragraph_1520514503304_1296686376","id":"20170921-191613_40013701","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1593"},{"title":"Compare the performance of Spark SQL tables on object store (swift) versus bdfs versus hdfs","text":"%spark\n\nval swift_df=spark.sql(\"select * from bike_trips_objectstore\")\nval bdfs_df=spark.sql(\"select * from bike_trips_objectstore_bdfs\")\nval hdfs_df=spark.sql(\"select * from bike_trips\")\n\n{\nvar t0 = System.nanoTime()\nprintln(\"# of rows: %s\".format(\n  swift_df.count() \n)) \nvar t1 = System.nanoTime()\nprintln(\"Swift Count Elapsed time: \" + (t1 - t0)/1000000000 + \"s\")\nprintln(\"..\")\n\nt0 = System.nanoTime()\nprintln(\"# of rows: %s\".format(\n  bdfs_df.count() \n)) \nt1 = System.nanoTime()\nprintln(\"BDFS Count Elapsed time: \" + (t1 - t0)/1000000000 + \"s\")\nprintln(\"..\")\n\nt0 = System.nanoTime()\nprintln(\"# of rows: %s\".format(\n  hdfs_df.count() \n)) \nt1 = System.nanoTime()\nprintln(\"HDFS Count Elapsed time: \" + (t1 - t0)/1000000000 + \"s\")\nprintln(\"..\")\n    \n}\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nswift_df: org.apache.spark.sql.DataFrame = [tripduration: int, starttime: timestamp ... 13 more fields]\n\nbdfs_df: org.apache.spark.sql.DataFrame = [tripduration: int, starttime: timestamp ... 13 more fields]\n\nhdfs_df: org.apache.spark.sql.DataFrame = [tripduration: int, starttime: timestamp ... 13 more fields]\n# of rows: 812192\nSwift Count Elapsed time: 5s\n..\n# of rows: 812192\nBDFS Count Elapsed time: 0s\n..\n# of rows: 812192\nHDFS Count Elapsed time: 3s\n..\n"}]},"apps":[],"jobName":"paragraph_1520514503304_1296686376","id":"20180203-185627_1570978413","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1594"},{"title":"Test Alluxio via the Spark Thrift Server","text":"%jdbc(sts)\nselect usertype, count(*) from bike_trips_objectstore_bdfs group by usertype\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"usertype\tcount(1)\nSubscriber\t774278\n\t5388\nCustomer\t32526\n"}]},"apps":[],"jobName":"paragraph_1520514503305_1296301627","id":"20180205-204210_1816790901","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1595"},{"text":"%md\n# View the Alluxio Web UI (port 19999)\n\nThe suggested way is to ssh into BDC and tunnel port 19999.  Then point your local browser to http://127.0.0.1:19999/configuration\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>View the Alluxio Web UI (port 19999)</h1>\n<p>The suggested way is to ssh into BDC and tunnel port 19999. Then point your local browser to <a href=\"http://127.0.0.1:19999/configuration\">http://127.0.0.1:19999/configuration</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520514503305_1296301627","id":"20170921-202457_745513108","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1596"},{"title":"Example of using the Alluxio interpreter in zeppelin","text":"%alluxio\nhelp\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"text","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Commands list:\n\t[help] - List all available commands.\n\t[cat <path>] - Prints the file's contents to the console.\n\t[chgrp [-R] <group> <path>] - Changes the group of a file or directory specified by args. Specify -R to change the group recursively.\n\t[chmod -R <mode> <path>] - Changes the permission of a file or directory specified by args. Specify -R to change the permission recursively.\n\t[chown -R <owner> <path>] - Changes the owner of a file or directory specified by args. Specify -R to change the owner recursively.\n\t[copyFromLocal <src> <remoteDst>] - Copies a file or a directory from local filesystem to Alluxio filesystem.\n\t[copyToLocal <src> <localDst>] - Copies a file or a directory from the Alluxio filesystem to the local filesystem.\n\t[count <path>] - Displays the number of files and directories matching the specified prefix.\n\t[createLineage <inputFile1,...> <outputFile1,...> [<cmd_arg1> <cmd_arg2> ...]] - Creates a lineage.\n\t[deleteLineage <lineageId> <cascade(true|false)>] - Deletes a lineage. If cascade is specified as true, dependent lineages will also be deleted.\n\t[du <path>] - Displays the size of the specified file or directory.\n\t[fileInfo <path>] - Displays all block info for the specified file.\n\t[free <file path|folder path>] - Removes the file or directory(recursively) from Alluxio memory space.\n\t[getCapacityBytes] - Gets the capacity of the Alluxio file system.\n\t[getUsedBytes] - Gets number of bytes used in the Alluxio file system.\n\t[listLineages] - Lists all lineages.\n\t[load <path>] - Loads a file or directory in Alluxio space, makes it resident in memory.\n\t[loadMetadata <path>] - Loads metadata for the given Alluxio path from the under file system.\n\t[location <path>] - Displays the list of hosts storing the specified file.\n\t[ls [-R] <path>] - Displays information for all files and directories directly under the specified path. Specify -R to display files and directories recursively.\n\t[mkdir <path1> [path2] ... [pathn]] - Creates the specified directories, including any parent directories that are required.\n\t[mount <alluxioPath> <ufsURI>] - Mounts a UFS path onto an Alluxio path.\n\t[mv <src> <dst>] - Renames a file or directory.\n\t[persist <alluxioPath>] - Persists a file or directory currently stored only in Alluxio to the UnderFileSystem.\n\t[pin <path>] - Pins the given file or directory in memory (works recursively for directories). Pinned files are never evicted from memory, unless TTL is set.\n\t[report <path>] - Reports to the master that a file is lost.\n\t[rm [-R] <path>] - Removes the specified file. Specify -R to remove file or directory recursively.\n\t[setTtl <path> <time to live(in milliseconds)>] - Sets a new TTL value for the file at path.\n\t[tail <path>] - Prints the file's last 1KB of contents to the console.\n\t[touch <path>] - Creates a 0 byte file. The file will be written to the under file system.\n\t[unmount <alluxioPath>] - Unmounts an Alluxio path.\n\t[unpin <path>] - Unpins the given file or folder from memory (works recursively for a directory).\n\\t[unsetTtl <path>] - Unsets the TTL value for the given path.\n\t[unpin <path>] - Unpin the given file to allow Alluxio to evict this file again. If the given path is a directory, it recursively unpins all files contained and any new files created within this directory.\n"}]},"apps":[],"jobName":"paragraph_1520514503305_1296301627","id":"20170801-195512_597380873","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1597"},{"text":"%alluxio\nls -R /citibike","dateUpdated":"2018-03-08T13:08:23+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1.00B     02-03-2018 15:54:03:180  Directory      /citibike/raw\n130.33MB  02-03-2018 15:54:03:181  In Memory      /citibike/raw/201612-citibike-tripdata.csv\n1.00B     02-03-2018 18:51:31:613  Directory      /citibike/modified\n130.33MB  02-03-2018 18:51:31:630  In Memory      /citibike/modified/201612-citibike-tripdata.nh.csv\n\n"}]},"apps":[],"jobName":"paragraph_1520514503306_1297455873","id":"20180203-191531_1493631145","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1598"},{"text":"%alluxio\n","dateUpdated":"2018-03-08T13:08:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":"false"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520514503306_1297455873","id":"20180203-191630_1309776848","dateCreated":"2018-03-08T13:08:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1599"}],"name":"Extras/Working with BDFS Alluxio","id":"2D841KFYS","angularObjects":{"2D9GP2FG2:shared_process":[],"2D88VWR7S:shared_process":[],"2D9YN3AGB:shared_process":[],"2D94JQH3J:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2D7VVR3AY:shared_process":[],"2DA7BPW87:shared_process":[],"2DAJDDCMP:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}