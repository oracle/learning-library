{"paragraphs":[{"text":"%md\n# xtra tutorial: Working with Oracle Data Visualization Desktop and Spark\n\nThis tutorial was built for BDCS-CE version 17.3.5-20 and Data Visualization Desktop 3.0 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n\nOracle Data Visualization Desktop ( <a href=\"https://docs.oracle.com/middleware/bidv1221/desktop/index.html\" target=\"_blank\">here</a> ) is a lightweight, single-file download tool to easily analyze data.  Data Visualization Desktop can connect to a variety of data sources.  In this tutorial, we will show you how you can setup the Spark thrift server in BDCS-CE so that DVD can connect to it.\n","user":"anonymous","dateUpdated":"2017-10-18T13:49:02+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>xtra tutorial: Working with Oracle Data Visualization Desktop and Spark</h1>\n<p>This tutorial was built for BDCS-CE version 17.3.5-20 and Data Visualization Desktop 3.0 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#x64;&#97;&#118;i&#x64;&#46;b&#x61;&#x79;a&#114;&#100;&#64;o&#114;&#x61;c&#x6c;&#x65;.co&#x6d;\">&#x64;&#97;&#118;i&#x64;&#46;b&#x61;&#x79;a&#114;&#100;&#64;o&#114;&#x61;c&#x6c;&#x65;.co&#x6d;</a></p>\n<p>Oracle Data Visualization Desktop ( <a href=\"https://docs.oracle.com/middleware/bidv1221/desktop/index.html\" target=\"_blank\">here</a> ) is a lightweight, single-file download tool to easily analyze data. Data Visualization Desktop can connect to a variety of data sources. In this tutorial, we will show you how you can setup the Spark thrift server in BDCS-CE so that DVD can connect to it.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1507743977586_-1562736036","id":"20170504-171842_974565851","dateCreated":"2017-10-11T17:46:17+0000","dateStarted":"2017-10-18T13:49:02+0000","dateFinished":"2017-10-18T13:49:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:108"},{"text":"%md\n# Configuring the Spark Thrift Server process to use binary transport\n\nIn order to connect to Spark from DVD, we need to configure BDCS-CE's Spark Thrift Server to use the binary transport protocol.  By default, BDCS-CE's spark thrift server is configured to use the http transport protocol.  These changes are done using the Ambari web console.\n\nHere are the steps:\n\n1.Follow the note \"xtra Connecting to Ambari\" to login to Ambari.\n2.Once connected to Ambari, click on \"Spark2\" on the left-hand list of services\n3.Then click on the \"Configs\" tab\n4.In the search box, type \"server2\"\n5.In the Advanced spark2-hive-site-override section, change the \"hive.server2.transport.mode\" to binary.\n6.In the Custom spark2-hive-site-override section, remove the property \"hive.server2.thrift.bind.host\" (by clicking on the red - symbol)\n7.Clear out the search box.  Then navigate down to the Custom spark2-thrift-sparkconf section\n8.In the Custom spark2-thrift-sparkconf section, click on the \"Add Property...\" link and then add the property \"spark.sql.shuffle.partitions=4\" and click Add. \n9.Expand the Advanced spark2-env section and change spark_daemon_memory to 2048 MB. **This step is not yet shown in the animation.**\n10.Also in the Advanced spark2-env section in the \"content\" field, add the following uncommented line: **This step is not yet shown in the animation.**\n    SPARK_EXECUTOR_MEMORY=\"2G\" \n11.Click Save at the top of the screen.\n12.In the notes field, enter \"switch to binary transport\"\n13.Click save again\n14.If you see a \"Configurations\" pop-up, click \"Proceed Anyway\"\n15.Click OK to acknowledge that changes were made successfully\n16.Then click Restart, then Restart All Affected\n17.Then click Confirm Restart All\n\n![AmbariSparkBinary](https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/AmbariSparkBinary.gif)\n\n\n","user":"anonymous","dateUpdated":"2017-10-18T17:21:15+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Configuring the Spark Thrift Server process to use binary transport</h1>\n<p>In order to connect to Spark from DVD, we need to configure BDCS-CE&rsquo;s Spark Thrift Server to use the binary transport protocol. By default, BDCS-CE&rsquo;s spark thrift server is configured to use the http transport protocol. These changes are done using the Ambari web console.</p>\n<p>Here are the steps:</p>\n<p>1.Follow the note &ldquo;xtra Connecting to Ambari&rdquo; to login to Ambari.<br/>2.Once connected to Ambari, click on &ldquo;Spark2&rdquo; on the left-hand list of services<br/>3.Then click on the &ldquo;Configs&rdquo; tab<br/>4.In the search box, type &ldquo;server2&rdquo;<br/>5.In the Advanced spark2-hive-site-override section, change the &ldquo;hive.server2.transport.mode&rdquo; to binary.<br/>6.In the Custom spark2-hive-site-override section, remove the property &ldquo;hive.server2.thrift.bind.host&rdquo; (by clicking on the red - symbol)<br/>7.Clear out the search box. Then navigate down to the Custom spark2-thrift-sparkconf section<br/>8.In the Custom spark2-thrift-sparkconf section, click on the &ldquo;Add Property&hellip;&rdquo; link and then add the property &ldquo;spark.sql.shuffle.partitions=4&rdquo; and click Add.<br/>9.Expand the Advanced spark2-env section and change spark_daemon_memory to 2048 MB. <strong>This step is not yet shown in the animation.</strong><br/>10.Also in the Advanced spark2-env section in the &ldquo;content&rdquo; field, add the following uncommented line: <strong>This step is not yet shown in the animation.</strong><br/> SPARK_EXECUTOR_MEMORY=&ldquo;2G&rdquo;<br/>11.Click Save at the top of the screen.<br/>12.In the notes field, enter &ldquo;switch to binary transport&rdquo;<br/>13.Click save again<br/>14.If you see a &ldquo;Configurations&rdquo; pop-up, click &ldquo;Proceed Anyway&rdquo;<br/>15.Click OK to acknowledge that changes were made successfully<br/>16.Then click Restart, then Restart All Affected<br/>17.Then click Confirm Restart All</p>\n<p><img src=\"https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/AmbariSparkBinary.gif\" alt=\"AmbariSparkBinary\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1507743977587_-1563120785","id":"20170911-203514_1066756941","dateCreated":"2017-10-11T17:46:17+0000","dateStarted":"2017-10-18T17:21:15+0000","dateFinished":"2017-10-18T17:21:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"%md\n# Connecting to the Spark Thrift Server port (10016)\n\nNow, you need to decide how you want to connect to the Spark Thrift Server port, which is port 10016.  You can either choose to use a SSH tunnel (which is very secure) or choose to open port 10016 to the outside world (which can be less secure).\n\n+ If you want to use a SSH tunnel, refer to the note \"xtra Connecting to Ambari\" which has an example of setting up a SSH tunnel (but you would use port 10016 instead of Ambari's 8080).\n![SSHSparkBinary](https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/SSHSparkBinary.gif)\n+ If instead of a SSH tunnel, you want to open up port 10016 to the internet, then you will need to create a new access rule for port 10016.  Refer to the note \"xtra Connecting via SSH\" or \"OEHCS Tutorial 1\" for examples of working with network access rules.\n\n","dateUpdated":"2017-10-11T17:46:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Connecting to the Spark Thrift Server port (10016)</h1>\n<p>Now, you need to decide how you want to connect to the Spark Thrift Server port, which is port 10016. You can either choose to use a SSH tunnel (which is very secure) or choose to open port 10016 to the outside world (which can be less secure).</p>\n<ul>\n  <li>If you want to use a SSH tunnel, refer to the note &ldquo;xtra Connecting to Ambari&rdquo; which has an example of setting up a SSH tunnel (but you would use port 10016 instead of Ambari&rsquo;s 8080).<br/><img src=\"https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/SSHSparkBinary.gif\" alt=\"SSHSparkBinary\" /></li>\n  <li>If instead of a SSH tunnel, you want to open up port 10016 to the internet, then you will need to create a new access rule for port 10016. Refer to the note &ldquo;xtra Connecting via SSH&rdquo; or &ldquo;OEHCS Tutorial 1&rdquo; for examples of working with network access rules.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1507743977588_-1565044530","id":"20170911-204125_1313643018","dateCreated":"2017-10-11T17:46:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%md\n# Define a connection in DV Desktop for the Spark connection\n\n+ Open up DV Desktop\n+ Click on Data Sources\n+ Click on Connection (Under Create)\n+ Click on Spark\n+ Enter the Connection Name\n+ Enter the Host Name.  If you are using SSH tunneling, then enter 127.0.0.1 or localhost.  If you have opened up port 10016, then use the IP for your BDCSCE instance.\n+ Enter the Port. It should now be 10016.\n+ Enter spark for the username\n+ Enter x for the password\n\n![DVDconnection](https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/snap0012378.jpg)\n\n\n\n\n","dateUpdated":"2017-10-11T17:46:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Define a connection in DV Desktop for the Spark connection</h1>\n<ul>\n  <li>Open up DV Desktop</li>\n  <li>Click on Data Sources</li>\n  <li>Click on Connection (Under Create)</li>\n  <li>Click on Spark</li>\n  <li>Enter the Connection Name</li>\n  <li>Enter the Host Name. If you are using SSH tunneling, then enter 127.0.0.1 or localhost. If you have opened up port 10016, then use the IP for your BDCSCE instance.</li>\n  <li>Enter the Port. It should now be 10016.</li>\n  <li>Enter spark for the username</li>\n  <li>Enter x for the password</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/snap0012378.jpg\" alt=\"DVDconnection\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1507743977589_-1565429278","id":"20170728-174228_1743240708","dateCreated":"2017-10-11T17:46:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"text":"%md\n# Create a DV Desktop Data Source for your connection\n\n**NOTE: With BDCS 17.3.5 and DVD3, you might need to follow the workaround in the following paragraph**\n\n+ Invoke the pop-up menu on your new Data Source and choose Create Data Source\n+ Navigate through the database, tables, and columns to choose the elements you want to add.\n+ Once you have selected your table and columns, click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon).  Then, click on the Refresh property.  Change this to be \"Live - Always use the database\".  \n+ Name the new data source and Add it\n\n![DVDdatasource](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkDataSource.jpg \"HODBC\")\n","dateUpdated":"2017-10-11T17:46:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Create a DV Desktop Data Source for your connection</h1>\n<p><strong>NOTE: With BDCS 17.3.5 and DVD3, you might need to follow the workaround in the following paragraph</strong></p>\n<ul>\n  <li>Invoke the pop-up menu on your new Data Source and choose Create Data Source</li>\n  <li>Navigate through the database, tables, and columns to choose the elements you want to add.</li>\n  <li>Once you have selected your table and columns, click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be &ldquo;Live - Always use the database&rdquo;.</li>\n  <li>Name the new data source and Add it</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkDataSource.jpg\" alt=\"DVDdatasource\" title=\"HODBC\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1507743977589_-1565429278","id":"20170731-202416_1602004865","dateCreated":"2017-10-11T17:46:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"%md\n# Tip - DVD 3.0 with Spark 2.1 no databases appear\n\nThis may be https://issues.apache.org/jira/browse/SPARK-9686\n\nIn any case, we have noticed that no databases appear when querying Spark2.1 from DVD 3.0.\n\nThere is a workaround...\n\nWhen you create your data source, use the \"Enter SQL\" feature to define your sql.  It can be as simple as a \"select * from tablename\" or more complex.  \n\nAnd once you have entered your sql, be sure to click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be “Live - Always use the database”. \n\nHere is a video:\n![SparkCatalogWorkaround](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkCatalogWorkaround.gif)\n\n\n\n","dateUpdated":"2017-10-11T17:46:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tip - DVD 3.0 with Spark 2.1 no databases appear</h1>\n<p>This may be <a href=\"https://issues.apache.org/jira/browse/SPARK-9686\">https://issues.apache.org/jira/browse/SPARK-9686</a></p>\n<p>In any case, we have noticed that no databases appear when querying Spark2.1 from DVD 3.0.</p>\n<p>There is a workaround&hellip;</p>\n<p>When you create your data source, use the &ldquo;Enter SQL&rdquo; feature to define your sql. It can be as simple as a &ldquo;select * from tablename&rdquo; or more complex. </p>\n<p>And once you have entered your sql, be sure to click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be “Live - Always use the database”. </p>\n<p>Here is a video:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkCatalogWorkaround.gif\" alt=\"SparkCatalogWorkaround\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1507743977590_-1564275032","id":"20170911-210523_432676084","dateCreated":"2017-10-11T17:46:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"%md\n# Tip - Tracking Spark queries\n\nRun the following shell paragraph to peak at queries sent to the Spark thrift server.\n\n\n","dateUpdated":"2017-10-11T17:46:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tip - Tracking Spark queries</h1>\n<p>Run the following shell paragraph to peak at queries sent to the Spark thrift server.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1507743977590_-1564275032","id":"20170911-204536_1923836389","dateCreated":"2017-10-11T17:46:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"title":"Shell command to peak at queries sent to Spark Thrift Server","text":"%sh\negrep $'Running|\\x0d|limit' /data/var/log/spark2-thrift/spark-hive-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-*-1.out | tail -400\n","user":"anonymous","dateUpdated":"2017-10-18T13:50:27+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"     D101.c4 as `c6`,\r\n     D101.c5 as `c7`,\r\n     D101.c6 as `c8`,\r\n     D101.c7 as `c9`,\r\n     D101.c8 as `c10`,\r\n     D101.c9 as `c11`,\r\n     D101.c10 as `c12`,\r\n     D101.c11 as `c13`,\r\n     D101.c12 as `c14`,\r\n     D101.c13 as `c15`,\r\n     D101.c14 as `c16`,\r\n     D101.c15 as `c17`,\r\n     D101.c16 as `c18`,\r\n     D101.c17 as `c19`,\r\n     D101.c18 as `c20`,\r\n     D101.c19 as `c21`,\r\n     D101.c20 as `c22`,\r\n     D101.c21 as `c23`,\r\n     D101.c22 as `c24`,\r\n     D101.c23 as `c25`,\r\n     D101.c24 as `c26`,\r\n     D101.c25 as `c27`\r\nfrom \r\n     (select T1000001.AVERAGEWIND as `c1`,\r\n               T1000001.BIKEID as `c2`,\r\n               T1000001.BIRTHYEAR as `c3`,\r\n               T1000001.DAY_OF_WEEK as `c4`,\r\n               T1000001.ENDSTATIONID as `c5`,\r\n               T1000001.ENDSTATIONLATITUDE as `c6`,\r\n               T1000001.ENDSTATIONLONGITUDE as `c7`,\r\n               T1000001.ENDSTATIONNAME as `c8`,\r\n               T1000001.GENDER as `c9`,\r\n               T1000001.GENDER_CODE as `c10`,\r\n               T1000001.HOLIDAY as `c11`,\r\n               T1000001.MAXTEMPERATURE as `c12`,\r\n               T1000001.MINTEMPERATURE as `c13`,\r\n               T1000001.PRECIPITATION as `c14`,\r\n               T1000001.SNOW as `c15`,\r\n               T1000001.SNOW_ON_GROUND as `c16`,\r\n               T1000001.STARTSTATIONID as `c17`,\r\n               T1000001.STARTSTATIONLATITUDE as `c18`,\r\n               T1000001.STARTSTATIONLONGITUDE as `c19`,\r\n               T1000001.STARTSTATIONNAME as `c20`,\r\n               T1000001.STARTTIME as `c21`,\r\n               T1000001.STOPTIME as `c22`,\r\n               T1000001.TRIPDURATION as `c23`,\r\n               T1000001.USERTYPE as `c24`,\r\n               T1000001.WORKDAY as `c25`\r\n          from \r\n               (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000001\r\n     ) D101 limit 21' with 03fea739-99ab-433a-a629-24d2ab3ffe24\n17/10/11 19:24:21 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     ROW_NUMBER() OVER ( ORDER BY D101.c1) as `c2`,\r\n     D101.c1 as `c3`,\r\n     D101.c2 as `c4`,\r\n     D101.c3 as `c5`,\r\n     D101.c4 as `c6`,\r\n     D101.c5 as `c7`,\r\n     D101.c6 as `c8`,\r\n     D101.c7 as `c9`,\r\n     D101.c8 as `c10`,\r\n     D101.c9 as `c11`,\r\n     D101.c10 as `c12`,\r\n     D101.c11 as `c13`,\r\n     D101.c12 as `c14`,\r\n     D101.c13 as `c15`,\r\n     D101.c14 as `c16`,\r\n     D101.c15 as `c17`,\r\n     D101.c16 as `c18`,\r\n     D101.c17 as `c19`,\r\n     D101.c18 as `c20`,\r\n     D101.c19 as `c21`,\r\n     D101.c20 as `c22`,\r\n     D101.c21 as `c23`,\r\n     D101.c22 as `c24`,\r\n     D101.c23 as `c25`,\r\n     D101.c24 as `c26`,\r\n     D101.c25 as `c27`\r\nfrom \r\n     (select T1000001.AVERAGEWIND as `c1`,\r\n               T1000001.BIKEID as `c2`,\r\n               T1000001.BIRTHYEAR as `c3`,\r\n               T1000001.DAY_OF_WEEK as `c4`,\r\n               T1000001.ENDSTATIONID as `c5`,\r\n               T1000001.ENDSTATIONLATITUDE as `c6`,\r\n               T1000001.ENDSTATIONLONGITUDE as `c7`,\r\n               T1000001.ENDSTATIONNAME as `c8`,\r\n               T1000001.GENDER as `c9`,\r\n               T1000001.GENDER_CODE as `c10`,\r\n               T1000001.HOLIDAY as `c11`,\r\n               T1000001.MAXTEMPERATURE as `c12`,\r\n               T1000001.MINTEMPERATURE as `c13`,\r\n               T1000001.PRECIPITATION as `c14`,\r\n               T1000001.SNOW as `c15`,\r\n               T1000001.SNOW_ON_GROUND as `c16`,\r\n               T1000001.STARTSTATIONID as `c17`,\r\n               T1000001.STARTSTATIONLATITUDE as `c18`,\r\n               T1000001.STARTSTATIONLONGITUDE as `c19`,\r\n               T1000001.STARTSTATIONNAME as `c20`,\r\n               T1000001.STARTTIME as `c21`,\r\n               T1000001.STOPTIME as `c22`,\r\n               T1000001.TRIPDURATION as `c23`,\r\n               T1000001.USERTYPE as `c24`,\r\n               T1000001.WORKDAY as `c25`\r\n          from \r\n               (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000001\r\n     ) D101 limit 21\n17/10/11 19:24:45 INFO SparkExecuteStatementOperation: Running query 'use default' with 0505ddf9-023b-453e-adfe-05b4b6b8d7e3\n17/10/11 19:24:45 INFO SparkExecuteStatementOperation: Running query 'select * from (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T0 WHERE 1=0' with 0bd4a4c2-0c7f-4a6f-bcd4-e851eec4ad6c\n17/10/11 19:24:50 INFO SparkExecuteStatementOperation: Running query 'use default' with 87f09c49-2fc9-4cdb-883e-cf2143e4d69b\n17/10/11 19:24:51 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     ROW_NUMBER() OVER ( ORDER BY D101.c1) as `c2`,\r\n     D101.c1 as `c3`,\r\n     D101.c2 as `c4`,\r\n     D101.c3 as `c5`,\r\n     D101.c4 as `c6`,\r\n     D101.c5 as `c7`,\r\n     D101.c6 as `c8`,\r\n     D101.c7 as `c9`,\r\n     D101.c8 as `c10`,\r\n     D101.c9 as `c11`,\r\n     D101.c10 as `c12`,\r\n     D101.c11 as `c13`,\r\n     D101.c12 as `c14`,\r\n     D101.c13 as `c15`,\r\n     D101.c14 as `c16`,\r\n     D101.c15 as `c17`,\r\n     D101.c16 as `c18`,\r\n     D101.c17 as `c19`,\r\n     D101.c18 as `c20`,\r\n     D101.c19 as `c21`,\r\n     D101.c20 as `c22`,\r\n     D101.c21 as `c23`,\r\n     D101.c22 as `c24`,\r\n     D101.c23 as `c25`,\r\n     D101.c24 as `c26`,\r\n     D101.c25 as `c27`\r\nfrom \r\n     (select T1000001.AVERAGEWIND as `c1`,\r\n               T1000001.BIKEID as `c2`,\r\n               T1000001.BIRTHYEAR as `c3`,\r\n               T1000001.DAY_OF_WEEK as `c4`,\r\n               T1000001.ENDSTATIONID as `c5`,\r\n               T1000001.ENDSTATIONLATITUDE as `c6`,\r\n               T1000001.ENDSTATIONLONGITUDE as `c7`,\r\n               T1000001.ENDSTATIONNAME as `c8`,\r\n               T1000001.GENDER as `c9`,\r\n               T1000001.GENDER_CODE as `c10`,\r\n               T1000001.HOLIDAY as `c11`,\r\n               T1000001.MAXTEMPERATURE as `c12`,\r\n               T1000001.MINTEMPERATURE as `c13`,\r\n               T1000001.PRECIPITATION as `c14`,\r\n               T1000001.SNOW as `c15`,\r\n               T1000001.SNOW_ON_GROUND as `c16`,\r\n               T1000001.STARTSTATIONID as `c17`,\r\n               T1000001.STARTSTATIONLATITUDE as `c18`,\r\n               T1000001.STARTSTATIONLONGITUDE as `c19`,\r\n               T1000001.STARTSTATIONNAME as `c20`,\r\n               T1000001.STARTTIME as `c21`,\r\n               T1000001.STOPTIME as `c22`,\r\n               T1000001.TRIPDURATION as `c23`,\r\n               T1000001.USERTYPE as `c24`,\r\n               T1000001.WORKDAY as `c25`\r\n          from \r\n               (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000001\r\n     ) D101 limit 101' with 9d057c40-5d72-4774-81e6-acf1947d1948\n17/10/11 19:24:51 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     ROW_NUMBER() OVER ( ORDER BY D101.c1) as `c2`,\r\n     D101.c1 as `c3`,\r\n     D101.c2 as `c4`,\r\n     D101.c3 as `c5`,\r\n     D101.c4 as `c6`,\r\n     D101.c5 as `c7`,\r\n     D101.c6 as `c8`,\r\n     D101.c7 as `c9`,\r\n     D101.c8 as `c10`,\r\n     D101.c9 as `c11`,\r\n     D101.c10 as `c12`,\r\n     D101.c11 as `c13`,\r\n     D101.c12 as `c14`,\r\n     D101.c13 as `c15`,\r\n     D101.c14 as `c16`,\r\n     D101.c15 as `c17`,\r\n     D101.c16 as `c18`,\r\n     D101.c17 as `c19`,\r\n     D101.c18 as `c20`,\r\n     D101.c19 as `c21`,\r\n     D101.c20 as `c22`,\r\n     D101.c21 as `c23`,\r\n     D101.c22 as `c24`,\r\n     D101.c23 as `c25`,\r\n     D101.c24 as `c26`,\r\n     D101.c25 as `c27`\r\nfrom \r\n     (select T1000001.AVERAGEWIND as `c1`,\r\n               T1000001.BIKEID as `c2`,\r\n               T1000001.BIRTHYEAR as `c3`,\r\n               T1000001.DAY_OF_WEEK as `c4`,\r\n               T1000001.ENDSTATIONID as `c5`,\r\n               T1000001.ENDSTATIONLATITUDE as `c6`,\r\n               T1000001.ENDSTATIONLONGITUDE as `c7`,\r\n               T1000001.ENDSTATIONNAME as `c8`,\r\n               T1000001.GENDER as `c9`,\r\n               T1000001.GENDER_CODE as `c10`,\r\n               T1000001.HOLIDAY as `c11`,\r\n               T1000001.MAXTEMPERATURE as `c12`,\r\n               T1000001.MINTEMPERATURE as `c13`,\r\n               T1000001.PRECIPITATION as `c14`,\r\n               T1000001.SNOW as `c15`,\r\n               T1000001.SNOW_ON_GROUND as `c16`,\r\n               T1000001.STARTSTATIONID as `c17`,\r\n               T1000001.STARTSTATIONLATITUDE as `c18`,\r\n               T1000001.STARTSTATIONLONGITUDE as `c19`,\r\n               T1000001.STARTSTATIONNAME as `c20`,\r\n               T1000001.STARTTIME as `c21`,\r\n               T1000001.STOPTIME as `c22`,\r\n               T1000001.TRIPDURATION as `c23`,\r\n               T1000001.USERTYPE as `c24`,\r\n               T1000001.WORKDAY as `c25`\r\n          from \r\n               (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000001\r\n     ) D101 limit 101\n17/10/11 19:25:34 INFO SparkExecuteStatementOperation: Running query 'use default' with 73c974c1-7bf3-40b2-9085-9fb63d90bf05\n17/10/11 19:25:35 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`\r\nfrom \r\n     (select distinct case  when count(D101.c1) over ()  < 0 then count(D101.c1) over ()  else 0 end  as `c1`,\r\n               case  when count(D101.c1) over ()  > 0 then count(D101.c1) over ()  else 0 end  as `c2`,\r\n               count(D101.c1) over ()  as `c3`\r\n          from \r\n               (select T1000002.TRIPDURATION as `c1`\r\n                    from \r\n                         (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000002\r\n               ) D101\r\n     ) D103 limit 5000001' with d89e5b32-8c98-47f2-baf0-687a7ef5406a\n17/10/11 19:25:35 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`\r\nfrom \r\n     (select distinct case  when count(D101.c1) over ()  < 0 then count(D101.c1) over ()  else 0 end  as `c1`,\r\n               case  when count(D101.c1) over ()  > 0 then count(D101.c1) over ()  else 0 end  as `c2`,\r\n               count(D101.c1) over ()  as `c3`\r\n          from \r\n               (select T1000002.TRIPDURATION as `c1`\r\n                    from \r\n                         (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000002\r\n               ) D101\r\n     ) D103 limit 5000001\n17/10/11 19:25:46 INFO SparkExecuteStatementOperation: Running query 'use default' with 724e4029-70de-4016-84a2-ba843b0a534c\n17/10/11 19:25:47 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     D103.c4 as `c5`,\r\n     0 as `c6`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               case  when count(D101.c2) < 0 then count(D101.c2) else 0 end  as `c2`,\r\n               case  when count(D101.c2) > 0 then count(D101.c2) else 0 end  as `c3`,\r\n               count(D101.c2) as `c4`\r\n          from \r\n               (select T1000002.WORKDAY as `c1`,\r\n                         T1000002.TRIPDURATION as `c2`\r\n                    from \r\n                         (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000002\r\n               ) D101\r\n          group by D101.c1\r\n17/10/11 19:25:47 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     D103.c4 as `c5`,\r\n     0 as `c6`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               case  when count(D101.c2) < 0 then count(D101.c2) else 0 end  as `c2`,\r\n               case  when count(D101.c2) > 0 then count(D101.c2) else 0 end  as `c3`,\r\n               count(D101.c2) as `c4`\r\n          from \r\n               (select T1000002.WORKDAY as `c1`,\r\n                         T1000002.TRIPDURATION as `c2`\r\n                    from \r\n                         (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000002\r\n               ) D101\r\n          group by D101.c1\r\n17/10/11 19:26:37 INFO SparkExecuteStatementOperation: Running query 'use default' with a37331c9-1e0a-42ed-a9f3-6ed897ac6087\n17/10/11 19:26:38 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     cast(NULL as  INT  ) as `c4`,\r\n     cast(NULL as  INT  ) as `c5`,\r\n     D103.c3 as `c6`,\r\n     0 as `c7`,\r\n     0 as `c8`,\r\n     0 as `c9`,\r\n     D103.c4 as `c10`,\r\n     D103.c5 as `c11`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               D101.c2 as `c2`,\r\n               count(D101.c3) as `c3`,\r\n               case  when count(D101.c3) < 0 then count(D101.c3) else 0 end  as `c4`,\r\n               case  when count(D101.c3) > 0 then count(D101.c3) else 0 end  as `c5`\r\n          from \r\n               (select T1000002.DAY_OF_WEEK as `c1`,\r\n                         T1000002.WORKDAY as `c2`,\r\n                         T1000002.TRIPDURATION as `c3`\r\n                    from \r\n                         (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000002\r\n               ) D101\r\n          group by D101.c1, D101.c2\r\n17/10/11 19:26:38 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     cast(NULL as  INT  ) as `c4`,\r\n     cast(NULL as  INT  ) as `c5`,\r\n     D103.c3 as `c6`,\r\n     0 as `c7`,\r\n     0 as `c8`,\r\n     0 as `c9`,\r\n     D103.c4 as `c10`,\r\n     D103.c5 as `c11`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               D101.c2 as `c2`,\r\n               count(D101.c3) as `c3`,\r\n               case  when count(D101.c3) < 0 then count(D101.c3) else 0 end  as `c4`,\r\n               case  when count(D101.c3) > 0 then count(D101.c3) else 0 end  as `c5`\r\n          from \r\n               (select T1000002.DAY_OF_WEEK as `c1`,\r\n                         T1000002.WORKDAY as `c2`,\r\n                         T1000002.TRIPDURATION as `c3`\r\n                    from \r\n                         (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000002\r\n               ) D101\r\n          group by D101.c1, D101.c2\r\n17/10/11 19:28:08 INFO SparkExecuteStatementOperation: Running query 'use default' with ecaaa135-8878-416c-9f65-cb591c56b812\n17/10/11 19:28:09 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     cast(NULL as  INT  ) as `c4`,\r\n     cast(NULL as  INT  ) as `c5`,\r\n     D103.c3 as `c6`,\r\n     0 as `c7`,\r\n     0 as `c8`,\r\n     0 as `c9`,\r\n     D103.c4 as `c10`,\r\n     D103.c5 as `c11`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               D101.c2 as `c2`,\r\n               count(D101.c3) as `c3`,\r\n               case  when count(D101.c3) < 0 then count(D101.c3) else 0 end  as `c4`,\r\n               case  when count(D101.c3) > 0 then count(D101.c3) else 0 end  as `c5`\r\n          from \r\n               (select cast(D1000001.c1 as  TIMESTAMP  ) as `c1`,\r\n                         D1000001.c2 as `c2`,\r\n                         D1000001.c3 as `c3`\r\n                    from \r\n                         (select T1000002.STARTTIME as `c1`,\r\n                                   T1000002.DAY_OF_WEEK as `c2`,\r\n                                   T1000002.TRIPDURATION as `c3`\r\n                              from \r\n                                   (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000002\r\n                         ) D1000001\r\n               ) D101\r\n          group by D101.c1, D101.c2\r\n17/10/11 19:28:09 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     cast(NULL as  INT  ) as `c4`,\r\n     cast(NULL as  INT  ) as `c5`,\r\n     D103.c3 as `c6`,\r\n     0 as `c7`,\r\n     0 as `c8`,\r\n     0 as `c9`,\r\n     D103.c4 as `c10`,\r\n     D103.c5 as `c11`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               D101.c2 as `c2`,\r\n               count(D101.c3) as `c3`,\r\n               case  when count(D101.c3) < 0 then count(D101.c3) else 0 end  as `c4`,\r\n               case  when count(D101.c3) > 0 then count(D101.c3) else 0 end  as `c5`\r\n          from \r\n               (select cast(D1000001.c1 as  TIMESTAMP  ) as `c1`,\r\n                         D1000001.c2 as `c2`,\r\n                         D1000001.c3 as `c3`\r\n                    from \r\n                         (select T1000002.STARTTIME as `c1`,\r\n                                   T1000002.DAY_OF_WEEK as `c2`,\r\n                                   T1000002.TRIPDURATION as `c3`\r\n                              from \r\n                                   (select * from bike_trips_weather_parquet bike_trips_weather_parquet) T1000002\r\n                         ) D1000001\r\n               ) D101\r\n          group by D101.c1, D101.c2\r\n"}]},"apps":[],"jobName":"paragraph_1507743977590_-1564275032","id":"20170911-204553_822044476","dateCreated":"2017-10-11T17:46:17+0000","dateStarted":"2017-10-11T19:28:35+0000","dateFinished":"2017-10-11T19:28:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"text":"","dateUpdated":"2017-10-11T17:46:17+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1507743977591_-1564659781","id":"20170504-171816_753503470","dateCreated":"2017-10-11T17:46:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"dateUpdated":"2017-10-11T17:46:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1507743977592_-1566583525","id":"20170921-124733_1996309865","dateCreated":"2017-10-11T17:46:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:117"}],"name":"xtra Connecting DV Desktop and Spark","id":"2CWA51HWR","angularObjects":{"2CXZJ9QJB:shared_process":[],"2CXPDTWPH:shared_process":[],"2CWDYXPTB:shared_process":[],"2CVW3CJG6:shared_process":[],"2CUQSP17V:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CWBKB9Z5:shared_process":[],"2CX6D87EF:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}