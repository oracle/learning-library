{"paragraphs":[{"text":"%md\n# Demonstration: Analyzing Presidential Speeches with Spark and Spark SQL\n\nThis tutorial was built for BDCS-CE version 17.3.1.  If you are using a later version of BDCS-CE, there may be a newer version of this tutorial notebook at <https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake> . Questions and feedback about the tutorial: david.bayard@oracle.com\n\n**Contents**\n\n+ About Spark and Spark SQL\n+ Example 1: Spark(Scala) with HDFS data\n+ Example 2: Spark(Python) with HDFS data\n+ Example 3: Spark(Scala) with linux file system data\n+ Example 4: Spark(Scala) with Object Store data\n+ Example 5: Wordcount(Spark Scala) [choose HDFS, linux, or Object Store data]\n+ Example 6: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)\n+ Example 7: Spark SQL against our Wordcounts table\n+ Example 8: More complex example- Speech Trends across time\n+ Example 9: Saving results to the Object Store\n+ Optional: Explore the Spark UI\n+ Next Steps\n\n\nAs a reminder, the documentation for BDCS-CE can be found here: <http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html>\n\n**NOTE: Please ensure that you have run the \"Working with the Object Store\" tutorial first**\n","user":"anonymous","dateUpdated":"2017-07-28T13:56:52+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Demonstration: Analyzing Presidential Speeches with Spark and Spark SQL</h1>\n<p>This tutorial was built for BDCS-CE version 17.3.1. If you are using a later version of BDCS-CE, there may be a newer version of this tutorial notebook at <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\">https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake</a> . Questions and feedback about the tutorial: <a href=\"mailto:&#100;&#97;&#118;&#x69;&#100;&#46;&#x62;&#97;&#x79;&#x61;&#x72;&#x64;&#64;&#x6f;&#114;&#97;c&#108;&#x65;&#x2e;c&#x6f;&#109;\">&#100;&#97;&#118;&#x69;&#100;&#46;&#x62;&#97;&#x79;&#x61;&#x72;&#x64;&#64;&#x6f;&#114;&#97;c&#108;&#x65;&#x2e;c&#x6f;&#109;</a></p>\n<p><strong>Contents</strong></p>\n<ul>\n  <li>About Spark and Spark SQL</li>\n  <li>Example 1: Spark(Scala) with HDFS data</li>\n  <li>Example 2: Spark(Python) with HDFS data</li>\n  <li>Example 3: Spark(Scala) with linux file system data</li>\n  <li>Example 4: Spark(Scala) with Object Store data</li>\n  <li>Example 5: Wordcount(Spark Scala) [choose HDFS, linux, or Object Store data]</li>\n  <li>Example 6: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)</li>\n  <li>Example 7: Spark SQL against our Wordcounts table</li>\n  <li>Example 8: More complex example- Speech Trends across time</li>\n  <li>Example 9: Saving results to the Object Store</li>\n  <li>Optional: Explore the Spark UI</li>\n  <li>Next Steps</li>\n</ul>\n<p>As a reminder, the documentation for BDCS-CE can be found here: <a href=\"http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\">http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html</a></p>\n<p><strong>NOTE: Please ensure that you have run the &ldquo;Working with the Object Store&rdquo; tutorial first</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597115_-1125077504","id":"20170310-125455_1477278610","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T13:56:45+0000","dateFinished":"2017-07-28T13:56:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:628"},{"text":"%md\n# About Spark and Spark SQL\n\nBDCS-CE version 17.3.1 comes with Spark version 1.6.1 (it also has Spark2 but this tutorial was written for Spark 1.6), Scala version 2.10, and Python version 2.6.6.  This BDCS-CE version supplies Zeppelin interpreters for Spark(Scala), Spark(Python), and Spark SQL.  This tutorial will give you examples using all of these.\n\nThe tutorial assumes you have a basic knowledge about Spark.  To learn more about Spark, check out <https://spark.apache.org/docs/1.6.1/quick-start.html> and <https://spark.apache.org/docs/1.6.1/sql-programming-guide.html>\n\n\n","user":"anonymous","dateUpdated":"2017-07-28T01:01:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About Spark and Spark SQL</h1>\n<p>BDCS-CE version 17.3.1 comes with Spark version 1.6.1 (it also has Spark2 but this tutorial was written for Spark 1.6), Scala version 2.10, and Python version 2.6.6. This BDCS-CE version supplies Zeppelin interpreters for Spark(Scala), Spark(Python), and Spark SQL. This tutorial will give you examples using all of these.</p>\n<p>The tutorial assumes you have a basic knowledge about Spark. To learn more about Spark, check out <a href=\"https://spark.apache.org/docs/1.6.1/quick-start.html\">https://spark.apache.org/docs/1.6.1/quick-start.html</a> and <a href=\"https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\">https://spark.apache.org/docs/1.6.1/sql-programming-guide.html</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597116_-1127001249","id":"20170403-145735_495154166","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:01:24+0000","dateFinished":"2017-07-28T01:01:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:629"},{"text":"%md\n# Example 1: Spark(Scala) with HDFS data\n\nOur first example will show a simple bit of Scala code accessing HDFS data.  Our example defines a Spark RDD (Resilient Distributed Dataset) against a text file stored in HDFS.  Then it runs a few actions against the RDD, such as counting the # of lines, displaying the first line, and counting the number of lines matching a given term. \n","user":"anonymous","dateUpdated":"2017-07-28T01:01:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 1: Spark(Scala) with HDFS data</h1>\n<p>Our first example will show a simple bit of Scala code accessing HDFS data. Our example defines a Spark RDD (Resilient Distributed Dataset) against a text file stored in HDFS. Then it runs a few actions against the RDD, such as counting the # of lines, displaying the first line, and counting the number of lines matching a given term.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597116_-1127001249","id":"20170403-152657_222008853","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:01:31+0000","dateFinished":"2017-07-28T01:01:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:630"},{"title":"Scala example with HDFS data","text":"%spark\n// to run Scala Spark code in zeppelin, use the %spark interpreter\n\n\n// Scala example\n\nprintln(\"Define a RDD against a HDFS textfile...\")\nval textFile = sc.textFile(\"speeches_hdfs/pres1861_lincoln.txt\")\n// Spark defaults to HDFS when specifying filenames.  This file was copied to HDFS when you ran the previous tutorial.\nprintln(\"..\")\n\n\nprintln(\"Count of # of rows...\")\ntextFile.count() // Number of lines in this RDD\nprintln(\"..\")\n\n\nprintln(\"The first line: %s\".format(\n    textFile.first() // First line in this RDD\n))\nprintln(\"..\")\n\nprintln(\"# of Lines with Constitution: %s\".format(\n  textFile.filter(line => line.contains(\"Constitution\")).count() \n)) // How many lines contain \"Constitution\"?\nprintln(\"..\")\n\nprintln(\"done\")","user":"anonymous","dateUpdated":"2017-07-28T13:57:01+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597116_-1127001249","id":"20170310-125552_378911250","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T13:57:01+0000","dateFinished":"2017-07-28T13:57:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:631","errorMessage":""},{"text":"%md\n# Example 2: Spark(Python) with HDFS data\n\nOur second example will show a simple bit of Python code accessing HDFS data.  It does the same logic as the first example, just using Python instead of Scala. \n","user":"anonymous","dateUpdated":"2017-07-28T01:01:38+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 2: Spark(Python) with HDFS data</h1>\n<p>Our second example will show a simple bit of Python code accessing HDFS data. It does the same logic as the first example, just using Python instead of Scala.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597117_-1127385998","id":"20170403-154326_852166402","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:01:38+0000","dateFinished":"2017-07-28T01:01:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:632"},{"title":"Python example with HDFS data","text":"%pyspark\n# to run Python Spark code in zeppelin, use the %pyspark interpreter\n\n\n#python example\n\n\ntextFile = sc.textFile(\"speeches_hdfs/pres1861_lincoln.txt\")\n\nnumCount=textFile.count() # Number of lines in this RDD\nprint(\"Count of # of lines is %i\" % numCount)\n\nfirst = textFile.first() # First line in this RDD\nprint(\"The First Line is:\" + first)\n\n\nnumConst = textFile.filter(lambda line: \"Constitution\" in line).count() # How many lines contain \"Constitution\"?\nprint(\"# of Lines with Constitution is %i\" % numConst)","user":"anonymous","dateUpdated":"2017-07-28T13:57:10+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597118_-1126231751","id":"20170314-130540_420933051","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T13:57:10+0000","dateFinished":"2017-07-28T13:57:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:633","errorMessage":""},{"text":"%md\n# Example 3: Spark(Scala) with linux file system data\n\nOur third example is a slight variation of the first example.  The only difference (the 3rd line of the example) is that it uses data on the zeppelin's server linux file system, not HDFS.\n","user":"anonymous","dateUpdated":"2017-07-28T01:01:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 3: Spark(Scala) with linux file system data</h1>\n<p>Our third example is a slight variation of the first example. The only difference (the 3rd line of the example) is that it uses data on the zeppelin&rsquo;s server linux file system, not HDFS.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597118_-1126231751","id":"20170403-154637_1284130064","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:01:44+0000","dateFinished":"2017-07-28T01:01:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:634"},{"title":"Example with local linux file system data (Scala)","text":"%spark\n// Scala example\n\nprintln(\"Define a RDD against a local filesystem textfile...\")\n\nval textFile = sc.textFile(\"file:/var/lib/zeppelin/speeches/pres1933_fdr1.txt\")\n// Observe that we specified the file: namespace to point to a file in the local linux file system (this file was copied there when you ran the previous tutorial)\n\n\nprintln(\"..\")\n\n\nprintln(\"Count of # of rows...\")\ntextFile.count() // Number of items in this RDD\nprintln(\"..\")\n\n\nprintln(\"The first line: %s\".format(\n    textFile.first() // First item in this RDD\n))\nprintln(\"..\")\n\nprintln(\"# of Lines with Constitution: %s\".format(\n  textFile.filter(line => line.contains(\"Constitution\")).count() \n)) // How many lines contain \"Constitution\"?\nprintln(\"..\")\n\nprintln(\"done\")","user":"anonymous","dateUpdated":"2017-07-28T13:57:15+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597121_-1141236958","id":"20170310-125626_48474354","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T13:57:16+0000","dateFinished":"2017-07-28T13:57:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:635","errorMessage":""},{"text":"%md\n# Example 4: Spark(Scala) with Object Store data\n\nOur fourth example is another variation of the first example.  The difference is that it works with data stored on the Object Store, not in HDFS.\n\nThis example will use the \"default\" Object Store connection, which is the Object Store credentials you specified when you created this BDCS-CE instance.  See the end of this tutorial for an example of how to define additional Object Store connections.\n","user":"anonymous","dateUpdated":"2017-07-28T01:01:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 4: Spark(Scala) with Object Store data</h1>\n<p>Our fourth example is another variation of the first example. The difference is that it works with data stored on the Object Store, not in HDFS.</p>\n<p>This example will use the &ldquo;default&rdquo; Object Store connection, which is the Object Store credentials you specified when you created this BDCS-CE instance. See the end of this tutorial for an example of how to define additional Object Store connections.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597121_-1141236958","id":"20170403-155009_862645126","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:01:51+0000","dateFinished":"2017-07-28T01:01:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:636"},{"title":"Example with Object Store data (Scala)","text":"%spark\n\nprintln(\"Define a RDD against a Object Store textfile...\")\nval textFile = sc.textFile(\"swift://demo.default/speeches/pres1981_reagan1.txt\")\n// Observe that we are specifying our file by the swift://ContainerName.ServiceName/ namespace.  The ServiceName (default in this example) points to set of configuration parameters.  The default configuration parameters were defined when you created this BDCS-CE instance. \n\n\nprintln(\"..\")\n\nprintln(\"Count of # of lines: %s\".format(\n    textFile.count() \n))    // Number of items in this RDD\nprintln(\"..\")\n\nprintln(\"The First Line: %s\".format(\n    textFile.first() // First item in this RDD\n))\nprintln(\"..\")\n\nprintln(\"# Lines with Constitution: %s\".format(\n  textFile.filter(line => line.contains(\"Constitution\")).count() \n)) // How many lines contain \"Constitution\"?\nprintln(\"..\")\n\nprintln(\"done\")","user":"anonymous","dateUpdated":"2017-07-28T14:09:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{"container":"demo"},"forms":{}},"apps":[],"jobName":"paragraph_1501203597122_-1140082711","id":"20170314-144912_872094449","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:09:27+0000","dateFinished":"2017-07-28T14:09:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:637","errorMessage":""},{"text":"%md\n# Example 5: Wordcount(Spark Scala) [choose HDFS, linux, or Object Store data]\n\nOur fifth example runs the classic Wordcount algorithm.  You can choose if you want to operate on HDFS, Object Store, or linux file system data by commenting out the appropriate code.  \n\nThe results of the Wordcount are an RDD named wordCounts that will also be used in the following example.\n","user":"anonymous","dateUpdated":"2017-07-28T01:01:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 5: Wordcount(Spark Scala) [choose HDFS, linux, or Object Store data]</h1>\n<p>Our fifth example runs the classic Wordcount algorithm. You can choose if you want to operate on HDFS, Object Store, or linux file system data by commenting out the appropriate code. </p>\n<p>The results of the Wordcount are an RDD named wordCounts that will also be used in the following example.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597122_-1140082711","id":"20170403-162628_622133907","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:01:59+0000","dateFinished":"2017-07-28T01:01:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:638"},{"title":"Wordcount Example (scala)","text":"%spark\n\n// pick your choice of file: HDFS, Local, or Object Store.  Uncomment one of the below.\n// val textFile = sc.textFile(\"speeches_hdfs/pres1861_lincoln1.txt\")\n// val textFile = sc.textFile(\"file:/var/lib/zeppelin/speeches/pres1861_lincoln1.txt\")\nval textFile = sc.textFile(\"swift://demo.default/speeches/pres1861_lincoln1.txt\")\n\n// Now let's do the classic wordcount example with our data\n// if you are curious, the replaceAll() strips out punctuation and the split() separates the string into words.  Both use regex expressions.\nval wordCounts = textFile.flatMap(line => line.replaceAll(\"[^\\\\w ]\",\" \").toLowerCase().split(\" +\")).map(word => (word, 1)).reduceByKey((a, b) => a + b)\n\nprintln(\"..\")\n\nprintln(\"Here is the output (partially shown)\")\nwordCounts.collect()\n\nprintln(\"\")\nprintln(\"..\")\n\nprintln(\"The first word is %s and the count of it is %s\".format(wordCounts.first()_1,wordCounts.first()_2))\nprintln(\"..\")\n\nprintln(\"done\")","user":"anonymous","dateUpdated":"2017-07-28T14:09:46+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597122_-1140082711","id":"20170310-125659_767950724","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:09:46+0000","dateFinished":"2017-07-28T14:09:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:639","errorMessage":""},{"text":"%md\n# Example 6: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)\n\nOur sixth example continues from the previous example. Specifically, it takes the wordCounts RDD and registers it as Spark DataFrame.  Then it registers the new data frame as a temporary Spark SQL table.\n\nAt this point, you might want to quickly review the Spark SQL programming guide for a refresher about Creating DataFrames from RDDs: <https://spark.apache.org/docs/1.6.1/sql-programming-guide.html>\n\n","user":"anonymous","dateUpdated":"2017-07-28T01:02:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 6: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)</h1>\n<p>Our sixth example continues from the previous example. Specifically, it takes the wordCounts RDD and registers it as Spark DataFrame. Then it registers the new data frame as a temporary Spark SQL table.</p>\n<p>At this point, you might want to quickly review the Spark SQL programming guide for a refresher about Creating DataFrames from RDDs: <a href=\"https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\">https://spark.apache.org/docs/1.6.1/sql-programming-guide.html</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597123_-1140467460","id":"20170403-163207_132564105","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:02:07+0000","dateFinished":"2017-07-28T01:02:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:640"},{"title":"Converting a RDD to a DataFrame and Registering it as Table (scala)","text":"%spark\n\n// this example follows the \"Inferring the Schema using Reflection\" of converting an RDD to a DataFrame. See https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\n\n// first define a case class that describes our Schema \ncase class Wordcount(word: String, wcount: Int)\nprintln(\"..\")\n\n// second, map our wordCounts RDD into an RDD using our Wordcount class and convert that into a DataFrame\nval wordcountsDF = wordCounts.map(p => Wordcount(p._1, p._2)).toDF()\nprintln(\"..\")\n\n// now register our wordcountsDF data frame as a temporary Spark SQL table\nwordcountsDF.registerTempTable(\"wordcounts\")\nprintln(\"..\")\n\nprintln(\"done\")","user":"anonymous","dateUpdated":"2017-07-28T14:09:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597124_-1142391205","id":"20170314-135312_277685101","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:09:55+0000","dateFinished":"2017-07-28T14:09:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:641","errorMessage":""},{"text":"%md\n# Example 7: Spark SQL against our wordcounts table\n\nOur seventh example continues from the previous example. Specifically, it provides two examples of running Spark SQL against our wordcounts table.  It also demonstrates some of the features of Zeppelin's Spark SQL interpreter and display visualization capabilities.\n","user":"anonymous","dateUpdated":"2017-07-28T01:02:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 7: Spark SQL against our wordcounts table</h1>\n<p>Our seventh example continues from the previous example. Specifically, it provides two examples of running Spark SQL against our wordcounts table. It also demonstrates some of the features of Zeppelin&rsquo;s Spark SQL interpreter and display visualization capabilities.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597124_-1142391205","id":"20170403-164432_219780017","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:02:13+0000","dateFinished":"2017-07-28T01:02:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:642"},{"title":"Spark SQL against our WordCounts table","text":"%sql\nselect * from wordcounts \nwhere length(word)>4\norder by wcount desc limit 15","user":"anonymous","dateUpdated":"2017-07-28T14:09:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"wcount","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"wcount","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597124_-1142391205","id":"20170314-140802_470559038","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:09:59+0000","dateFinished":"2017-07-28T14:10:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:643","errorMessage":""},{"title":"More Spark SQL","text":"%sql\nselect * from wordcounts\nwhere word in ('god','america','constitution','nation','people','states','rights','freedom','union')\norder by wcount desc","user":"anonymous","dateUpdated":"2017-07-28T13:57:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"wcount","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"wcount","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597125_-1142775954","id":"20170314-140955_1723818486","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T13:57:57+0000","dateFinished":"2017-07-28T13:57:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:644","errorMessage":""},{"text":"%md\n# Example 8: More complex example- Speech Trends across time\n\nOur eighth example continues from the previous example. Specifically, it builds an RDD against all of the speeches we stored in the Object Store.  Then it performs a word count against the RDD and converts the result into a Data Frame that we register as a Spark SQL temporary table.  Then we use some SparkSQL to prepare a couple of charts.\n","user":"anonymous","dateUpdated":"2017-07-28T01:02:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 8: More complex example- Speech Trends across time</h1>\n<p>Our eighth example continues from the previous example. Specifically, it builds an RDD against all of the speeches we stored in the Object Store. Then it performs a word count against the RDD and converts the result into a Data Frame that we register as a Spark SQL temporary table. Then we use some SparkSQL to prepare a couple of charts.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597125_-1142775954","id":"20170403-165335_535102146","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:02:21+0000","dateFinished":"2017-07-28T01:02:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:645"},{"text":"%spark\n// more complex example.  Let's look for trends across presidents\n\n\nval filebase = \"swift://demo.default/speeches/pres\"\n\nval textFiles = sc.wholeTextFiles(filebase+\"*.txt\")\n\nprintln(\"We found %s speeches\".format(textFiles.count()))\nprintln(\"..\")\n\n// this cleans up the filename to make it shorter and removes some punctuation from the text\nval words = textFiles.map(file => (file._1.replace(filebase,\"\"),file._2.replaceAll(\"[^\\\\w ]\",\" \").toLowerCase()))\nprintln(\"..\")\n\n// this splits the String of text into an array of words/Strings\nval wordArray = words.map(file => (file._1,file._2.split(\" +\")))\nprintln(\"..\")\n\n// define our class/schema\ncase class filewordsraw(file: String, words: Array[String])\nprintln(\"..\")\n\n// create a DataFrame from our RDD\nval filewordsrawDF = wordArray.map(p => filewordsraw(p._1, p._2)).toDF()\nprintln(\"..\")\n\n// show the schema\nfilewordsrawDF.printSchema()\nprintln(\"..\")\n\n// register this as a table (we'll do the rest of the word count in SQL)\nfilewordsrawDF.registerTempTable(\"filewordsraw\")\nprintln(\"..\")\n\nprintln(\"done\")\n","user":"anonymous","dateUpdated":"2017-07-28T14:10:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597126_-1141621707","id":"20170314-161337_919830878","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:10:09+0000","dateFinished":"2017-07-28T14:10:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:646","errorMessage":""},{"title":"SQL Showing the # of words per Speech","text":"%sql\nselect file, count(*) words from (select file, explode(words) word from filewordsraw) a\ngroup by file\norder by file","user":"anonymous","dateUpdated":"2017-07-28T14:10:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"keys":[{"name":"file","index":0,"aggr":"sum"}],"values":[{"name":"words","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"file","index":0,"aggr":"sum"}},"forceY":true}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597126_-1141621707","id":"20170403-165913_699538396","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:10:20+0000","dateFinished":"2017-07-28T14:10:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:647","errorMessage":""},{"title":"Chart showing the usage of certain words over time","text":"%sql\nselect file, word, count(*) wc from (select file, explode(words) word from filewordsraw) a\nwhere word in ('god','america','constitution','nation','rights','freedom','people')\ngroup by file, word\norder by file","user":"anonymous","dateUpdated":"2017-07-28T14:10:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"keys":[{"name":"file","index":0,"aggr":"sum"}],"values":[{"name":"wc","index":2,"aggr":"sum"}],"groups":[{"name":"word","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"file","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597126_-1141621707","id":"20170314-173832_82214833","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:10:30+0000","dateFinished":"2017-07-28T14:10:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:648","errorMessage":""},{"text":"%md\n# Example 9: Saving a Data Frame back to the Object Store\n\nOur final example continues from the previous example. Specifically, it defines a new data frame based off one of the sample SQL statements and writes that data frame back to the Object Store (as a json file).  Then it reads the json data back from the Object Store into a new Data Frame.\n","user":"anonymous","dateUpdated":"2017-07-28T01:02:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example 9: Saving a Data Frame back to the Object Store</h1>\n<p>Our final example continues from the previous example. Specifically, it defines a new data frame based off one of the sample SQL statements and writes that data frame back to the Object Store (as a json file). Then it reads the json data back from the Object Store into a new Data Frame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597127_-1142006456","id":"20170403-200206_1232128960","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:02:30+0000","dateFinished":"2017-07-28T01:02:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:649"},{"text":"%spark\n\n// \nval results = sqlContext.sql(\"select file, count(*) words from (select file, explode(words) word from filewordsraw) a group by file\")\nprintln(\"..\")\n\nval filebase = \"swift://demo.default/speeches/pres_wordcount.json\"\nprintln(\"..\")\n\n// save in json format.  the repartition(1) ensures that we write a single output file, which makes sense since we know the output is small\nresults.repartition(1).write.format(\"json\").mode(\"overwrite\").save(filebase)\nprintln(\"..\")\n\n// now lets read it back\nval df = sqlContext.read.format(\"json\").load(filebase)\nprintln(\"..\")\n\ndf.collect()\nprintln(\"..\")\n\nprintln(\"done\")\n\n","user":"anonymous","dateUpdated":"2017-07-28T14:10:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501203597128_-1143930200","id":"20170403-200515_1585180633","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:10:46+0000","dateFinished":"2017-07-28T14:11:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:650","errorMessage":""},{"title":"Optional: List the contents of your Object Store container to see the structure of the saved data frame.","text":"%sh\nsource swift_env.sh\nswift list demo","user":"anonymous","dateUpdated":"2017-07-28T14:15:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"editOnDblClick":"false","language":"sh"},"editorHide":false,"tableHide":false},"settings":{"params":{"container":"demo"},"forms":{}},"apps":[],"jobName":"paragraph_1501203597128_-1143930200","id":"20170403-203400_319359235","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:11:23+0000","dateFinished":"2017-07-28T14:11:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:651","errorMessage":""},{"text":"%md\n# Optional: Explore the Spark UI\n\nWhen you use Spark (via Scala, Python, and/or SQL), you start a session with the Spark server.  In many situations, it can be helpful to view the \"Spark UI\" for your session.  BDCS-CE provides easy access to Spark UI for your Zeppelin session.\n\nTo view it, follow these steps...\n\n + Click on the Jobs tab\n + Find the running (\"Processing\") Zeppelin job\n + From the pop-up menu, choose Spark UI\n![spark ui](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011441.jpg \"Spark UI\")\n + Click on Attempt_1\n + Explore the Spark UI\n![SparkUI](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011442.jpg \"Spark UI\")\n\n\n\n\n\n","user":"anonymous","dateUpdated":"2017-07-28T14:15:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Optional: Explore the Spark UI</h1>\n<p>When you use Spark (via Scala, Python, and/or SQL), you start a session with the Spark server. In many situations, it can be helpful to view the &ldquo;Spark UI&rdquo; for your session. BDCS-CE provides easy access to Spark UI for your Zeppelin session.</p>\n<p>To view it, follow these steps&hellip;</p>\n<ul>\n  <li>Click on the Jobs tab</li>\n  <li>Find the running (&ldquo;Processing&rdquo;) Zeppelin job</li>\n  <li>From the pop-up menu, choose Spark UI<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011441.jpg\" alt=\"spark ui\" title=\"Spark UI\" /></li>\n  <li>Click on Attempt_1</li>\n  <li>Explore the Spark UI<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011442.jpg\" alt=\"SparkUI\" title=\"Spark UI\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597128_-1143930200","id":"20170405-085653_1479537842","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:15:29+0000","dateFinished":"2017-07-28T14:15:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:652"},{"text":"%md\n# Next Steps\n\n+ Review the Spark and Spark SQL documentation at <https://spark.apache.org/docs/1.6.1/quick-start.html> and <https://spark.apache.org/docs/1.6.1/sql-programming-guide.html>\n+ Experiment with your own data sets\n+ Proceed to one of the other tutorials","user":"anonymous","dateUpdated":"2017-07-28T01:02:50+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Next Steps</h1>\n<ul>\n  <li>Review the Spark and Spark SQL documentation at <a href=\"https://spark.apache.org/docs/1.6.1/quick-start.html\">https://spark.apache.org/docs/1.6.1/quick-start.html</a> and <a href=\"https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\">https://spark.apache.org/docs/1.6.1/sql-programming-guide.html</a></li>\n  <li>Experiment with your own data sets</li>\n  <li>Proceed to one of the other tutorials</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597129_-1144314949","id":"20170310-130044_26976236","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:02:50+0000","dateFinished":"2017-07-28T01:02:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:653"},{"text":"%md\n# Appendix: Programmatically defining additional connections to the Object Store\n\nBDCS-CE defines a \"default\" set of Object Store credentials for use with Spark/Hadoop.  The following paragraph shows you how to define an additional set of credentials if needed.  In the paragraph below, the new set is called \"main\".  Once defined in your session, you can then refer to objects in the Object Store via paths like \"swift://container.main/speeches/pres_wordcount.json\", where you substitute the real container name for \"container\".\n\n\nNote: with BDCS-CE version 17.2.1, some of the values you set for the Object Store configuration will be cached/fixed for your running Spark session (which will continue indefinitely).  Therefore, you may need to restart the Zeppelin notebook (via the Settings tab) to pickup a different value.","user":"anonymous","dateUpdated":"2017-07-28T01:02:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Appendix: Programmatically defining additional connections to the Object Store</h1>\n<p>BDCS-CE defines a &ldquo;default&rdquo; set of Object Store credentials for use with Spark/Hadoop. The following paragraph shows you how to define an additional set of credentials if needed. In the paragraph below, the new set is called &ldquo;main&rdquo;. Once defined in your session, you can then refer to objects in the Object Store via paths like &ldquo;<a href=\"swift://container.main/speeches/pres_wordcount.json\"\">swift://container.main/speeches/pres_wordcount.json\"</a>, where you substitute the real container name for &rdquo;container&quot;.</p>\n<p>Note: with BDCS-CE version 17.2.1, some of the values you set for the Object Store configuration will be cached/fixed for your running Spark session (which will continue indefinitely). Therefore, you may need to restart the Zeppelin notebook (via the Settings tab) to pickup a different value.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597129_-1144314949","id":"20170413-132914_1396761079","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T01:02:59+0000","dateFinished":"2017-07-28T01:02:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:654"},{"title":"Configuring Spark for the Object Store (Scala)","text":"%spark\n\n// Snippet of code to configure your Spark session to connect to the Object Store.  Do this at least once per session.  \n// If you subsequently change one of these values, you may need to restart the notebook (at least as of 17.2.1)\n\n// This section uses some Zepellin APIs to create some user entry fields\n// Learn more here: https://zeppelin.apache.org/docs/0.6.1/interpreter/spark.html#form-creation\n/* Create text input form with default value */\nz.angularBind(\"BIND_tenant\", z.input(\"identity domain\", \"gse00010212\"))\nz.angularBind(\"BIND_username\", z.input(\"username\", \"cloud.admin\"))\nz.angularBind(\"BIND_password\", z.input(\"password\", \"yourpassword\"))\n\n// This section sets properties in the HadoopConfiguration for a Object Store service name we will call main.\nval hconf = sc.hadoopConfiguration;\nhconf.set(\"fs.swift.SERVICE_NAME\", \"main\");\nhconf.set(\"fs.swift.service.main.auth.url\", \"https://\"+z.angular(\"BIND_tenant\")+\".storage.oraclecloud.com/auth/v2.0/tokens\");\nhconf.set(\"fs.swift.service.main.public\", \"true\");\nhconf.set(\"fs.swift.service.http.location-aware\", \"false\");\n\n// Fill up the credentials for Oracle cloud storage\nhconf.set(\"fs.swift.service.main.tenant\", \"Storage-\"+z.angular(\"BIND_tenant\"));\nhconf.set(\"fs.swift.service.main.username\", z.angular(\"BIND_username\").toString());\nhconf.set(\"fs.swift.service.main.password\", z.angular(\"BIND_password\").toString());\n\n// hint to Python users: if you are looking to set HadoopConfiguration parameters in Python code, try using Python syntax like this:\n// sc._jsc.hadoopConfiguration().set('fs.swift.SERVICE_NAME', 'main')\n\n// Note: The above properties can also be set in /etc/hadoop/conf/core-site.xml \n// The directory location should be what is reported by the scala sys.env(\"HADDOP_CONF_DIR\") output.\n// for more details, see https://spark.apache.org/docs/1.6.1/storage-openstack-swift.html\n\nprintln(\"Sanity check.  Here is the auth.url: \" + hconf.get(\"fs.swift.service.main.auth.url\"))\nprintln(\"Done\")","dateUpdated":"2017-07-28T00:59:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{"container":"dcb-bdcs-apr12","identity domain":"gse00010212","password":"xxxx","username":"cloud.admin"},"forms":{"identity domain":{"name":"identity domain","displayName":"identity domain","type":"input","defaultValue":"gse00010212","hidden":false,"$$hashKey":"object:1346"},"password":{"name":"password","displayName":"password","type":"input","defaultValue":"yourpassword","hidden":false,"$$hashKey":"object:1347"},"username":{"name":"username","displayName":"username","type":"input","defaultValue":"cloud.admin","hidden":false,"$$hashKey":"object:1348"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Sanity check.  Here is the auth.url: https://gse00010212.storage.oraclecloud.com/auth/v2.0/tokens\nDone\nhconf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n"}]},"apps":[],"jobName":"paragraph_1501203597130_-1143160703","id":"20170314-144629_847210145","dateCreated":"2017-07-28T00:59:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:655"},{"text":"%md\n### Change Log\nJuly 28, 2017 - Updated to work with BDCSCE 17.3.1-20","user":"anonymous","dateUpdated":"2017-07-28T14:12:13+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>July 28, 2017 - Updated to work with BDCSCE 17.3.1-20</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501203597130_-1143160703","id":"20170424-170410_551484051","dateCreated":"2017-07-28T00:59:57+0000","dateStarted":"2017-07-28T14:12:13+0000","dateFinished":"2017-07-28T14:12:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:656"},{"text":"%md\n","user":"anonymous","dateUpdated":"2017-07-28T14:12:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501251133240_926355867","id":"20170728-141213_1235978622","dateCreated":"2017-07-28T14:12:13+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:657"}],"name":"Demonstration Presidential Speeches with Spark and Spark SQL","id":"2CNHJBZC1","angularObjects":{"2CQVFQFWU:shared_process":[],"2CRFREMNU:shared_process":[{"name":"BIND_container","object":"demo","noteId":"2CNHJBZC1"}],"2CPRZVQKR:shared_process":[],"2CQWU46XZ:shared_process":[],"2CPKCF4R9:shared_process":[],"2CR7Y2CFK:shared_process":[],"2CQQM64RT:shared_process":[],"2CQX3UW3S:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}