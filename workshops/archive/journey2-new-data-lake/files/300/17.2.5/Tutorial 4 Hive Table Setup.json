{"paragraphs":[{"text":"%md\n# Demonstration: Citi Bike New York - Table Setup for Hive\n\nThis tutorial was built for BDCS-CE version 17.2.5. If you are using a later version of BDCS-CE, please look for a newer version of this tutorial notebook at <https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake> . Questions and feedback about the tutorial: david.bayard@oracle.com\n\nBe sure to run the previous Tutorials: \"Citi Bike New York Introduction and Setup\" and \"Setting up your BDCSCE Environment\" \n\nThis tutorial will illustrate using the Shell interpreter to run a few hive command lines.\n\n## Contents\n+ Create a Hive Table with the shell interpreter \n+ Running simple queries (select, show databases, show tables)\n+ Query a simple Hive table and graph the results\n+ Next Steps\n\n### Things to do before you run the notebook paragraphs:\n1. You will need to have a Cluster using the Full profile for this example run hive.\n2. The zeppelin user must have \"sudo\" privledges (see the Setting up your BDCSCE Environment tutorial).\n3. The data file listed below must be in an Object Storage container called by default \"citibike\" (see Citi Bike New York Introduction and Setup tutorial).\n\nAs a reminder, the documentation for BDCS-CE can be found here: <http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html>","dateUpdated":"2017-07-19T12:28:05+0000","config":{"tableHide":false,"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Demonstration: Citi Bike New York - Table Setup for Hive</h1>\n<p>This tutorial was built for BDCS-CE version 17.2.5. If you are using a later version of BDCS-CE, please look for a newer version of this tutorial notebook at <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\">https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake</a> . Questions and feedback about the tutorial: david.bayard@oracle.com</p>\n<p>Be sure to run the previous Tutorials: &ldquo;Citi Bike New York Introduction and Setup&rdquo; and &ldquo;Setting up your BDCSCE Environment&rdquo;</p>\n<p>This tutorial will illustrate using the Shell interpreter to run a few hive command lines.</p>\n<h2>Contents</h2>\n<ul>\n<li>Create a Hive Table with the shell interpreter</li>\n<li>Running simple queries (select, show databases, show tables)</li>\n<li>Query a simple Hive table and graph the results</li>\n<li>Next Steps</li>\n</ul>\n<h3>Things to do before you run the notebook paragraphs:</h3>\n<ol>\n<li>You will need to have a Cluster using the Full profile for this example run hive.</li>\n<li>The zeppelin user must have &ldquo;sudo&rdquo; privledges (see the Setting up your BDCSCE Environment tutorial).</li>\n<li>The data file listed below must be in an Object Storage container called by default &ldquo;citibike&rdquo; (see Citi Bike New York Introduction and Setup tutorial).</li>\n</ol>\n<p>As a reminder, the documentation for BDCS-CE can be found here: <a href=\"http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\">http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html</a></p>\n"}]},"apps":[],"jobName":"paragraph_1500467285589_-740760628","id":"20160524-153718_1397925708","dateCreated":"2017-07-19T12:28:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:306"},{"title":"Create the Bike Table from the CSV file","text":"%sh \n\nCONTAINER=citibike\nFILENAME=201612-citibike-tripdata\n\necho \"Object Storage Container Name        :\" $CONTAINER\necho \"Data Set name (remove .zip or .csv)  :\" $FILENAME\necho \"-----------------------------------------------------------------\"\n\n# remove the staging hadoop file if present\nhadoop fs -rm -r /tmp/base.csv\n\n# Stage the base data file to a temporary location\n# we are using an internal hive table so the file will be moved into hive\nsudo -u hive hadoop fs -cp swift://$CONTAINER.default/${FILENAME}_nh.csv /tmp/base.csv\n\n# run hive\nsudo -u hive hive 2>&1 <<EOF\nDROP TABLE bike_trips;\n\nCREATE TABLE bike_trips ( \nTripDuration int,\nStartTime timestamp,\nStopTime timestamp,\nStartStationID string,\nStartStationName string,\nStartStationLatitude string,\nStartStationLongitude string,\nEndStationID string,\nEndStationName int,\nEndStationLatitude string,\nEndStationLongitude string,\nBikeID int,\nUserType string,\nBirthYear int, \nGender int\n) \nROW FORMAT delimited \nFIELDS TERMINATED BY ',' ;\n\nLOAD DATA INPATH '/tmp/base.csv' into table bike_trips;\n\nSELECT count(*) FROM bike_trips;\n\nexit;\n\nEOF\n","dateUpdated":"2017-07-19T12:28:28+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":431,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"DataFile":"base_bike_nh_long.csv","localFS":"/tmp/hive-demo","data":"base_bike_nh.csv","prefix":"/tmp/hive-demo","CONTAINER":"citibike","hadoopFS":"/user/zeppelin/hive-demo","FILENAME":"201612-citibike-tripdata"},"forms":{"FILENAME":{"name":"FILENAME","defaultValue":"","hidden":false,"$$hashKey":"object:808"}}},"apps":[],"jobName":"paragraph_1500467285589_-740760628","id":"20170413-132012_1207762175","dateCreated":"2017-07-19T12:28:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:307"},{"title":"Showing the Hive tables defined","text":"%sh\n\nsudo -u hive hive 2>&1 <<EOF\nshow tables;\nEOF\n","dateUpdated":"2017-07-19T12:28:47+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"gender","index":0,"aggr":"sum"}],"values":[{"name":"a.trip_count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"gender","index":0,"aggr":"sum"},"yAxis":{"name":"a.trip_count","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1500467285590_-739606381","id":"20170511-135148_457890564","dateCreated":"2017-07-19T12:28:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:308"},{"title":"Sample query via Hive Command Line","text":"%sh\n\nsudo -u hive hive 2>&1 <<EOF\nselect \n case when a.gender=1 then 'Male' when a.gender=2 then 'Female' else 'unknown' end gender ,\n        a.trip_count \nfrom (select gender, count(*) trip_count from bike_trips\ngroup by gender) a;\nEOF","dateUpdated":"2017-07-19T12:29:23+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1500467285590_-739606381","id":"20170425-120754_372055516","dateCreated":"2017-07-19T12:28:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:309"},{"text":"%md\n#Next Steps\n\nSo far, we\n\n1) have downloaded a Citi Bike data zip, manipulated some csv files, and stored the files into the Object Storage. \n2) set up a Hive table\n\nIn the next Tutorial, we use Zeppelin's Hive interpretor to run some more queries.\n","dateUpdated":"2017-07-19T12:29:17+0000","config":{"colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Next Steps</h1>\n<p>So far, we</p>\n<p>1) have downloaded a Citi Bike data zip, manipulated some csv files, and stored the files into the Object Storage.\n<br  />2) set up a Hive table</p>\n<p>In the next Tutorial, we use Zeppelin's Hive interpretor to run some more queries.</p>\n"}]},"apps":[],"jobName":"paragraph_1500467285591_-739991130","id":"20170512-090716_2012584955","dateCreated":"2017-07-19T12:28:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:310"},{"text":"%md\n###Tip: Running Hive and Zeppelin/Spark on a 2 OCPUs instance\n\nSome of you may be running this tutorial on the smallest configuration size (2 OCPUs).  If you are using this small shape, there can be scenarios where your Hive commands get \"Accepted\" but do not progress to \"Processing\".  You can see this behavior in the Jobs tab.  If this happens, then your Hive commands will hang indefinitely.  The reason this happens is that with 2 OCPUs, all of the available processing threads in the YARN resource manager can be occupied by other jobs.  There is a simple solution if you see this happen:  simply go to the Jobs tab, find the \"Zeppelin\" job, and abort it.  You will find that your Hive job will then move from \"Accepted\" to \"Processing\" and then finish.  The \"Zeppelin\" job will relaunch itself automatically the next time you run something with the Spark Interpreter inside Zeppelin.\n\nAn alternate solution is to add another node to your BDCS-CE cluster.  BDCS-CE is scalable, so you can add a node which will provide additional processing threads for YARN to work with.  You can also drop the node later if you want.","dateUpdated":"2017-07-19T12:28:05+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Tip: Running Hive and Zeppelin/Spark on a 2 OCPUs instance</h3>\n<p>Some of you may be running this tutorial on the smallest configuration size (2 OCPUs).  If you are using this small shape, there can be scenarios where your Hive commands get &ldquo;Accepted&rdquo; but do not progress to &ldquo;Processing&rdquo;.  You can see this behavior in the Jobs tab.  If this happens, then your Hive commands will hang indefinitely.  The reason this happens is that with 2 OCPUs, all of the available processing threads in the YARN resource manager can be occupied by other jobs.  There is a simple solution if you see this happen:  simply go to the Jobs tab, find the &ldquo;Zeppelin&rdquo; job, and abort it.  You will find that your Hive job will then move from &ldquo;Accepted&rdquo; to &ldquo;Processing&rdquo; and then finish.  The &ldquo;Zeppelin&rdquo; job will relaunch itself automatically the next time you run something with the Spark Interpreter inside Zeppelin.</p>\n<p>An alternate solution is to add another node to your BDCS-CE cluster.  BDCS-CE is scalable, so you can add a node which will provide additional processing threads for YARN to work with.  You can also drop the node later if you want.</p>\n"}]},"apps":[],"jobName":"paragraph_1500467285591_-739991130","id":"20170512-135634_1459754506","dateCreated":"2017-07-19T12:28:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:311"},{"dateUpdated":"2017-07-19T12:28:05+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1500467285591_-739991130","id":"20170616-134351_1883942168","dateCreated":"2017-07-19T12:28:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:312"}],"name":" Tutorial 4 Hive Table Setup","id":"2CQFWX38Q","angularObjects":{"2CNW8HM6F:shared_process":[],"2CQA8RVBH:shared_process":[],"2CPBC1JBJ:shared_process":[],"2CP9D5M3M:shared_process":[],"2CMFH5UVC:shared_process":[],"2CPGG2FKV:shared_process":[],"2CPA8A394:shared_process":[],"2CQ2JJV7J:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}