{"paragraphs":[{"text":"%md\n#Demonstration: Analyzing Presidential Speeches with Spark and Spark SQL\n\nThis tutorial was built for BDCS-CE version 17.2.5.  If you are using a later version of BDCS-CE, please look for a newer version of this tutorial notebook at <https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake> . Questions and feedback about the tutorial: david.bayard@oracle.com\n\n**Contents**\n+ About Spark and Spark SQL\n+ Example 1: Spark(Scala) with HDFS data\n+ Example 2: Spark(Python) with HDFS data\n+ Example 3: Spark(Scala) with linux file system data\n+ Example 4: Spark(Scala) with Object Store data\n+ Example 5: Wordcount(Spark Scala) [choose HDFS, linux, or Object Store data]\n+ Example 6: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)\n+ Example 7: Spark SQL against our Wordcounts table\n+ Example 8: More complex example- Speech Trends across time\n+ Example 9: Saving results to the Object Store\n+ Optional: Explore the Spark UI\n+ Next Steps\n\n\nAs a reminder, the documentation for BDCS-CE can be found here: <http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html>\n\n**NOTE: Please ensure that you have run the \"Working with the Object Store\" tutorial first**\n","authenticationInfo":{},"dateUpdated":"Jun 16, 2017 10:46:58 AM","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810923_-1857352442","id":"20170310-125455_1477278610","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Demonstration: Analyzing Presidential Speeches with Spark and Spark SQL</h1>\n<p>This tutorial was built for BDCS-CE version 17.2.5.  If you are using a later version of BDCS-CE, please look for a newer version of this tutorial notebook at <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\">https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake</a> . Questions and feedback about the tutorial: david.bayard@oracle.com</p>\n<p><strong>Contents</strong></p>\n<ul>\n<li>About Spark and Spark SQL</li>\n<li>Example 1: Spark(Scala) with HDFS data</li>\n<li>Example 2: Spark(Python) with HDFS data</li>\n<li>Example 3: Spark(Scala) with linux file system data</li>\n<li>Example 4: Spark(Scala) with Object Store data</li>\n<li>Example 5: Wordcount(Spark Scala) [choose HDFS, linux, or Object Store data]</li>\n<li>Example 6: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)</li>\n<li>Example 7: Spark SQL against our Wordcounts table</li>\n<li>Example 8: More complex example- Speech Trends across time</li>\n<li>Example 9: Saving results to the Object Store</li>\n<li>Optional: Explore the Spark UI</li>\n<li>Next Steps</li>\n</ul>\n<p>As a reminder, the documentation for BDCS-CE can be found here: <a href=\"http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\">http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html</a></p>\n<p><strong>NOTE: Please ensure that you have run the &ldquo;Working with the Object Store&rdquo; tutorial first</strong></p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","dateStarted":"Jun 16, 2017 10:46:54 AM","dateFinished":"Jun 16, 2017 10:46:54 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"%md\n#About Spark and Spark SQL\n\nBDCS-CE version 17.2.1 comes with Spark version 1.6.1, Scala version 2.10, and Python version 2.6.6.  This BDCS-CE version supplies Zeppelin interpreters for Spark(Scala), Spark(Python), and Spark SQL.  This tutorial will give you examples using all of these.\n\nThe tutorial assumes you have a basic knowledge about Spark.  To learn more about Spark, check out <https://spark.apache.org/docs/1.6.1/quick-start.html> and <https://spark.apache.org/docs/1.6.1/sql-programming-guide.html>\n\n\n","dateUpdated":"Jun 16, 2017 10:46:04 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810923_-1857352442","id":"20170403-145735_495154166","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>About Spark and Spark SQL</h1>\n<p>BDCS-CE version 17.2.1 comes with Spark version 1.6.1, Scala version 2.10, and Python version 2.6.6.  This BDCS-CE version supplies Zeppelin interpreters for Spark(Scala), Spark(Python), and Spark SQL.  This tutorial will give you examples using all of these.</p>\n<p>The tutorial assumes you have a basic knowledge about Spark.  To learn more about Spark, check out <a href=\"https://spark.apache.org/docs/1.6.1/quick-start.html\">https://spark.apache.org/docs/1.6.1/quick-start.html</a> and <a href=\"https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\">https://spark.apache.org/docs/1.6.1/sql-programming-guide.html</a></p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:200"},{"text":"%md\n#Example 1: Spark(Scala) with HDFS data\n\nOur first example will show a simple bit of Scala code accessing HDFS data.  Our example defines a Spark RDD (Resilient Distributed Dataset) against a text file stored in HDFS.  Then it runs a few actions against the RDD, such as counting the # of lines, displaying the first line, and counting the number of lines matching a given term. \n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810924_-1859276186","id":"20170403-152657_222008853","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 1: Spark(Scala) with HDFS data</h1>\n<p>Our first example will show a simple bit of Scala code accessing HDFS data.  Our example defines a Spark RDD (Resilient Distributed Dataset) against a text file stored in HDFS.  Then it runs a few actions against the RDD, such as counting the # of lines, displaying the first line, and counting the number of lines matching a given term.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:201"},{"title":"Scala example with HDFS data","text":"%spark\n// to run Scala Spark code in zeppelin, use the %spark interpreter\n\n\n// Scala example\n\nprintln(\"Define a RDD against a HDFS textfile...\")\nval textFile = sc.textFile(\"speeches_hdfs/pres1861_lincoln.txt\")\n// Spark defaults to HDFS when specifying filenames.  This file was copied to HDFS when you ran the previous tutorial.\nprintln(\"..\")\n\n\nprintln(\"Count of # of rows...\")\ntextFile.count() // Number of lines in this RDD\nprintln(\"..\")\n\n\nprintln(\"The first line: %s\".format(\n    textFile.first() // First line in this RDD\n))\nprintln(\"..\")\n\nprintln(\"# of Lines with Constitution: %s\".format(\n  textFile.filter(line => line.contains(\"Constitution\")).count() \n)) // How many lines contain \"Constitution\"?\nprintln(\"..\")\n\nprintln(\"done\")","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810924_-1859276186","id":"20170310-125552_378911250","result":{"code":"SUCCESS","type":"TEXT","msg":"Define a RDD against a HDFS textfile...\ntextFile: org.apache.spark.rdd.RDD[String] = speeches_hdfs/pres1861_lincoln.txt MapPartitionsRDD[1] at textFile at <console>:29\n..\nCount of # of rows...\nres4: Long = 364\n..\nThe first line:    First Inaugural Address of Abraham Lincoln\n..\n# of Lines with Constitution: 24\n..\ndone\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"text":"%md\n#Example 2: Spark(Python) with HDFS data\n\nOur second example will show a simple bit of Python code accessing HDFS data.  It does the same logic as the first example, just using Python instead of Scala. \n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810924_-1859276186","id":"20170403-154326_852166402","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 2: Spark(Python) with HDFS data</h1>\n<p>Our second example will show a simple bit of Python code accessing HDFS data.  It does the same logic as the first example, just using Python instead of Scala.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:203"},{"title":"Python example with HDFS data","text":"%pyspark\n# to run Python Spark code in zeppelin, use the %pyspark interpreter\n\n\n#python example\n\n\ntextFile = sc.textFile(\"speeches_hdfs/pres1861_lincoln.txt\")\n\nnumCount=textFile.count() # Number of lines in this RDD\nprint(\"Count of # of lines is %i\" % numCount)\n\nfirst = textFile.first() # First line in this RDD\nprint(\"The First Line is:\" + first)\n\n\nnumConst = textFile.filter(lambda line: \"Constitution\" in line).count() # How many lines contain \"Constitution\"?\nprint(\"# of Lines with Constitution is %i\" % numConst)","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/python","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810924_-1859276186","id":"20170314-130540_420933051","result":{"code":"SUCCESS","type":"TEXT","msg":"Count of # of lines is 364\nThe First Line is:   First Inaugural Address of Abraham Lincoln\n# of Lines with Constitution is 24\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:204"},{"text":"%md\n#Example 3: Spark(Scala) with linux file system data\n\nOur third example is a slight variation of the first example.  The only difference (the 3rd line of the example) is that it uses data on the zeppelin's server linux file system, not HDFS.\n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810924_-1859276186","id":"20170403-154637_1284130064","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 3: Spark(Scala) with linux file system data</h1>\n<p>Our third example is a slight variation of the first example.  The only difference (the 3rd line of the example) is that it uses data on the zeppelin's server linux file system, not HDFS.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:205"},{"title":"Example with local linux file system data (Scala)","text":"%spark\n// Scala example\n\nprintln(\"Define a RDD against a local filesystem textfile...\")\n\nval textFile = sc.textFile(\"file:/var/lib/zeppelin/speeches/pres1933_fdr1.txt\")\n// Observe that we specified the file: namespace to point to a file in the local linux file system (this file was copied there when you ran the previous tutorial)\n\n\nprintln(\"..\")\n\n\nprintln(\"Count of # of rows...\")\ntextFile.count() // Number of items in this RDD\nprintln(\"..\")\n\n\nprintln(\"The first line: %s\".format(\n    textFile.first() // First item in this RDD\n))\nprintln(\"..\")\n\nprintln(\"# of Lines with Constitution: %s\".format(\n  textFile.filter(line => line.contains(\"Constitution\")).count() \n)) // How many lines contain \"Constitution\"?\nprintln(\"..\")\n\nprintln(\"done\")","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810924_-1859276186","id":"20170310-125626_48474354","result":{"code":"SUCCESS","type":"TEXT","msg":"Define a RDD against a local filesystem textfile...\ntextFile: org.apache.spark.rdd.RDD[String] = file:/var/lib/zeppelin/speeches/pres1933_fdr1.txt MapPartitionsRDD[141] at textFile at <console>:42\n..\nCount of # of rows...\nres53: Long = 198\n..\nThe first line:    First Inaugural Address of Franklin D. Roosevelt\n..\n# of Lines with Constitution: 1\n..\ndone\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:206"},{"text":"%md\n#Example 4: Spark(Scala) with Object Store data\n\nOur fourth example is another variation of the first example.  The difference is that it works with data stored on the Object Store, not in HDFS.\n\nThis example will use the \"default\" Object Store connection, which is the Object Store credentials you specified when you created this BDCS-CE instance.  See the end of this tutorial for an example of how to define additional Object Store connections.\n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810924_-1859276186","id":"20170403-155009_862645126","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 4: Spark(Scala) with Object Store data</h1>\n<p>Our fourth example is another variation of the first example.  The difference is that it works with data stored on the Object Store, not in HDFS.</p>\n<p>This example will use the &ldquo;default&rdquo; Object Store connection, which is the Object Store credentials you specified when you created this BDCS-CE instance.  See the end of this tutorial for an example of how to define additional Object Store connections.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:207"},{"title":"Example with Object Store data (Scala)","text":"%spark\n// This section uses some Zepellin APIs to create some user entry fields\n// Learn more here: https://zeppelin.apache.org/docs/0.6.1/interpreter/spark.html#form-creation\n/* Create text input form with default value */\nz.angularBind(\"BIND_container\", z.input(\"container\", \"demo\"))\n\nprintln(\"Define a RDD against a Object Store textfile...\")\nval textFile = sc.textFile(\"swift://\"+z.angular(\"BIND_container\")+\".default/speeches/pres1981_reagan1.txt\")\n// Observe that we are specifying our file by the swift://ContainerName.ServiceName/ namespace.  The ServiceName (default in this example) points to set of configuration parameters.  The default configuration parameters were defined when you created this BDCS-CE instance. \n\n\nprintln(\"..\")\n\nprintln(\"Count of # of lines: %s\".format(\n    textFile.count() \n))    // Number of items in this RDD\nprintln(\"..\")\n\nprintln(\"The First Line: %s\".format(\n    textFile.first() // First item in this RDD\n))\nprintln(\"..\")\n\nprintln(\"# Lines with Constitution: %s\".format(\n  textFile.filter(line => line.contains(\"Constitution\")).count() \n)) // How many lines contain \"Constitution\"?\nprintln(\"..\")\n\nprintln(\"done\")","dateUpdated":"Jun 16, 2017 2:19:14 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"container":"demo"},"forms":{"container":{"name":"container","displayName":"container","type":"input","defaultValue":"demo","hidden":false}}},"jobName":"paragraph_1497623810924_-1859276186","id":"20170314-144912_872094449","result":{"code":"SUCCESS","type":"TEXT","msg":"Define a RDD against a Object Store textfile...\n..\nCount of # of lines: 256\n..\nThe First Line:    First Inaugural Address of Ronald Reagan\n..\n# Lines with Constitution: 1\n..\ndone\ntextFile: org.apache.spark.rdd.RDD[String] = swift://demo.default/speeches/pres1981_reagan1.txt MapPartitionsRDD[216] at textFile at <console>:37\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:208","authenticationInfo":{},"dateFinished":"Jun 16, 2017 2:19:15 PM","dateStarted":"Jun 16, 2017 2:19:14 PM","focus":true},{"text":"%md\n#Example 5: Wordcount(Spark Scala) [choose HDFS, linux, or Object Store data]\n\nOur fifth example runs the classic Wordcount algorithm.  You can choose if you want to operate on HDFS, Object Store, or linux file system data by commenting out the appropriate code.  \n\nThe results of the Wordcount are an RDD named wordCounts that will also be used in the following example.\n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810925_-1859660935","id":"20170403-162628_622133907","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 5: Wordcount(Spark Scala) [choose HDFS, linux, or Object Store data]</h1>\n<p>Our fifth example runs the classic Wordcount algorithm.  You can choose if you want to operate on HDFS, Object Store, or linux file system data by commenting out the appropriate code.</p>\n<p>The results of the Wordcount are an RDD named wordCounts that will also be used in the following example.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:209"},{"title":"Wordcount Example (scala)","text":"%spark\n\n// pick your choice of file: HDFS, Local, or Object Store.  Uncomment one of the below.\n// val textFile = sc.textFile(\"speeches_hdfs/pres1861_lincoln1.txt\")\n// val textFile = sc.textFile(\"file:/var/lib/zeppelin/speeches/pres1861_lincoln1.txt\")\nval textFile = sc.textFile(\"swift://\"+z.angular(\"BIND_container\")+\".default/speeches/pres1861_lincoln1.txt\")\n\n// Now let's do the classic wordcount example with our data\n// if you are curious, the replaceAll() strips out punctuation and the split() separates the string into words.  Both use regex expressions.\nval wordCounts = textFile.flatMap(line => line.replaceAll(\"[^\\\\w ]\",\" \").toLowerCase().split(\" +\")).map(word => (word, 1)).reduceByKey((a, b) => a + b)\n\nprintln(\"..\")\n\nprintln(\"Here is the output (partially shown)\")\nwordCounts.collect()\n\nprintln(\"\")\nprintln(\"..\")\n\nprintln(\"The first word is %s and the count of it is %s\".format(wordCounts.first()_1,wordCounts.first()_2))\nprintln(\"..\")\n\nprintln(\"done\")","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810925_-1859660935","id":"20170310-125659_767950724","result":{"code":"SUCCESS","type":"TEXT","msg":"textFile: org.apache.spark.rdd.RDD[String] = swift://dcb-bdcs-apr12.default/speeches/pres1861_lincoln1.txt MapPartitionsRDD[1] at textFile at <console>:34\nwordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:33\n..\nHere is the output (partially shown)\nres3: Array[(String, Int)] = Array((sovereign,1), (ills,2), (call,1), (country,3), (weary,1), (admitted,1), (humane,1), (greater,1), (intervals,1), (accession,1), (privileges,1), (contrary,2), (inadmissible,1), (maintained,1), (order,1), (recanted,1), (apprehension,2), (modification,1), (national,9), (been,4), (pretext,2), (resident,1), (advantageous,1), (evil,1), (over,3), (executive,2), (parties,3), (any,27), (make,6), (conflict,1), (instead,1), (consideration,1), (overthrow,1), (contract,2), (institution,1), (assumed,1), (me,6), (divorced,1), (parts,2), (are,20), (sentiments,3), (fugitives,2), (existed,1), (brief,1), (element,1), (scarcely,1), (unmade,1), (hazard,1), (audacity,1), (faithfully,2), (favor,1), (our,13), (bloodshed,1), (territories,2), (circumstances,3), (branch,1), (for...\n..\nwarning: there were 2 feature warning(s); re-run with -feature for details\nThe first word is sovereign and the count of it is 1\n..\ndone\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:210"},{"text":"%md\n#Example 6: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)\n\nOur sixth example continues from the previous example. Specifically, it takes the wordCounts RDD and registers it as Spark DataFrame.  Then it registers the new data frame as a temporary Spark SQL table.\n\nAt this point, you might want to quickly review the Spark SQL programming guide for a refresher about Creating DataFrames from RDDs: <https://spark.apache.org/docs/1.6.1/sql-programming-guide.html>\n\n","dateUpdated":"Jun 16, 2017 2:19:31 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810925_-1859660935","id":"20170403-163207_132564105","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 6: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)</h1>\n<p>Our sixth example continues from the previous example. Specifically, it takes the wordCounts RDD and registers it as Spark DataFrame.  Then it registers the new data frame as a temporary Spark SQL table.</p>\n<p>At this point, you might want to quickly review the Spark SQL programming guide for a refresher about Creating DataFrames from RDDs: <a href=\"https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\">https://spark.apache.org/docs/1.6.1/sql-programming-guide.html</a></p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:211"},{"title":"Converting a RDD to a DataFrame and Registering it as Table (scala)","text":"%spark\n\n// this example follows the \"Inferring the Schema using Reflection\" of converting an RDD to a DataFrame. See https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\n\n// first define a case class that describes our Schema \ncase class Wordcount(word: String, wcount: Int)\nprintln(\"..\")\n\n// second, map our wordCounts RDD into an RDD using our Wordcount class and convert that into a DataFrame\nval wordcountsDF = wordCounts.map(p => Wordcount(p._1, p._2)).toDF()\nprintln(\"..\")\n\n// now register our wordcountsDF data frame as a temporary Spark SQL table\nwordcountsDF.registerTempTable(\"wordcounts\")\nprintln(\"..\")\n\nprintln(\"done\")","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810925_-1859660935","id":"20170314-135312_277685101","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class Wordcount\n..\nwordcountsDF: org.apache.spark.sql.DataFrame = [word: string, wcount: int]\n..\n..\ndone\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:212"},{"text":"%md\n#Example 7: Spark SQL against our wordcounts table\n\nOur seventh example continues from the previous example. Specifically, it provides two examples of running Spark SQL against our wordcounts table.  It also demonstrates some of the features of Zeppelin's Spark SQL interpreter and display visualization capabilities.\n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810926_-1858506688","id":"20170403-164432_219780017","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 7: Spark SQL against our wordcounts table</h1>\n<p>Our seventh example continues from the previous example. Specifically, it provides two examples of running Spark SQL against our wordcounts table.  It also demonstrates some of the features of Zeppelin's Spark SQL interpreter and display visualization capabilities.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:213"},{"title":"Spark SQL against our WordCounts table","text":"%sql\nselect * from wordcounts \nwhere length(word)>4\norder by wcount desc limit 15","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/sql","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"wcount","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"wcount","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810926_-1858506688","id":"20170314-140802_470559038","result":{"code":"SUCCESS","type":"TABLE","msg":"word\twcount\nconstitution\t24\nunion\t20\npeople\t20\nstates\t19\nwhich\t18\nthere\t18\ngovernment\t18\nshall\t16\ntheir\t14\nwhile\t13\nright\t11\nwould\t10\nother\t10\nnational\t9\nstate\t8\n","comment":"","msgTable":[[{"key":"wcount","value":"constitution"},{"key":"wcount","value":"24"}],[{"value":"union"},{"value":"20"}],[{"value":"people"},{"value":"20"}],[{"value":"states"},{"value":"19"}],[{"value":"which"},{"value":"18"}],[{"value":"there"},{"value":"18"}],[{"value":"government"},{"value":"18"}],[{"value":"shall"},{"value":"16"}],[{"value":"their"},{"value":"14"}],[{"value":"while"},{"value":"13"}],[{"value":"right"},{"value":"11"}],[{"value":"would"},{"value":"10"}],[{"value":"other"},{"value":"10"}],[{"value":"national"},{"value":"9"}],[{"value":"state"},{"value":"8"}]],"columnNames":[{"name":"word","index":0,"aggr":"sum"},{"name":"wcount","index":1,"aggr":"sum"}],"rows":[["constitution","24"],["union","20"],["people","20"],["states","19"],["which","18"],["there","18"],["government","18"],["shall","16"],["their","14"],["while","13"],["right","11"],["would","10"],["other","10"],["national","9"],["state","8"]]},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:214"},{"title":"More Spark SQL","text":"%sql\nselect * from wordcounts\nwhere word in ('god','america','constitution','nation','people','states','rights','freedom','union')\norder by wcount desc","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/sql","title":true,"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"wcount","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"wcount","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810926_-1858506688","id":"20170314-140955_1723818486","result":{"code":"SUCCESS","type":"TABLE","msg":"word\twcount\nconstitution\t24\npeople\t20\nunion\t20\nstates\t19\nrights\t3\n","comment":"","msgTable":[[{"key":"wcount","value":"constitution"},{"key":"wcount","value":"24"}],[{"value":"people"},{"value":"20"}],[{"value":"union"},{"value":"20"}],[{"value":"states"},{"value":"19"}],[{"value":"rights"},{"value":"3"}]],"columnNames":[{"name":"word","index":0,"aggr":"sum"},{"name":"wcount","index":1,"aggr":"sum"}],"rows":[["constitution","24"],["people","20"],["union","20"],["states","19"],["rights","3"]]},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:215"},{"text":"%md\n#Example 8: More complex example- Speech Trends across time\n\nOur eighth example continues from the previous example. Specifically, it builds an RDD against all of the speeches we stored in the Object Store.  Then it performs a word count against the RDD and converts the result into a Data Frame that we register as a Spark SQL temporary table.  Then we use some SparkSQL to prepare a couple of charts.\n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810926_-1858506688","id":"20170403-165335_535102146","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 8: More complex example- Speech Trends across time</h1>\n<p>Our eighth example continues from the previous example. Specifically, it builds an RDD against all of the speeches we stored in the Object Store.  Then it performs a word count against the RDD and converts the result into a Data Frame that we register as a Spark SQL temporary table.  Then we use some SparkSQL to prepare a couple of charts.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216"},{"text":"%spark\n// more complex example.  Let's look for trends across presidents\n\n\nval filebase = \"swift://\"+z.angular(\"BIND_container\")+\".default/speeches/pres\"\n\nval textFiles = sc.wholeTextFiles(filebase+\"*.txt\")\n\nprintln(\"We found %s speeches\".format(textFiles.count()))\nprintln(\"..\")\n\n// this cleans up the filename to make it shorter and removes some punctuation from the text\nval words = textFiles.map(file => (file._1.replace(filebase,\"\"),file._2.replaceAll(\"[^\\\\w ]\",\" \").toLowerCase()))\nprintln(\"..\")\n\n// this splits the String of text into an array of words/Strings\nval wordArray = words.map(file => (file._1,file._2.split(\" +\")))\nprintln(\"..\")\n\n// define our class/schema\ncase class filewordsraw(file: String, words: Array[String])\nprintln(\"..\")\n\n// create a DataFrame from our RDD\nval filewordsrawDF = wordArray.map(p => filewordsraw(p._1, p._2)).toDF()\nprintln(\"..\")\n\n// show the schema\nfilewordsrawDF.printSchema()\nprintln(\"..\")\n\n// register this as a table (we'll do the rest of the word count in SQL)\nfilewordsrawDF.registerTempTable(\"filewordsraw\")\nprintln(\"..\")\n\nprintln(\"done\")\n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810926_-1858506688","id":"20170314-161337_919830878","result":{"code":"SUCCESS","type":"TEXT","msg":"filebase: String = swift://dcb-bdcs-apr12.default/speeches/pres\ntextFiles: org.apache.spark.rdd.RDD[(String, String)] = swift://dcb-bdcs-apr12.default/speeches/pres*.txt MapPartitionsRDD[17598] at wholeTextFiles at <console>:190\nWe found 5 speeches\n..\nwords: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[17599] at map at <console>:192\n..\nwordArray: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[17600] at map at <console>:194\n..\ndefined class filewordsraw\n..\nfilewordsrawDF: org.apache.spark.sql.DataFrame = [file: string, words: array<string>]\n..\nroot\n |-- file: string (nullable = true)\n |-- words: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n..\n..\ndone\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:217"},{"title":"SQL Showing the # of words per Speech","text":"%sql\nselect file, count(*) words from (select file, explode(words) word from filewordsraw) a\ngroup by file\norder by file","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/sql","title":true,"graph":{"mode":"lineChart","height":300,"optionOpen":false,"keys":[{"name":"file","index":0,"aggr":"sum"}],"values":[{"name":"words","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"file","index":0,"aggr":"sum"}},"forceY":true},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810926_-1858506688","id":"20170403-165913_699538396","result":{"code":"SUCCESS","type":"TABLE","msg":"file\twords\n1789_wash1.txt\t1447\n1861_lincoln1.txt\t3648\n1933_fdr1.txt\t1895\n1961_jfk.txt\t1379\n1981_reagan1.txt\t2463\n","comment":"","msgTable":[[{"key":"words","value":"1789_wash1.txt"},{"key":"words","value":"1447"}],[{"value":"1861_lincoln1.txt"},{"value":"3648"}],[{"value":"1933_fdr1.txt"},{"value":"1895"}],[{"value":"1961_jfk.txt"},{"value":"1379"}],[{"value":"1981_reagan1.txt"},{"value":"2463"}]],"columnNames":[{"name":"file","index":0,"aggr":"sum"},{"name":"words","index":1,"aggr":"sum"}],"rows":[["1789_wash1.txt","1447"],["1861_lincoln1.txt","3648"],["1933_fdr1.txt","1895"],["1961_jfk.txt","1379"],["1981_reagan1.txt","2463"]]},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:218"},{"title":"Chart showing the usage of certain words over time","text":"%sql\nselect file, word, count(*) wc from (select file, explode(words) word from filewordsraw) a\nwhere word in ('god','america','constitution','nation','rights','freedom','people')\ngroup by file, word\norder by file","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/sql","title":true,"graph":{"mode":"lineChart","height":300,"optionOpen":false,"keys":[{"name":"file","index":0,"aggr":"sum"}],"values":[{"name":"wc","index":2,"aggr":"sum"}],"groups":[{"name":"word","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"file","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810926_-1858506688","id":"20170314-173832_82214833","result":{"code":"SUCCESS","type":"TABLE","msg":"file\tword\twc\n1789_wash1.txt\tnation\t2\n1789_wash1.txt\tconstitution\t1\n1789_wash1.txt\tpeople\t4\n1789_wash1.txt\trights\t1\n1861_lincoln1.txt\tpeople\t20\n1861_lincoln1.txt\tconstitution\t24\n1861_lincoln1.txt\trights\t3\n1933_fdr1.txt\tgod\t2\n1933_fdr1.txt\tpeople\t8\n1933_fdr1.txt\trights\t1\n1933_fdr1.txt\tnation\t6\n1933_fdr1.txt\tconstitution\t1\n1961_jfk.txt\tpeople\t1\n1961_jfk.txt\tfreedom\t4\n1961_jfk.txt\tamerica\t2\n1961_jfk.txt\tnation\t2\n1961_jfk.txt\trights\t2\n1961_jfk.txt\tgod\t3\n1981_reagan1.txt\tamerica\t6\n1981_reagan1.txt\tfreedom\t8\n1981_reagan1.txt\tgod\t5\n1981_reagan1.txt\tconstitution\t1\n1981_reagan1.txt\tnation\t6\n1981_reagan1.txt\tpeople\t9\n","comment":"","msgTable":[[{"key":"word","value":"1789_wash1.txt"},{"key":"word","value":"nation"},{"key":"word","value":"2"}],[{"key":"wc","value":"1789_wash1.txt"},{"key":"wc","value":"constitution"},{"key":"wc","value":"1"}],[{"value":"1789_wash1.txt"},{"value":"people"},{"value":"4"}],[{"value":"1789_wash1.txt"},{"value":"rights"},{"value":"1"}],[{"value":"1861_lincoln1.txt"},{"value":"people"},{"value":"20"}],[{"value":"1861_lincoln1.txt"},{"value":"constitution"},{"value":"24"}],[{"value":"1861_lincoln1.txt"},{"value":"rights"},{"value":"3"}],[{"value":"1933_fdr1.txt"},{"value":"god"},{"value":"2"}],[{"value":"1933_fdr1.txt"},{"value":"people"},{"value":"8"}],[{"value":"1933_fdr1.txt"},{"value":"rights"},{"value":"1"}],[{"value":"1933_fdr1.txt"},{"value":"nation"},{"value":"6"}],[{"value":"1933_fdr1.txt"},{"value":"constitution"},{"value":"1"}],[{"value":"1961_jfk.txt"},{"value":"people"},{"value":"1"}],[{"value":"1961_jfk.txt"},{"value":"freedom"},{"value":"4"}],[{"value":"1961_jfk.txt"},{"value":"america"},{"value":"2"}],[{"value":"1961_jfk.txt"},{"value":"nation"},{"value":"2"}],[{"value":"1961_jfk.txt"},{"value":"rights"},{"value":"2"}],[{"value":"1961_jfk.txt"},{"value":"god"},{"value":"3"}],[{"value":"1981_reagan1.txt"},{"value":"america"},{"value":"6"}],[{"value":"1981_reagan1.txt"},{"value":"freedom"},{"value":"8"}],[{"value":"1981_reagan1.txt"},{"value":"god"},{"value":"5"}],[{"value":"1981_reagan1.txt"},{"value":"constitution"},{"value":"1"}],[{"value":"1981_reagan1.txt"},{"value":"nation"},{"value":"6"}],[{"value":"1981_reagan1.txt"},{"value":"people"},{"value":"9"}]],"columnNames":[{"name":"file","index":0,"aggr":"sum"},{"name":"word","index":1,"aggr":"sum"},{"name":"wc","index":2,"aggr":"sum"}],"rows":[["1789_wash1.txt","nation","2"],["1789_wash1.txt","constitution","1"],["1789_wash1.txt","people","4"],["1789_wash1.txt","rights","1"],["1861_lincoln1.txt","people","20"],["1861_lincoln1.txt","constitution","24"],["1861_lincoln1.txt","rights","3"],["1933_fdr1.txt","god","2"],["1933_fdr1.txt","people","8"],["1933_fdr1.txt","rights","1"],["1933_fdr1.txt","nation","6"],["1933_fdr1.txt","constitution","1"],["1961_jfk.txt","people","1"],["1961_jfk.txt","freedom","4"],["1961_jfk.txt","america","2"],["1961_jfk.txt","nation","2"],["1961_jfk.txt","rights","2"],["1961_jfk.txt","god","3"],["1981_reagan1.txt","america","6"],["1981_reagan1.txt","freedom","8"],["1981_reagan1.txt","god","5"],["1981_reagan1.txt","constitution","1"],["1981_reagan1.txt","nation","6"],["1981_reagan1.txt","people","9"]]},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:219"},{"text":"%md\n#Example 9: Saving a Data Frame back to the Object Store\n\nOur final example continues from the previous example. Specifically, it defines a new data frame based off one of the sample SQL statements and writes that data frame back to the Object Store (as a json file).  Then it reads the json data back from the Object Store into a new Data Frame.\n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810927_-1858891437","id":"20170403-200206_1232128960","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example 9: Saving a Data Frame back to the Object Store</h1>\n<p>Our final example continues from the previous example. Specifically, it defines a new data frame based off one of the sample SQL statements and writes that data frame back to the Object Store (as a json file).  Then it reads the json data back from the Object Store into a new Data Frame.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:220"},{"text":"%spark\n\n// \nval results = sqlContext.sql(\"select file, count(*) words from (select file, explode(words) word from filewordsraw) a group by file\")\nprintln(\"..\")\n\nval filebase = \"swift://\"+z.angular(\"BIND_container\")+\".default/speeches/pres_wordcount.json\"\nprintln(\"..\")\n\n// save in json format.  the repartition(1) ensures that we write a single output file, which makes sense since we know the output is small\nresults.repartition(1).write.format(\"json\").mode(\"overwrite\").save(filebase)\nprintln(\"..\")\n\n// now lets read it back\nval df = sqlContext.read.format(\"json\").load(filebase)\nprintln(\"..\")\n\ndf.collect()\nprintln(\"..\")\n\nprintln(\"done\")\n\n","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810927_-1858891437","id":"20170403-200515_1585180633","result":{"code":"SUCCESS","type":"TEXT","msg":"results: org.apache.spark.sql.DataFrame = [file: string, words: bigint]\n..\nfilebase: String = swift://dcb-bdcs-apr12.default/speeches/pres_wordcount.json\n..\n..\ndf: org.apache.spark.sql.DataFrame = [file: string, words: bigint]\n..\nres37: Array[org.apache.spark.sql.Row] = Array([1981_reagan1.txt,2463], [1961_jfk.txt,1379], [1933_fdr1.txt,1895], [1789_wash1.txt,1447], [1861_lincoln1.txt,3648])\n..\ndone\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:221"},{"title":"Optional: List the contents of your Object Store container to see the structure of the saved data frame.","text":"%sh\nsource swift_env.sh\nswift list ${container}","dateUpdated":"Jun 16, 2017 2:19:56 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"container":"demo"},"forms":{"container":{"name":"container","defaultValue":"","hidden":false}}},"jobName":"paragraph_1497623810927_-1858891437","id":"20170403-203400_319359235","dateCreated":"Jun 16, 2017 10:36:50 AM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:222","authenticationInfo":{},"dateFinished":"Jun 16, 2017 2:19:59 PM","dateStarted":"Jun 16, 2017 2:19:56 PM","focus":true},{"text":"%md\n#Optional: Explore the Spark UI\n\nWhen you use Spark (via Scala, Python, and/or SQL), you start a session with the Spark server.  In many situations, it can be helpful to view the \"Spark UI\" for your session.  BDCS-CE provides easy access to Spark UI for your Zeppelin session.\n\nTo view it, follow these steps...\n + Click on the Jobs tab\n + Find the running (\"Processing\") Zeppelin job\n + From the pop-up menu, choose Spark UI\n![spark ui](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011441.jpg \"Spark UI\")\n + Click on Attempt_1\n + Explore the Spark UI\n![SparkUI](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011442.jpg \"Spark UI\")\n\n\nNOTE: In BDCS-CE 17.2.1, there is an issue with the Spark UI link.  As a workaround, navigate to https://141.144.29.xx:1080/spark-history/ (where 141.144.29.xx is your BDCS-CE server public IP) and then click on \"Show incomplete applications\" to get to the Spark UI for the running Zeppelin session.\n\n\n","authenticationInfo":{},"dateUpdated":"Jun 16, 2017 10:57:46 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810927_-1858891437","id":"20170405-085653_1479537842","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Optional: Explore the Spark UI</h1>\n<p>When you use Spark (via Scala, Python, and/or SQL), you start a session with the Spark server.  In many situations, it can be helpful to view the &ldquo;Spark UI&rdquo; for your session.  BDCS-CE provides easy access to Spark UI for your Zeppelin session.</p>\n<p>To view it, follow these steps&hellip;</p>\n<ul>\n<li>Click on the Jobs tab</li>\n<li>Find the running (&ldquo;Processing&rdquo;) Zeppelin job</li>\n<li>From the pop-up menu, choose Spark UI\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011441.jpg\" alt=\"spark ui\" title=\"Spark UI\" /></li>\n<li>Click on Attempt_1</li>\n<li>Explore the Spark UI\n<br  /><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011442.jpg\" alt=\"SparkUI\" title=\"Spark UI\" /></li>\n</ul>\n<p>NOTE: In BDCS-CE 17.2.1, there is an issue with the Spark UI link.  As a workaround, navigate to https://141.144.29.xx:1080/spark-history/ (where 141.144.29.xx is your BDCS-CE server public IP) and then click on &ldquo;Show incomplete applications&rdquo; to get to the Spark UI for the running Zeppelin session.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","dateStarted":"Jun 16, 2017 10:57:44 AM","dateFinished":"Jun 16, 2017 10:57:44 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:223"},{"text":"%md\n#Next Steps\n\n+ Review the Spark and Spark SQL documentation at <https://spark.apache.org/docs/1.6.1/quick-start.html> and <https://spark.apache.org/docs/1.6.1/sql-programming-guide.html>\n+ Experiment with your own data sets\n+ Proceed to one of the other tutorials","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810927_-1858891437","id":"20170310-130044_26976236","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Next Steps</h1>\n<ul>\n<li>Review the Spark and Spark SQL documentation at <a href=\"https://spark.apache.org/docs/1.6.1/quick-start.html\">https://spark.apache.org/docs/1.6.1/quick-start.html</a> and <a href=\"https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\">https://spark.apache.org/docs/1.6.1/sql-programming-guide.html</a></li>\n<li>Experiment with your own data sets</li>\n<li>Proceed to one of the other tutorials</li>\n</ul>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224"},{"text":"%md\n#Appendix: Programmatically defining additional connections to the Object Store\n\nBDCS-CE defines a \"default\" set of Object Store credentials for use with Spark/Hadoop.  The following paragraph shows you how to define an additional set of credentials if needed.  In the paragraph below, the new set is called \"main\".  Once defined in your session, you can then refer to objects in the Object Store via paths like \"swift://container.main/speeches/pres_wordcount.json\", where you substitute the real container name for \"container\".\n\n\nNote: with BDCS-CE version 17.2.1, some of the values you set for the Object Store configuration will be cached/fixed for your running Spark session (which will continue indefinitely).  Therefore, you may need to restart the Zeppelin notebook (via the Settings tab) to pickup a different value.","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810927_-1858891437","id":"20170413-132914_1396761079","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Appendix: Programmatically defining additional connections to the Object Store</h1>\n<p>BDCS-CE defines a &ldquo;default&rdquo; set of Object Store credentials for use with Spark/Hadoop.  The following paragraph shows you how to define an additional set of credentials if needed.  In the paragraph below, the new set is called &ldquo;main&rdquo;.  Once defined in your session, you can then refer to objects in the Object Store via paths like &ldquo;swift://container.main/speeches/pres_wordcount.json&rdquo;, where you substitute the real container name for &ldquo;container&rdquo;.</p>\n<p>Note: with BDCS-CE version 17.2.1, some of the values you set for the Object Store configuration will be cached/fixed for your running Spark session (which will continue indefinitely).  Therefore, you may need to restart the Zeppelin notebook (via the Settings tab) to pickup a different value.</p>\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:225"},{"title":"Configuring Spark for the Object Store (Scala)","text":"%spark\n\n// Snippet of code to configure your Spark session to connect to the Object Store.  Do this at least once per session.  \n// If you subsequently change one of these values, you may need to restart the notebook (at least as of 17.2.1)\n\n// This section uses some Zepellin APIs to create some user entry fields\n// Learn more here: https://zeppelin.apache.org/docs/0.6.1/interpreter/spark.html#form-creation\n/* Create text input form with default value */\nz.angularBind(\"BIND_tenant\", z.input(\"identity domain\", \"gse00010212\"))\nz.angularBind(\"BIND_username\", z.input(\"username\", \"cloud.admin\"))\nz.angularBind(\"BIND_password\", z.input(\"password\", \"yourpassword\"))\n\n// This section sets properties in the HadoopConfiguration for a Object Store service name we will call main.\nval hconf = sc.hadoopConfiguration;\nhconf.set(\"fs.swift.SERVICE_NAME\", \"main\");\nhconf.set(\"fs.swift.service.main.auth.url\", \"https://\"+z.angular(\"BIND_tenant\")+\".storage.oraclecloud.com/auth/v2.0/tokens\");\nhconf.set(\"fs.swift.service.main.public\", \"true\");\nhconf.set(\"fs.swift.service.http.location-aware\", \"false\");\n\n// Fill up the credentials for Oracle cloud storage\nhconf.set(\"fs.swift.service.main.tenant\", \"Storage-\"+z.angular(\"BIND_tenant\"));\nhconf.set(\"fs.swift.service.main.username\", z.angular(\"BIND_username\").toString());\nhconf.set(\"fs.swift.service.main.password\", z.angular(\"BIND_password\").toString());\n\n// hint to Python users: if you are looking to set HadoopConfiguration parameters in Python code, try using Python syntax like this:\n// sc._jsc.hadoopConfiguration().set('fs.swift.SERVICE_NAME', 'main')\n\n// Note: The above properties can also be set in /etc/hadoop/conf/core-site.xml \n// The directory location should be what is reported by the scala sys.env(\"HADDOP_CONF_DIR\") output.\n// for more details, see https://spark.apache.org/docs/1.6.1/storage-openstack-swift.html\n\nprintln(\"Sanity check.  Here is the auth.url: \" + hconf.get(\"fs.swift.service.main.auth.url\"))\nprintln(\"Done\")","dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"container":"dcb-bdcs-apr12","identity domain":"gse00010212","password":"xxxx","username":"cloud.admin"},"forms":{"identity domain":{"name":"identity domain","displayName":"identity domain","type":"input","defaultValue":"gse00010212","hidden":false},"password":{"name":"password","displayName":"password","type":"input","defaultValue":"yourpassword","hidden":false},"username":{"name":"username","displayName":"username","type":"input","defaultValue":"cloud.admin","hidden":false}}},"jobName":"paragraph_1497623810927_-1858891437","id":"20170314-144629_847210145","result":{"code":"SUCCESS","type":"TEXT","msg":"Sanity check.  Here is the auth.url: https://gse00010212.storage.oraclecloud.com/auth/v2.0/tokens\nDone\nhconf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n"},"dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:226"},{"dateUpdated":"Jun 16, 2017 10:36:50 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497623810927_-1858891437","id":"20170424-170410_551484051","dateCreated":"Jun 16, 2017 10:36:50 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:227"}],"name":"Demonstration Presidential Speeches with Spark and Spark SQL","id":"2CKSTS52Y","angularObjects":{"2CKYUH771":[],"2CHQCE7WB":[],"2CM73VNZC":[],"2CJ9M7X3E":[],"2CM9MFPDC":[],"2CKUFFGB5":[],"2CH76629W":[],"2CJ6JMU49":[],"2CMM8Y2GW":[]},"config":{"looknfeel":"default"},"info":{}}